{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './' # should be a full path, e.g. /PATH_FROM_ROOT/cs492i-layout-generation\n",
    "LOAD_CHECKPOINT = 'transf-mse-chkpt.pt'\n",
    "PARENT_FOLDER = 'transf-mse'\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from preprocess import init_dataset\n",
    "from model_gan import SlideDeckEncoder, Generator, Discriminator\n",
    "#from model_layoutGAN import Generator, Discriminator\n",
    "#from model_combined import SlideDeckEncoder, CombinedGenerator, CombinedDiscriminator\n",
    "\n",
    "from utils_transformer import SortByRefSlide, draw_bbs, get_Tensor, get_device, get_args, draw_all_bbs\n",
    "\n",
    "from metrics import compute_alignment, compute_overlap, compute_alignment, compute_iou\n",
    "\n",
    "device = get_device()\n",
    "args = get_args()\n",
    "Tensor = get_Tensor()\n",
    "\n",
    "result_dir = Path(root) / 'results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chekpoint(path, models, optimizers):\n",
    "    path = os.path.join(path, LOAD_CHECKPOINT)\n",
    "    print(path)\n",
    "    try:\n",
    "       checkpoint = torch.load(path)\n",
    "    except:\n",
    "        print(\"Couldn't load the last checkpoint!\")\n",
    "        return models, optimizers, -1\n",
    "\n",
    "    models['encoder'].load_state_dict(checkpoint['model_encoder_state_dict'])\n",
    "    models['discriminator'].load_state_dict(checkpoint['model_discriminator_state_dict'])\n",
    "    models['generator'].load_state_dict(checkpoint['model_generator_state_dict'])\n",
    "    \n",
    "    optimizers['encoder'].load_state_dict(checkpoint['optimizer_encoder_state_dict'])\n",
    "    optimizers['generator'].load_state_dict(checkpoint['optimizer_generator_state_dict'])\n",
    "    optimizers['discriminator'].load_state_dict(checkpoint['optimizer_discriminator_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "\n",
    "    return models, optimizers, epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataset, test_dataset) = init_dataset(root, args.normalized)\n",
    "test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Uncomment appropriate model\n",
    "\n",
    "# MSE\n",
    "encoder = SlideDeckEncoder(\n",
    "    args.num_label, args.slide_deck_embedding_size, args.slide_deck_N, args.padding_idx, \n",
    "    args.D_d_model, args.D_nhead, args.D_num_layers\n",
    ").to(device)\n",
    "\n",
    "generator = Generator(\n",
    "    args.latent_size, args.num_label, args.slide_deck_embedding_size, args.padding_idx,\n",
    "    d_model=args.G_d_model,\n",
    "    nhead=args.G_nhead,\n",
    "    num_layers=args.G_num_layers,\n",
    ").to(device)\n",
    "\n",
    "discriminator = Discriminator(\n",
    "    args.num_label, args.slide_deck_embedding_size, args.max_seq_length, args.padding_idx,\n",
    "    d_model=args.D_d_model,\n",
    "    nhead=args.D_nhead,\n",
    "    num_layers=args.D_num_layers,\n",
    ").to(device)\n",
    "\n",
    "\n",
    "# COMBINED\n",
    "# encoder = SlideDeckEncoder(\n",
    "#     args.num_label, args.slide_deck_embedding_size, args.small_dim_slide, args.slide_deck_N, args.padding_idx, \n",
    "#     args.D_d_model, args.D_nhead, args.D_num_layers\n",
    "# ).to(device)\n",
    "\n",
    "# generator = CombinedGenerator(\n",
    "#     args.latent_size, args.num_label, args.small_dim_slide, args.padding_idx,\n",
    "#     d_model=args.G_d_model,\n",
    "#     nhead=args.G_nhead,\n",
    "#     num_layers=args.G_num_layers,\n",
    "# ).to(device)\n",
    "\n",
    "# discriminator = CombinedDiscriminator(\n",
    "#     args.num_label, args.slide_deck_embedding_size, args.small_dim_slide, args.max_seq_length, args.padding_idx,\n",
    "#     d_model=args.D_d_model,\n",
    "#     nhead=args.D_nhead,\n",
    "#     num_layers=args.D_num_layers,\n",
    "# ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "models = {\n",
    "    \"discriminator\": discriminator,\n",
    "    \"encoder\" : encoder,\n",
    "    \"generator\" : generator,\n",
    "}\n",
    "optimizers = {\n",
    "    \"discriminator\": torch.optim.Adam(models[\"discriminator\"].parameters(), lr=args.lr),\n",
    "    \"generator\": torch.optim.Adam(models[\"generator\"].parameters(), lr=args.lr),\n",
    "    \"encoder\" : torch.optim.Adam(models[\"encoder\"].parameters(), lr=args.lr)\n",
    "}\n",
    "\n",
    "\n",
    "parent_dir = result_dir / PARENT_FOLDER\n",
    "\n",
    "\n",
    "(models, optimizers, loaded_epoch) = load_chekpoint(parent_dir, models, optimizers)\n",
    "\n",
    "print(f\"Loaded Epoch: {loaded_epoch} from {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    models[model].eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate (Transformer + no Discriminator) + (with slide deck)\n",
    "\n",
    "total_iou = 0\n",
    "total_accuracy = 0\n",
    "n = 0\n",
    "total_overlap_real = 0\n",
    "total_overlap_fake = 0\n",
    "total_alignment_fake = 0\n",
    "total_alignment_real = 0\n",
    "total_loss_D_real = 0\n",
    "total_loss_D_fake = 0\n",
    "\n",
    "for run in range(10):\n",
    "    for batch in test_loader:\n",
    "        batch = SortByRefSlide(batch)\n",
    "        shape = batch[\"shape\"].to(device)\n",
    "        slide_deck = batch[\"slide_deck\"].to(device)\n",
    "        lengths_slide_deck = batch[\"lengths_slide_deck\"].to(device)\n",
    "        ref_length = batch[\"length_ref_types\"].to(device)\n",
    "        ref_types = batch[\"ref_types\"].to(device).long()\n",
    "        ref_slide = batch[\"ref_slide\"].to(device)\n",
    "\n",
    "        # deck encdoing\n",
    "        slide_deck = torch.transpose(slide_deck, 0, 1)\n",
    "        lengths_slide_deck = torch.transpose(lengths_slide_deck, 0, 1)\n",
    "        bboxes = slide_deck[:, :, :, :-1]\n",
    "        labels = slide_deck[:, :, :, -1].long()\n",
    "        padding_masks = ~(lengths_slide_deck[:, :, None] > torch.arange(labels.size(2)).to(device)[None, :])\n",
    "\n",
    "        deck_enc = models['encoder'](bboxes, labels, padding_masks)\n",
    "\n",
    "        #deck_enc = torch.rand(deck_enc.size()).to(device)\n",
    "        #deck_enc = torch.zeros(deck_enc.size()).to(device)\n",
    "        label = ref_types\n",
    "\n",
    "        padding_mask = ~(ref_length[:, None] > torch.arange(label.size(1)).to(device)[None, :])\n",
    "        bbox_real = ref_slide[:, :, :-1]\n",
    "\n",
    "        z = torch.autograd.Variable(Tensor(np.random.normal(0, 1, (label.size(0), label.size(1), args.latent_size))))\n",
    "        bbox_fake = models['generator'](z, label, deck_enc, padding_mask).detach()\n",
    "\n",
    "        n += 1\n",
    "        with torch.no_grad():\n",
    "            total_loss_D_real += torch.mean(models['discriminator'](bbox_real, label, deck_enc, padding_mask))\n",
    "            total_loss_D_fake += torch.mean(models['discriminator'](bbox_fake, label, deck_enc, padding_mask))\n",
    "\n",
    "        iou = torch.mean(torch.nan_to_num(compute_iou(bbox_real, bbox_fake)))\n",
    "\n",
    "        overlap_fake = torch.mean(torch.nan_to_num(compute_overlap(bbox_fake, ~padding_mask)))\n",
    "        overlap_real = torch.mean(torch.nan_to_num(compute_overlap(bbox_real, ~padding_mask)))\n",
    "\n",
    "        alignment_real = torch.mean(torch.nan_to_num(compute_alignment(bbox_real, ~padding_mask)))\n",
    "        alignment_fake = torch.mean(torch.nan_to_num(compute_alignment(bbox_fake, ~padding_mask)))\n",
    "\n",
    "        accuracy = torch.mean(F.mse_loss(bbox_fake, bbox_real))\n",
    "\n",
    "        if torch.isnan(iou) == False:\n",
    "            total_iou += iou\n",
    "        \n",
    "        if torch.isnan(overlap_real) == False:\n",
    "            total_overlap_real += overlap_real\n",
    "\n",
    "        if torch.isnan(overlap_fake) == False:\n",
    "            total_overlap_fake += overlap_fake\n",
    "\n",
    "        if torch.isnan(alignment_real) == False:\n",
    "            total_alignment_real += alignment_real\n",
    "\n",
    "        if torch.isnan(alignment_fake) == False:\n",
    "            total_alignment_fake += alignment_fake\n",
    "\n",
    "        if torch.isnan(accuracy) == False:\n",
    "            total_accuracy += accuracy\n",
    "\n",
    "print(n, total_accuracy / n, total_iou / n, total_overlap_fake / n, total_overlap_real / n, total_loss_D_real / n, total_loss_D_fake / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = list(test_loader)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = SortByRefSlide(batch)\n",
    "shape = batch[\"shape\"].to(device)\n",
    "slide_deck = batch[\"slide_deck\"].to(device)\n",
    "lengths_slide_deck = batch[\"lengths_slide_deck\"].to(device)\n",
    "ref_length = batch[\"length_ref_types\"].to(device)\n",
    "ref_types = batch[\"ref_types\"].to(device).long()\n",
    "ref_slide = batch[\"ref_slide\"].to(device)\n",
    "\n",
    "# deck encdoing\n",
    "slide_deck = torch.transpose(slide_deck, 0, 1)\n",
    "lengths_slide_deck = torch.transpose(lengths_slide_deck, 0, 1)\n",
    "bboxes = slide_deck[:, :, :, :-1]\n",
    "labels = slide_deck[:, :, :, -1].long()\n",
    "padding_masks = ~(lengths_slide_deck[:, :, None] > torch.arange(labels.size(2)).to(device)[None, :])\n",
    "models['encoder'].zero_grad()\n",
    "deck_enc = models['encoder'](bboxes, labels, padding_masks)\n",
    "deck_enc_rand = torch.rand(deck_enc.size()).to(device)\n",
    "deck_enc_zero = torch.zeros(deck_enc.size()).to(device)\n",
    "\n",
    "label = ref_types\n",
    "\n",
    "padding_mask = ~(ref_length[:, None] > torch.arange(label.size(1)).to(device)[None, :])\n",
    "bbox_real = ref_slide[:, :, :-1]\n",
    "\n",
    "real_layouts_bbs = ref_slide[:,:,:-1]\n",
    "\n",
    "z = torch.autograd.Variable(Tensor(np.random.normal(0, 1, (label.size(0), label.size(1), args.latent_size))))\n",
    "\n",
    "optimizers[\"encoder\"].zero_grad()\n",
    "optimizers[\"discriminator\"].zero_grad()\n",
    "\n",
    "# Sample noise as generator input\n",
    "fake_layouts_bbs = models['generator'](z, label, deck_enc, padding_mask).detach()\n",
    "fake_layouts_bbs_rand = models['generator'](z, label, deck_enc_rand, padding_mask).detach()\n",
    "fake_layouts_bbs_zero = models['generator'](z, label, deck_enc_zero, padding_mask).detach()\n",
    "\n",
    "list_bboxes = []\n",
    "list_labels = []\n",
    "for n in range(40):\n",
    "    for i in range(4):\n",
    "        list_bboxes.append(bboxes[i, n, :, :])\n",
    "        list_labels.append(labels[i, n, :])\n",
    "\n",
    "    list_bboxes.append(real_layouts_bbs[n])\n",
    "    list_labels.append(ref_types[n])\n",
    "\n",
    "    list_bboxes.append(fake_layouts_bbs[n])\n",
    "    list_labels.append(ref_types[n])\n",
    "\n",
    "    list_bboxes.append(fake_layouts_bbs_rand[n])\n",
    "    list_labels.append(ref_types[n])\n",
    "\n",
    "    list_bboxes.append(fake_layouts_bbs_zero[n])\n",
    "    list_labels.append(ref_types[n])\n",
    "\n",
    "draw_all_bbs(shape[n], list_bboxes, list_labels, 8, False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d3a26476f123d4e289f7bd6c8fe3f560f1d9f587964ae2681470863cea7877f5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('cs492i': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
