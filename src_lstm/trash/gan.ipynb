{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../'\n",
    "\n",
    "import os, sys\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import mask_rcnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = []\n",
    "\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "BB_TYPES = [\n",
    "    'title',\n",
    "    'header',\n",
    "    'text box',\n",
    "    'footer',\n",
    "    'picture',\n",
    "    'instructor',\n",
    "    'diagram',\n",
    "    'table',\n",
    "    'figure',\n",
    "    'handwriting',\n",
    "    'chart',\n",
    "    'schematic diagram',\n",
    "]\n",
    "\n",
    "args = edict()\n",
    "args.batch_size = 1\n",
    "args.lr = 1e-4\n",
    "args.momentum = 0.9\n",
    "args.weight_decay = 5e-4\n",
    "args.epoch = 10\n",
    "args.tensorboard = False\n",
    "args.gpu = True\n",
    "args.train_portion = 0.7\n",
    "args.slide_deck_embedding_dim = 128\n",
    "args.bbtype_num = len(BB_TYPES)\n",
    "args.latent_dim = 32\n",
    "args.hidden_size = 64\n",
    "args.input_size = 64 \n",
    "args.dropout_rate = 0.5\n",
    "\n",
    "assert(args.latent_dim == args.input_size//2)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "\n",
    "# Create directory name.\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tensorboard:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"{str(result_dir)}\" --samples_per_plugin images=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(image, bbs):\n",
    "    if (torch.is_tensor(image)):\n",
    "        image = np.array(image.tolist()).transpose((1, 2, 0))\n",
    "    if (torch.is_tensor(bbs)):\n",
    "        bbs = np.array(bbs.tolist())\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    for bb in bbs:\n",
    "        rect = patches.Rectangle((bb[0], bb[1]), bb[2], bb[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "class FitVidDataset(Dataset):\n",
    "    \"\"\" FitVid Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, img_data, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_data = img_data\n",
    "        self.img_filenames = list(self.img_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def show_image(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        filename = self.img_filenames[idx]\n",
    "        img_dir = os.path.join(self.root_dir, filename)\n",
    "        \n",
    "        image = io.imread(img_dir)\n",
    "        draw_image(image, self.img_data[filename]['bbs'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        filename = self.img_filenames[idx]\n",
    "        img_dir = os.path.join(self.root_dir, filename)\n",
    "        image = io.imread(img_dir)\n",
    "\n",
    "        bbs = self.img_data[filename]['bbs']\n",
    "\n",
    "        sample = {\n",
    "            'image' : image,\n",
    "            'labels' : bbs,\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbs(shape, bbs):\n",
    "    if (torch.is_tensor(bbs)):\n",
    "        bbs = np.array(bbs.tolist())\n",
    "    if (torch.is_tensor(shape)):\n",
    "        [h, w] = np.array(shape.tolist())\n",
    "        shape = (h, w)\n",
    "    \n",
    "    h, w = shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    background=patches.Rectangle((0, 0), w, h, linewidth=2, edgecolor='b', facecolor='black')\n",
    "    ax.add_patch(background)\n",
    "    for bb in bbs:\n",
    "        rect = patches.Rectangle((bb[0], bb[1]), bb[2], bb[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.autoscale(True, 'both')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_BB_types(bbs):\n",
    "    return bbs[:, 4]\n",
    "\n",
    "class BBSlideDeckDataset(Dataset):\n",
    "    \"\"\" Slide Deck Dataset but with Bounding Boxes\"\"\"\n",
    "    def __init__(self, slide_deck_data, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.slide_deck_data = slide_deck_data\n",
    "        self.slide_deck_ids = list(self.slide_deck_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slide_deck_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        slide_deck_id = self.slide_deck_ids[idx]\n",
    "        (h, w) = self.slide_deck_data[slide_deck_id][\"shape\"]\n",
    "        lengths_slide_deck = []\n",
    "        slides = []\n",
    "        max_len_bbs = 0\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            lengths_slide_deck.append(len(slide))\n",
    "            if len(slide) > max_len_bbs:\n",
    "                max_len_bbs = len(slide)\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            np_slide = np.zeros((max_len_bbs, 5))\n",
    "            for i, bb in enumerate(slide):\n",
    "                np_slide[i] = bb\n",
    "            slides.append(np_slide)\n",
    "\n",
    "        ref_slide = slides[0]\n",
    "        slide_deck = slides[1:]\n",
    "        length_ref_types = lengths_slide_deck.pop(0)\n",
    "        sample = {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": get_BB_types(ref_slide),\n",
    "            \"slide_deck\": np.asarray(slide_deck),\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleBB(object):\n",
    "    \"\"\"Rescale the bounding boxes in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def _resize_single_slide(self, slide, original_shape, new_shape):\n",
    "        h, w = original_shape\n",
    "        new_h, new_w = new_shape\n",
    "        slide = slide * np.array([new_w / w, new_h / h, new_w / w, new_h / h, 1]).T\n",
    "        return slide\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        ref_slide = self._resize_single_slide(ref_slide, (h, w), (new_h, new_w))\n",
    "        for i, slide in enumerate(slide_deck):\n",
    "            slide_deck[i] = self._resize_single_slide(slide, (h, w), (new_h, new_w))\n",
    "\n",
    "        return {\n",
    "            \"shape\": (new_h, new_w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class LeaveN(object):\n",
    "    def __init__ (self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if slide_deck.shape[0] > self.N:\n",
    "            slide_deck = np.delete(slide_deck, range(self.N, slide_deck.shape[0]), 0)\n",
    "            lengths_slide_deck = lengths_slide_deck[:self.N]\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ShuffleRefSlide(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        lengths_slide_deck.append(length_ref_types)\n",
    "        slide_deck = np.vstack((slide_deck, ref_slide[None, :]))\n",
    "\n",
    "        idxs = np.array([*range(0, len(lengths_slide_deck))], dtype=np.int32)\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        slide_deck = slide_deck[idxs]\n",
    "\n",
    "        lengths_slide_deck = np.array(lengths_slide_deck)\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck.tolist()\n",
    "        \n",
    "        slide_deck = slide_deck.tolist()\n",
    "        ref_slide = np.asarray(slide_deck.pop())\n",
    "        length_ref_types = lengths_slide_deck.pop()\n",
    "        ref_types = get_BB_types(ref_slide)\n",
    "\n",
    "        slide_deck = np.asarray(slide_deck)\n",
    "        \n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ToTensorBB(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "        return {\n",
    "            \"shape\": torch.tensor([h, w]),\n",
    "            \"ref_slide\": torch.from_numpy(ref_slide),\n",
    "            \"ref_types\": torch.from_numpy(ref_types),\n",
    "            \"slide_deck\": torch.from_numpy(slide_deck),\n",
    "            \"lengths_slide_deck\": torch.tensor(lengths_slide_deck),\n",
    "            \"length_ref_types\": torch.tensor(length_ref_types)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleBB(object):\n",
    "    \"\"\"Rescale the bounding boxes in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def _resize_single_slide(self, slide, original_shape, new_shape):\n",
    "        h, w = original_shape\n",
    "        new_h, new_w = new_shape\n",
    "        slide = slide * np.array([new_w / w, new_h / h, new_w / w, new_h / h, 1]).T\n",
    "        return slide\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        ref_slide = self._resize_single_slide(ref_slide, (h, w), (new_h, new_w))\n",
    "        for i, slide in enumerate(slide_deck):\n",
    "            slide_deck[i] = self._resize_single_slide(slide, (h, w), (new_h, new_w))\n",
    "\n",
    "        return {\n",
    "            \"shape\": (new_h, new_w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"slide_deck\": slide_deck\n",
    "        }\n",
    "\n",
    "class LeaveN(object):\n",
    "    def __init__ (self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "\n",
    "        np.random.shuffle(slide_deck)\n",
    "\n",
    "        if slide_deck.shape[0] > self.N:\n",
    "            slide_deck = np.delete(slide_deck, range(self.N, slide_deck.shape[0]), 0)\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"slide_deck\": slide_deck\n",
    "        }\n",
    "\n",
    "class ShuffleRefSlide(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        \n",
    "        slide_deck = np.vstack((slide_deck, ref_slide[None, :]))\n",
    "        np.random.shuffle(slide_deck)\n",
    "        slide_deck = slide_deck.tolist()\n",
    "        ref_slide = np.asarray(slide_deck.pop())\n",
    "        slide_deck = np.asarray(slide_deck)\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"slide_deck\": slide_deck\n",
    "        }\n",
    "\n",
    "class ShuffleSlideDeck(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        np.random.shuffle(slide_deck)\n",
    "        \n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"slide_deck\": slide_deck\n",
    "        }\n",
    "\n",
    "class ToTensorBB(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        return {\n",
    "            'shape': torch.tensor([h, w]),\n",
    "            'ref_slide': torch.from_numpy(ref_slide),\n",
    "            'slide_deck': torch.from_numpy(slide_deck)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slide_deck_dataset(all_dataset):\n",
    "    slide_deck_data = {}\n",
    "    for entrance in all_dataset.iloc:\n",
    "        slide_deck_id = entrance['Slide Deck Id']\n",
    "        \n",
    "        slide_id = entrance[\"Slide Id\"]\n",
    "        if (slide_deck_id not in slide_deck_data):\n",
    "            slide_deck_data[slide_deck_id] = {\n",
    "                'slides': {},\n",
    "                'shape': (entrance['Image Height'], entrance['Image Width'])\n",
    "            }\n",
    "        \n",
    "        if slide_id not in slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            slide_deck_data[slide_deck_id][\"slides\"][slide_id] = []\n",
    "        bb_type = BB_TYPES.index(entrance['Type'])\n",
    "        if (bb_type < 0 or bb_type >= len(BB_TYPES)):\n",
    "            bb_type = len(BB_TYPES)\n",
    "\n",
    "        bb = np.array([\n",
    "            entrance['X'],\n",
    "            entrance['Y'],\n",
    "            entrance['BB Width'],\n",
    "            entrance['BB Height'],\n",
    "            bb_type + 1\n",
    "        ]).T\n",
    "        slide_deck_data[slide_deck_id]['slides'][slide_id].append(bb)\n",
    "    for key in slide_deck_data.keys():\n",
    "        \n",
    "        # if key == 100:\n",
    "        #     for (id, value) in slide_deck_data[key][\"slides\"].items():\n",
    "        #         print(56, id)\n",
    "        #         draw_bbs(slide_deck_data[key][\"shape\"], value)\n",
    "\n",
    "        values = list(slide_deck_data[key][\"slides\"].values())\n",
    "        slide_deck_data[key][\"slides\"] = [np.asarray(value) for value in values]\n",
    "    return slide_deck_data\n",
    "\n",
    "def slice_dict(dictionary, l, r):\n",
    "    keys = list(dictionary.keys())\n",
    "    keys = keys[l:r]\n",
    "    ret_dictionary = {}\n",
    "    for key in keys:\n",
    "        ret_dictionary[key] = dictionary[key]\n",
    "    return ret_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"slide_deck_dataset.csv\")\n",
    "\n",
    "dataset = pd.read_csv(csv_file)\n",
    "slide_deck_data = process_slide_deck_dataset(dataset)\n",
    "\n",
    "division = int(args.train_portion * len(slide_deck_data))\n",
    "\n",
    "train_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, 0, division),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        ShuffleSlideDeck(),\n",
    "        LeaveN(5),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, division, len(slide_deck_data)),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        #ShuffleSlideDeck(),\n",
    "        #ShuffleRefSlide(),\n",
    "        #LeaveN(5),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5]) torch.Size([5, 7, 5])\n",
      "____________\n",
      "shape\n",
      "torch.Size([2])\n",
      "____________\n",
      "ref_slide\n",
      "torch.Size([7, 5])\n",
      "____________\n",
      "slide_deck\n",
      "torch.Size([5, 7, 5])\n"
     ]
    }
   ],
   "source": [
    "single = train_slide_deck_dataset[0]\n",
    "draw_bbs(single[\"shape\"], single[\"ref_slide\"])\n",
    "print(single[\"ref_slide\"].shape, single[\"slide_deck\"].shape)\n",
    "for key, sl in single.items():\n",
    "    print(\"____________\")\n",
    "    print(key)\n",
    "    print(sl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, torch.autograd.Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "    return torch.autograd.Variable(y_one_hot) if isinstance(y, torch.autograd.Variable) else y_one_hot\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding=False):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.exist_embedding = embedding\n",
    "        if self.exist_embedding:\n",
    "            self.embed = nn.Embedding(args.bbtype_num + 1, args.input_size//2, padding_idx=0)\n",
    "            input_size = args.input_size\n",
    "        else:\n",
    "            input_size = args.input_size//2 + args.bbtype_num + 1\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=args.hidden_size, num_layers=2, \n",
    "            batch_first=True, dropout=args.dropout_rate, bias=True)\n",
    "        self.linear1 = nn.Linear(args.slide_deck_embedding_dim, args.hidden_size)\n",
    "        self.linear2 = nn.Linear(args.hidden_size, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x, z, slide_deck_embedding, length=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "            z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            length (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            bb sequence: (tensor), (Batch_size, Sequence_size, 5)\n",
    "        \"\"\"\n",
    "        (Batch_size, Sequence_size) = x.shape\n",
    "        if self.exist_embedding:\n",
    "            temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        else:\n",
    "            temp_input_1 = to_one_hot(x, n_dims=args.bbtype_num + 1)\n",
    "            temp_input_1[:, :, 0] = 0.0\n",
    "            temp_input_1 = temp_input_1.to(device)\n",
    "\n",
    "                \n",
    "        temp_input_2 = z.unsqueeze(1).repeat((1, Sequence_size, 1))\n",
    "        input_1 = torch.cat((temp_input_2, temp_input_1), dim=-1)\n",
    "        hidden_0 = self.dropout(self.linear1(slide_deck_embedding)).unsqueeze(0).repeat((2, 1, 1))\n",
    "        c_0 = torch.zeros(size=(2,Batch_size, args.hidden_size)).to(device)\n",
    "        output, (h_n, c_n) = self.lstm(input_1, (hidden_0, c_0))\n",
    "        output = output.transpose(0, 1)\n",
    "        return self.linear2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 5])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "input = torch.LongTensor([[0,2,4,0],[4,3,0,0]])\n",
    "embedding(input).shape\n",
    "\n",
    "a = to_one_hot(input)\n",
    "a[:,:,0] = 0\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 2) #input_size  = 10, hidden = 20, layer=2\n",
    "input = torch.randn(5, 3, 10) # 3 batch, \n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 20]), torch.Size([2, 3, 20]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2,  3,  2],\n",
       "         [ 2,  3,  2]],\n",
       "\n",
       "        [[23,  2,  2],\n",
       "         [23,  2,  2]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[2, 3,2],[23,2, 2]])\n",
    "print(x.shape)\n",
    "x.unsqueeze(1).repeat((1,2,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45dc61766396859fdffae760accc4b74216ea8fbbed6d1a9d5e8fb1914e35062"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
