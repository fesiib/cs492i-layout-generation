{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../'\n",
    "\n",
    "import os, sys\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from functools import cmp_to_key\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import mask_rcnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "#!pip install easydict\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "args = edict()\n",
    "args.batch_size = 64\n",
    "args.nlayers = 2\n",
    "\n",
    "args.embedding_size = 4\n",
    "args.ninp = 4 + args.embedding_size\n",
    "args.nhid = 64 #512\n",
    "\n",
    "args.dropout = 0.2\n",
    "args.gpu = True\n",
    "\n",
    "args.tensorboard = False\n",
    "args.train_portion = 0.7\n",
    "args.slide_deck_N = 5\n",
    "args.slide_deck_embedding_size = 512\n",
    "args.padding_idx = 0\n",
    "args.max_seq_length = 8\n",
    "\n",
    "# Decoder\n",
    "args.latent_vector_dim = 28\n",
    "\n",
    "# GAN\n",
    "args.n_epochs = 200\n",
    "args.lr = 0.00005\n",
    "args.n_cpu = 4\n",
    "args.latent_dim = 100\n",
    "args.channels = 1\n",
    "args.clip_value = 0.1\n",
    "args.sample_interval = 400\n",
    "args.n_critic = 2\n",
    "args.b1 = 0.5\n",
    "args.b2 = 0.999\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "# Create directory name.\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tensorboard:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"{str(result_dir)}\" --samples_per_plugin images=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_TYPES = [\n",
    "    '<pad>',\n",
    "    'title',\n",
    "    'header',\n",
    "    'text box',\n",
    "    'footer',\n",
    "    'picture',\n",
    "    'instructor',\n",
    "    'diagram',\n",
    "    'table',\n",
    "    'figure',\n",
    "    'handwriting',\n",
    "    'chart',\n",
    "    'schematic diagram',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbs(shape, bbs):\n",
    "    if (torch.is_tensor(bbs)):\n",
    "        bbs = np.array(bbs.tolist())\n",
    "    if (torch.is_tensor(shape)):\n",
    "        [h, w] = np.array(shape.tolist())\n",
    "        shape = (h, w)\n",
    "    \n",
    "    h, w = shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    background=patches.Rectangle((0, 0), w, h, linewidth=2, edgecolor='b', facecolor='black')\n",
    "    ax.add_patch(background)\n",
    "    for bb in bbs:\n",
    "        rect = patches.Rectangle((bb[0], bb[1]), bb[2], bb[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.autoscale(True, 'both')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_BB_types(bbs):\n",
    "    return bbs[:, 4]\n",
    "\n",
    "class BBSlideDeckDataset(Dataset):\n",
    "    \"\"\" Slide Deck Dataset but with Bounding Boxes\"\"\"\n",
    "    def __init__(self, slide_deck_data, slide_deck_N, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.slide_deck_data = {}\n",
    "\n",
    "        for key, val in slide_deck_data.items():\n",
    "            n = len(val['slides'])\n",
    "            for i in range(n):\n",
    "                if (n - i < slide_deck_N):\n",
    "                    break\n",
    "                if (i % slide_deck_N == 0 or (n - i) == slide_deck_N):\n",
    "                    current_slide_deck = {\n",
    "                        'slides': val['slides'][i:(i+slide_deck_N)],\n",
    "                        'shape': val['shape'],\n",
    "                    }\n",
    "                    self.slide_deck_data[str(key) + '_' + str(i)] = current_slide_deck\n",
    "        print(len(self.slide_deck_data))\n",
    "        self.slide_deck_ids = list(self.slide_deck_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slide_deck_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        slide_deck_id = self.slide_deck_ids[idx]\n",
    "        (h, w) = self.slide_deck_data[slide_deck_id][\"shape\"]\n",
    "        lengths_slide_deck = []\n",
    "        \n",
    "        slides = []\n",
    "        max_len_bbs = args.max_seq_length\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            lengths_slide_deck.append(min(max_len_bbs, len(slide)))\n",
    "            np_slide = np.zeros((max_len_bbs, 5), dtype=np.double)\n",
    "            for i, bb in enumerate(slide):\n",
    "                if (i >= max_len_bbs):\n",
    "                    break\n",
    "                np_slide[i] = bb\n",
    "            slides.append(np_slide)\n",
    "        ref_slide = slides[0]\n",
    "        slide_deck = slides[1:]\n",
    "        length_ref_types = lengths_slide_deck.pop(0)\n",
    "        sample = {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": get_BB_types(ref_slide),\n",
    "            \"slide_deck\": np.asarray(slide_deck),\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleBB(object):\n",
    "    \"\"\"Rescale the bounding boxes in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def _resize_single_slide(self, slide, original_shape, new_shape):\n",
    "        h, w = original_shape\n",
    "        new_h, new_w = new_shape\n",
    "        slide = slide * np.array([new_w / w, new_h / h, new_w / w, new_h / h, 1]).T\n",
    "        return slide\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        ref_slide = self._resize_single_slide(ref_slide, (h, w), (new_h, new_w))\n",
    "        for i, slide in enumerate(slide_deck):\n",
    "            slide_deck[i] = self._resize_single_slide(slide, (h, w), (new_h, new_w))\n",
    "\n",
    "        return {\n",
    "            \"shape\": (new_h, new_w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class LeaveN(object):\n",
    "    def __init__ (self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if slide_deck.shape[0] > self.N:\n",
    "            slide_deck = np.delete(slide_deck, range(self.N, slide_deck.shape[0]), 0)\n",
    "            lengths_slide_deck = lengths_slide_deck[:self.N]\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ShuffleRefSlide(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        lengths_slide_deck.append(length_ref_types)\n",
    "        slide_deck = np.vstack((slide_deck, ref_slide[None, :]))\n",
    "\n",
    "        idxs = np.array([*range(0, len(lengths_slide_deck))], dtype=np.int32)\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        slide_deck = slide_deck[idxs]\n",
    "\n",
    "        lengths_slide_deck = np.array(lengths_slide_deck, dtype=np.int32)\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck.tolist()\n",
    "        \n",
    "        slide_deck = slide_deck.tolist()\n",
    "        ref_slide = np.asarray(slide_deck.pop())\n",
    "        length_ref_types = lengths_slide_deck.pop()\n",
    "        ref_types = get_BB_types(ref_slide)\n",
    "\n",
    "        slide_deck = np.asarray(slide_deck)\n",
    "        \n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ToTensorBB(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        idxs = [*range(0, len(lengths_slide_deck))]\n",
    "\n",
    "        def by_length(p1, p2):\n",
    "            return lengths_slide_deck[p2] - lengths_slide_deck[p1]\n",
    "        idxs = sorted(idxs, key=cmp_to_key(by_length))\n",
    "\n",
    "        shape = torch.tensor([h, w], dtype=torch.float64)\n",
    "        ref_slide = torch.from_numpy(ref_slide).float()\n",
    "        ref_types = torch.from_numpy(ref_types).float()\n",
    "        \n",
    "        slide_deck = torch.from_numpy(slide_deck).float()\n",
    "        lengths_slide_deck = torch.tensor(lengths_slide_deck, dtype=torch.int32)\n",
    "        \n",
    "        slide_deck = slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "\n",
    "        length_ref_types = torch.tensor(length_ref_types, dtype=torch.int32)\n",
    "\n",
    "        return {\n",
    "            \"shape\": shape,\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slide_deck_dataset(all_dataset):\n",
    "    slide_deck_data = {}\n",
    "    for entrance in all_dataset.iloc:\n",
    "        slide_deck_id = entrance['Slide Deck Id']\n",
    "        \n",
    "        slide_id = entrance[\"Slide Id\"]\n",
    "        if (slide_deck_id not in slide_deck_data):\n",
    "            slide_deck_data[slide_deck_id] = {\n",
    "                'slides': {},\n",
    "                'shape': (entrance['Image Height'], entrance['Image Width'])\n",
    "            }\n",
    "        \n",
    "        if slide_id not in slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            slide_deck_data[slide_deck_id][\"slides\"][slide_id] = []\n",
    "        bb_type = BB_TYPES.index(entrance['Type'])\n",
    "        if (bb_type < 0 or bb_type >= len(BB_TYPES)):\n",
    "            bb_type = len(BB_TYPES)\n",
    "\n",
    "        bb = np.array([\n",
    "            entrance['X'],\n",
    "            entrance['Y'],\n",
    "            entrance['BB Width'],\n",
    "            entrance['BB Height'],\n",
    "            bb_type\n",
    "        ]).T\n",
    "        slide_deck_data[slide_deck_id]['slides'][slide_id].append(bb)\n",
    "    for key in slide_deck_data.keys():\n",
    "        \n",
    "        # if key == 100:\n",
    "        #     for (id, value) in slide_deck_data[key][\"slides\"].items():\n",
    "        #         print(56, id)\n",
    "        #         draw_bbs(slide_deck_data[key][\"shape\"], value)\n",
    "\n",
    "        values = list(slide_deck_data[key][\"slides\"].values())\n",
    "        slide_deck_data[key][\"slides\"] = [np.asarray(value) for value in values]\n",
    "    return slide_deck_data\n",
    "\n",
    "def slice_dict(dictionary, l, r):\n",
    "    keys = list(dictionary.keys())\n",
    "    keys = keys[l:r]\n",
    "    ret_dictionary = {}\n",
    "    for key in keys:\n",
    "        ret_dictionary[key] = dictionary[key]\n",
    "    return ret_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file:  slide_deck_dataset_bmvc13.csv\n",
      "file:  slide_deck_dataset_acl17.csv\n",
      "file:  slide_deck_dataset_bmvc12.csv\n",
      "file:  slide_deck_dataset_cvpr10.csv\n",
      "file:  slide_deck_dataset_icml20.csv\n",
      "file:  slide_deck_dataset_eccv20.csv\n",
      "file:  slide_deck_dataset_emnlp17.csv\n",
      "file:  slide_deck_dataset_icml19.csv\n",
      "file:  slide_deck_dataset_nips18.csv\n",
      "file:  slide_deck_dataset_eccv14.csv\n",
      "file:  slide_deck_dataset_nips19.csv\n",
      "file:  slide_deck_dataset_naacl19.csv\n",
      "file:  slide_deck_dataset_eccv12.csv\n",
      "file:  slide_deck_dataset_fg15.csv\n",
      "file:  slide_deck_dataset_iclr20.csv\n",
      "file:  slide_deck_dataset_emnlp18.csv\n",
      "file:  slide_deck_dataset_acl18.csv\n",
      "file:  slide_deck_dataset_cvpr20.csv\n",
      "file:  slide_deck_dataset_acl20.csv\n",
      "22212\n",
      "7028\n"
     ]
    }
   ],
   "source": [
    "csv_files_root = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"bbs\")\n",
    "\n",
    "dataset = None\n",
    "\n",
    "for _, dirs, files in os.walk(csv_files_root):\n",
    "    for file in files:\n",
    "        if file.endswith('.csv'):\n",
    "            print('file: ', file)\n",
    "            csv_file_path = os.path.join(csv_files_root, file)\n",
    "            cur_dataset = pd.read_csv(csv_file_path)\n",
    "            if dataset is None:\n",
    "                dataset = cur_dataset\n",
    "            else:\n",
    "                dataset = pd.concat([dataset, cur_dataset])\n",
    "slide_deck_data = process_slide_deck_dataset(dataset)\n",
    "\n",
    "division = int(args.train_portion * len(slide_deck_data))\n",
    "\n",
    "train_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, 0, division),\n",
    "    slide_deck_N=args.slide_deck_N+1,\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(args.slide_deck_N),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, division, len(slide_deck_data)),\n",
    "    slide_deck_N=args.slide_deck_N+1,\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(args.slide_deck_N),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "single = train_slide_deck_dataset[0]\n",
    "draw_bbs(single[\"shape\"], single[\"ref_slide\"])\n",
    "print(single[\"ref_slide\"].shape)\n",
    "print(single[\"shape\"])\n",
    "# print(single[\"ref_types\"])\n",
    "# for i in range(5):\n",
    "#     print(single[\"slide_deck\"][i])\n",
    "# print(single[\"lengths_slide_deck\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortByRefSlide(batch):\n",
    "    idx = [*range(batch[\"ref_slide\"].shape[0])]\n",
    "\n",
    "    def by_length(p1, p2):\n",
    "        return batch[\"length_ref_types\"][p2] - batch[\"length_ref_types\"][p1]\n",
    "    idx = sorted(idx, key=cmp_to_key(by_length))\n",
    "\n",
    "    idx = torch.tensor(idx).to(device).long()\n",
    "    for prop in batch.keys():\n",
    "        batch[prop] = batch[prop][idx]\n",
    "    \n",
    "    return batch\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_slide_deck_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_slide_deck_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlideEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlideEncoder, self).__init__()\n",
    "        ninp = args.ninp\n",
    "        nhid = args.nhid\n",
    "        nlayers = args.nlayers\n",
    "        dropout = args.dropout\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, args.padding_idx)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, bias=True).float()\n",
    "\n",
    "    def forward(self, x, states, lengths=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor(B, L, 5)\n",
    "            states: List[Tuple(h_0, c_0), ..., Tuple(h_{B-1}, c_{B-1})]\n",
    "            lengths: tensor(B)\n",
    "        \"\"\"\n",
    "        idxs = [*range(0, len(lengths))]\n",
    "        def by_lengths(p1, p2):\n",
    "            return lengths[p2] - lengths[p1]\n",
    "\n",
    "        idxs = sorted(idxs, key=cmp_to_key(by_lengths))\n",
    "\n",
    "        x = x[:, idxs]\n",
    "        lengths = lengths[idxs]\n",
    "        \n",
    "        input = x[:, :, :-1]\n",
    "        types = x[:, :, -1:].long()\n",
    "        types = torch.squeeze(self.embed(types))\n",
    "        input = torch.cat((input, types), dim=-1)\n",
    "\n",
    "        output = self.dropout(input)\n",
    "        \n",
    "\n",
    "        output = torch.nn.utils.rnn.pack_padded_sequence(output, lengths.cpu())\n",
    "\n",
    "        h_0 = torch.stack([h for (h, _) in states], dim=0)\n",
    "        c_0 = torch.stack([c for (_, c) in states], dim=0)\n",
    "\n",
    "        (output, context_vector) = self.lstm(output.to(device), (h_0, c_0))\n",
    "        output, lengths = torch.nn.utils.rnn.pad_packed_sequence(output, total_length = args.max_seq_length)\n",
    "        return (output, context_vector)\n",
    "\n",
    "class SlideDeckEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlideDeckEncoder, self).__init__()\n",
    "        self.slide_encoder = SlideEncoder()\n",
    "\n",
    "        input_size = args.nhid * args.slide_deck_N\n",
    "        output_size = args.slide_deck_embedding_size\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        return\n",
    "\n",
    "    def _get_init_states(self, x):\n",
    "        init_states = [\n",
    "            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n",
    "            torch.zeros((x.size(1), args.nhid)).to(x.device))\n",
    "            for _ in range(args.nlayers)\n",
    "        ]\n",
    "        return init_states\n",
    "    \n",
    "    def forward(self, xs, lengths):\n",
    "        states = None\n",
    "        embedding = []\n",
    "        for i, x in enumerate(xs):\n",
    "            if states is None:\n",
    "                states = self._get_init_states(x)\n",
    "            length = lengths[i]\n",
    "            output, states = self.slide_encoder(x, states, length)\n",
    "            output = output[length.long() - 1,:,:]\n",
    "            idxs = torch.arange(args.batch_size)\n",
    "            output = output[idxs, idxs, :]\n",
    "            embedding.append(output.squeeze())\n",
    "        \n",
    "        output = torch.cat(embedding, dim=-1)\n",
    "        output = self.relu(self.linear(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input, output and indices must be on the current device",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-54e9835ffc5f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"lengths_slide_deck\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mslide_deck_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-a3910374c9cb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, xs, lengths)\u001b[0m\n\u001b[0;32m     70\u001b[0m                 \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_init_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m             \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslide_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0midxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-63-a3910374c9cb>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, states, lengths)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         return F.embedding(\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1911\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1913\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input, output and indices must be on the current device"
     ]
    }
   ],
   "source": [
    "model = SlideDeckEncoder().to(device)\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "    for batch in train_loader:\n",
    "        xs = torch.transpose(batch[\"slide_deck\"], 0, 1)\n",
    "        xs = torch.transpose(xs, 1, 2)\n",
    "        lengths = torch.transpose(batch[\"lengths_slide_deck\"], 0, 1)\n",
    "        slide_deck_embedding = model(xs, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, torch.autograd.Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "    return torch.autograd.Variable(y_one_hot) if isinstance(y, torch.autograd.Variable) else y_one_hot\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, embed_weights=None, ganlike=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ganlike = ganlike\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size=args.latent_vector_dim + args.embedding_size, hidden_size=args.nhid, num_layers=2, \n",
    "            batch_first=True, dropout=args.dropout, bias=True)\n",
    "        self.linear1 = nn.Linear(args.slide_deck_embedding_size, args.nhid)\n",
    "        self.linear2 = nn.Linear(args.nhid, 4)\n",
    "        if embed_weights is not None:\n",
    "            self.embed.weight.data = embed_weights\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            # if normalize:\n",
    "            #     layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.gen_model = nn.Sequential(\n",
    "            *block(args.nhid, 32, normalize=False),\n",
    "            *block(32, 32),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, z, slide_deck_embedding, lengths=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "            z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            lengths (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            bb sequence: (tensor), (Batch_size, Sequence_size, 5)\n",
    "        \"\"\"\n",
    "        # print(x.shape, z.shape, slide_deck_embedding.shape, lengths)\n",
    "        x = x.int()\n",
    "        (Batch_size, Sequence_size) = x.shape\n",
    "        temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        # print(temp_input_1)\n",
    "        # print(\"1\",temp_input_1.shape)\n",
    "        temp_input_2 = z.unsqueeze(1).repeat((1, Sequence_size, 1))\n",
    "        # print(\"2\",temp_input_2.shape)\n",
    "        input_1 = torch.cat((temp_input_2, temp_input_1), dim=-1)\n",
    "        # print(input_1.shape)\n",
    "        input_1 = torch.nn.utils.rnn.pack_padded_sequence(input_1, lengths.cpu(), batch_first=True)\n",
    "        # print(input_1.data.shape)\n",
    "        # print(\"3\",input_1.shape)\n",
    "        hidden_0 = self.dropout(self.linear1(slide_deck_embedding)).unsqueeze(0).repeat((2, 1, 1))\n",
    "        # print(\"4\",hidden_0.shape)\n",
    "        c_0 = torch.zeros(size=(2, Batch_size, args.nhid)).to(device)\n",
    "        # print(\"5\",c_0.shape)\n",
    "        output, (h_n, c_n) = self.lstm(input_1.to(device), (hidden_0, c_0))\n",
    "        # print(output.data.shape)\n",
    "        output, length = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=args.max_seq_length)\n",
    "\n",
    "        # output = output.transpose(0, 1)\n",
    "        if self.ganlike:\n",
    "            # output = output.transpose(1, 2)\n",
    "            # print(output.shape)\n",
    "            output = self.gen_model(output)\n",
    "        else:\n",
    "            output = self.linear2(output)\n",
    "\n",
    "        return output, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-15a2fa89a22e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'length_ref_types'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#print(t,l)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmydec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslide_deck_embedding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslide_deck_embedding_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'length_ref_types'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;31m# draw_bbs(,)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-e3cbcdb75e86>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, z, slide_deck_embedding, lengths)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mc_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnhid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# print(\"5\",c_0.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_n\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhidden_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         \u001b[1;31m# print(output.data.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    662\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    663\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[0;32m    665\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0;32m    666\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu"
     ]
    }
   ],
   "source": [
    "#print(args.batch_size)\n",
    "mydec = Generator()\n",
    "z = torch.randn((args.batch_size, args.latent_vector_dim))\n",
    "s = SortByRefSlide(list(train_loader)[0])\n",
    "# print(len(s))\n",
    "# for sl in s:\n",
    "#     print(sl)\n",
    "#     print(s[sl].shape)\n",
    "\n",
    "idx = [i for i in range(s['ref_types'].size(0)-1, -1, -1)]\n",
    "idx = torch.LongTensor(idx)\n",
    "#print(idx)\n",
    "t = s['ref_types']\n",
    "l = s['length_ref_types']\n",
    "#print(t,l)\n",
    "v = mydec(x=t, z=z, slide_deck_embedding=torch.randn((args.batch_size, args.slide_deck_embedding_size)), lengths=s['length_ref_types'])\n",
    "# draw_bbs(,)\n",
    "v[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_weights=None):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.lstm = nn.LSTM(input_size = args.ninp, hidden_size=args.nhid, num_layers=args.nlayers, \n",
    "            batch_first=True, bias=True)\n",
    "        self.linear1 = nn.Linear(args.slide_deck_embedding_size, args.nhid)\n",
    "        self.d_model = nn.Sequential(\n",
    "            nn.Linear(args.nhid, args.nhid//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(args.nhid//2, args.nhid//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(args.nhid//2, 1)\n",
    "        )\n",
    "        if embed_weights is not None:\n",
    "            self.embed.weight.data = embed_weights\n",
    "\n",
    "\n",
    "    def forward(self, x, bb, slide_deck_embedding, lengths=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): type labels, (Batch_size, Sequence_size)\n",
    "            bb (tensor): (Batch_size, Sequence_size, 4)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            length (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            \n",
    "        \"\"\"\n",
    "        x = x.int()\n",
    "        \n",
    "        (Batch_size, _) = x.shape\n",
    "        temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        input_1 = torch.cat((bb, temp_input_1), dim=-1)\n",
    "        input_1 = torch.nn.utils.rnn.pack_padded_sequence(input_1, lengths.cpu(), batch_first=True)\n",
    "\n",
    "        h_0 = self.dropout(self.linear1(slide_deck_embedding)).unsqueeze(0).repeat((2, 1, 1))\n",
    "        c_0 = torch.zeros(size=(2,Batch_size, args.nhid)).to(device)\n",
    "        output, (h_n, c_n) = self.lstm(input_1.to(device), (h_0, c_0))\n",
    "        output, length = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=args.max_seq_length)\n",
    "        \n",
    "        output = output[:,length-1,:].squeeze()\n",
    "        idxs = torch.arange(args.batch_size)\n",
    "        output = output[idxs, idxs, :]\n",
    "        #print(output[:, :10])\n",
    "        output = self.d_model(output.squeeze())\n",
    "        #print(output[:, :10])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-106-bb470e15ce5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#     print(s[sl].shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# print(v[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0msss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ref_types'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslide_deck_embedding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslide_deck_embedding_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'length_ref_types'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0msss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-67f80c2a6c74>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, bb, slide_deck_embedding, lengths)\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mh_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslide_deck_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[0mc_0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mBatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnhid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_n\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpad_packed_sequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    662\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    663\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 664\u001b[1;33m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0m\u001b[0;32m    665\u001b[0m                               self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0;32m    666\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input and hidden tensors are not at the same device, found input tensor at cuda:0 and hidden tensor at cpu"
     ]
    }
   ],
   "source": [
    "c = Discriminator()\n",
    "\n",
    "z = torch.randn((args.batch_size, args.latent_vector_dim))\n",
    "s = SortByRefSlide(list(train_loader)[0])\n",
    "# print(len(s))\n",
    "# for sl in s:\n",
    "#     print(sl)\n",
    "#     print(s[sl].shape)\n",
    "# print(v[0])\n",
    "sss = c(x=s['ref_types'], bb = v[0], slide_deck_embedding=torch.randn((args.batch_size, args.slide_deck_embedding_size)), lengths=s['length_ref_types'])\n",
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch=\"last\"):\n",
    "    torch.save(model.state_dict(),  result_dir / f'{type(model).__name__}_{mode}.ckpt')\n",
    "\n",
    "def load_model(model, epoch=\"last\"):\n",
    "    if os.path.exists(result_dir / f'{type(model).__name__}_{mode}.ckpt'):\n",
    "        model.load_state_dict(torch.load(result_dir / f'{type(model).__name__}_{mode}.ckpt'))\n",
    "\n",
    "def load_model(model, epoch=\"last\"):\n",
    "    if os.path.exists(result_dir / f'{type(model).__name__}_{mode}.ckpt'):\n",
    "        model.load_state_dict(torch.load(result_dir / f'{type(model).__name__}_{mode}.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs and ckpts will be saved in : ..\\results\\trial_0\n"
     ]
    }
   ],
   "source": [
    "num_trial=0\n",
    "result_dir= Path(root) / 'results'\n",
    "parent_dir = result_dir / f'trial_{num_trial}'\n",
    "# while parent_dir.is_dir():\n",
    "#     num_trial = int(parent_dir.name.replace('trial_',''))\n",
    "#     parent_dir = result_dir / f'trial_{num_trial+1}'\n",
    "\n",
    "# Modify parent_dir here if you want to resume from a checkpoint, or to rename directory.\n",
    "# parent_dir = result_dir / 'trial_99'\n",
    "print(f'Logs and ckpts will be saved in : {parent_dir}')\n",
    "\n",
    "log_dir = parent_dir\n",
    "ckpt_dir = parent_dir\n",
    "encoder_ckpt_path = parent_dir / 'encoder.pt'\n",
    "generator_ckpt_path = parent_dir / 'generator.pt'\n",
    "discriminator_ckpt_path = parent_dir / 'discriminator.pt'\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([64, 2])\n",
      "ref_slide torch.Size([64, 8, 5])\n",
      "ref_types torch.Size([64, 8])\n",
      "slide_deck torch.Size([64, 5, 8, 5])\n",
      "lengths_slide_deck torch.Size([64, 5])\n",
      "length_ref_types torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for k in list(train_loader)[0].keys():\n",
    "    print(k,list(train_loader)[0][k].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SlideDeckEncoder().to(device)\n",
    "discriminator = Discriminator(encoder.slide_encoder.embed.weight.data).to(device)\n",
    "generator = Generator(encoder.slide_encoder.embed.weight.data, False).to(device)\n",
    "\n",
    "models = {\n",
    "    \"discriminator\": discriminator,\n",
    "    \"encoder\" : encoder,\n",
    "    \"generator\" : generator,\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    \"discriminator\": torch.optim.RMSprop(models[\"discriminator\"].parameters(), lr=args.lr),\n",
    "    \"generator\": torch.optim.RMSprop(models[\"generator\"].parameters(), lr=args.lr),\n",
    "    \"encoder\" : torch.optim.RMSprop(models[\"encoder\"].parameters(), lr=args.lr)\n",
    "}\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if device == 'cuda:0' else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(models, optimizers, is_train=True, dataloader=None):\n",
    "    batches_done = 0\n",
    "    for epoch in range(args.n_epochs):\n",
    "        total_loss_G = 0\n",
    "        total_loss_D = 0\n",
    "        G_num = 0\n",
    "        D_num = 0\n",
    "        loss_G = 0\n",
    "        loss_D = 0\n",
    "        if is_train:\n",
    "            for model in models:\n",
    "                models[model].train()\n",
    "        else:\n",
    "            for model in models:\n",
    "                models[model].eval()\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch = SortByRefSlide(batch)\n",
    "\n",
    "            # ['shape', 'ref_slide', 'ref_types', 'slide_deck', 'lengths_slide_deck', 'length_ref_types']\n",
    "\n",
    "            # conditioning\n",
    "            x_slide_deck = batch[\"slide_deck\"].to(device)\n",
    "            length_ref = batch[\"length_ref_types\"].to(device)\n",
    "            ref_types = batch[\"ref_types\"].to(device)\n",
    "            ref_slide = batch[\"ref_slide\"].to(device)\n",
    "\n",
    "            x_slide_deck = torch.transpose(x_slide_deck, 0, 1)\n",
    "            x_slide_deck = torch.transpose(x_slide_deck, 1, 2)\n",
    "            lengths_slide_deck = torch.transpose(batch[\"lengths_slide_deck\"], 0, 1).to(device)\n",
    "            \n",
    "            optimizers[\"encoder\"].zero_grad()\n",
    "            optimizers[\"discriminator\"].zero_grad()\n",
    "\n",
    "            slide_deck_embedding = models['encoder'](x_slide_deck, lengths_slide_deck)\n",
    "            \n",
    "            batch_size, _ = ref_types.shape\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            z = torch.autograd.Variable(Tensor(np.random.normal(0, 1, (batch_size, args.latent_vector_dim))))\n",
    "            \n",
    "            #   x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "            #     z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "            #     slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            #     length (tensor): (Batch_size,)\n",
    "            # (batch_size, seq, 4)\n",
    "            \n",
    "            # Configure input\n",
    "            # both have ref_types\n",
    "            \n",
    "            real_layouts_bbs = ref_slide[:,:,:-1]\n",
    "\n",
    "            fake_layouts_bbs = models['generator'](ref_types, z, slide_deck_embedding, length_ref)[0].detach()\n",
    "\n",
    "    #        print(\"true\", real_layouts_bbs[:,:,:4])\n",
    "   #         print(\"fake\", fake_layouts_bbs[:,:,:4])\n",
    "\n",
    "\n",
    "  #          print(\"true: \", models[\"discriminator\"](ref_types, real_layouts_bbs, slide_deck_embedding, length_ref))\n",
    " #           print(\"false: \", models[\"discriminator\"](ref_types, fake_layouts_bbs, slide_deck_embedding, length_ref))\n",
    "\n",
    "#            break\n",
    "\n",
    "            loss_D = (-torch.mean(models[\"discriminator\"](ref_types, real_layouts_bbs, slide_deck_embedding, length_ref))\n",
    "                + torch.mean(models[\"discriminator\"](ref_types, fake_layouts_bbs, slide_deck_embedding, length_ref)))\n",
    "            #     x (tensor): type labels, (Batch_size, Sequence_size)\n",
    "            #     bb (tensor): (Batch_size, Sequence_size, 4)\n",
    "            #     slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            #     length (tensor): (Batch_size,)\n",
    "            total_loss_D += loss_D.item()\n",
    "            D_num += 1\n",
    "            loss_D.backward()\n",
    "            optimizers[\"discriminator\"].step()\n",
    "            optimizers[\"encoder\"].step()\n",
    "\n",
    "            # Clip weights of discriminator\n",
    "            # for p in models[\"discriminator\"].parameters():\n",
    "            #     p.data.clamp_(-args.clip_value, args.clip_value)\n",
    "\n",
    "            # Train the generator every n_critic iterations\n",
    "            if i % args.n_critic == 0:\n",
    "                optimizers[\"encoder\"].zero_grad()\n",
    "                optimizers[\"generator\"].zero_grad()\n",
    "\n",
    "                slide_deck_embedding = models['encoder'](x_slide_deck, lengths_slide_deck)\n",
    "                layouts_bbs = models['generator'](ref_types, z, slide_deck_embedding, length_ref)[0]\n",
    "                \n",
    "                # Adversarial loss\n",
    "                loss_G = -torch.mean(models[\"discriminator\"](ref_types, layouts_bbs, slide_deck_embedding, length_ref))\n",
    "                total_loss_G += loss_G.item()\n",
    "                G_num += 1\n",
    "                loss_G.backward()\n",
    "                optimizers[\"generator\"].step()\n",
    "                optimizers[\"encoder\"].step()\n",
    "        \n",
    "            #if batches_done % args.sample_interval == 0:\n",
    "            batches_done += 1\n",
    "           \n",
    "        print(\n",
    "            \"[Epoch %d/%d] [D loss: %f | Steps: %d] [G loss: %f Steps: %d]\"\n",
    "            % (epoch, args.n_epochs, total_loss_D/D_num, D_num, total_loss_G/G_num, G_num)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [D loss: -0.006190 | Steps: 83] [G loss: 0.130889 Steps: 42]\n",
      "[Epoch 1/200] [D loss: 0.002810 | Steps: 83] [G loss: 0.084845 Steps: 42]\n",
      "[Epoch 2/200] [D loss: 0.001768 | Steps: 83] [G loss: 0.081954 Steps: 42]\n",
      "[Epoch 3/200] [D loss: -0.001177 | Steps: 83] [G loss: 0.081965 Steps: 42]\n",
      "[Epoch 4/200] [D loss: -0.001507 | Steps: 83] [G loss: 0.076024 Steps: 42]\n",
      "[Epoch 5/200] [D loss: 0.000411 | Steps: 83] [G loss: 0.068442 Steps: 42]\n",
      "[Epoch 6/200] [D loss: -0.000185 | Steps: 83] [G loss: 0.058063 Steps: 42]\n",
      "[Epoch 7/200] [D loss: -0.002667 | Steps: 83] [G loss: 0.046102 Steps: 42]\n",
      "[Epoch 8/200] [D loss: 0.000406 | Steps: 83] [G loss: 0.033990 Steps: 42]\n",
      "[Epoch 9/200] [D loss: 0.000384 | Steps: 83] [G loss: 0.036905 Steps: 42]\n",
      "[Epoch 10/200] [D loss: 0.000034 | Steps: 83] [G loss: 0.041047 Steps: 42]\n",
      "[Epoch 11/200] [D loss: -0.000057 | Steps: 83] [G loss: 0.035779 Steps: 42]\n",
      "[Epoch 12/200] [D loss: -0.001177 | Steps: 83] [G loss: 0.032657 Steps: 42]\n",
      "[Epoch 13/200] [D loss: -0.000745 | Steps: 83] [G loss: 0.023953 Steps: 42]\n",
      "[Epoch 14/200] [D loss: 0.000321 | Steps: 83] [G loss: 0.019175 Steps: 42]\n",
      "[Epoch 15/200] [D loss: 0.000050 | Steps: 83] [G loss: 0.016275 Steps: 42]\n",
      "[Epoch 16/200] [D loss: -0.000407 | Steps: 83] [G loss: 0.021344 Steps: 42]\n",
      "[Epoch 17/200] [D loss: -0.000330 | Steps: 83] [G loss: 0.016544 Steps: 42]\n",
      "[Epoch 18/200] [D loss: -0.000153 | Steps: 83] [G loss: 0.008700 Steps: 42]\n",
      "[Epoch 19/200] [D loss: -0.000197 | Steps: 83] [G loss: 0.001026 Steps: 42]\n",
      "[Epoch 20/200] [D loss: -0.000425 | Steps: 83] [G loss: -0.000784 Steps: 42]\n",
      "[Epoch 21/200] [D loss: -0.000124 | Steps: 83] [G loss: -0.004157 Steps: 42]\n",
      "[Epoch 22/200] [D loss: -0.000012 | Steps: 83] [G loss: -0.005021 Steps: 42]\n",
      "[Epoch 23/200] [D loss: -0.000332 | Steps: 83] [G loss: -0.004519 Steps: 42]\n",
      "[Epoch 24/200] [D loss: -0.000093 | Steps: 83] [G loss: -0.004936 Steps: 42]\n",
      "[Epoch 25/200] [D loss: -0.000317 | Steps: 83] [G loss: -0.007529 Steps: 42]\n",
      "[Epoch 26/200] [D loss: -0.000698 | Steps: 83] [G loss: -0.003404 Steps: 42]\n",
      "[Epoch 27/200] [D loss: -0.001350 | Steps: 83] [G loss: -0.002501 Steps: 42]\n",
      "[Epoch 28/200] [D loss: -0.000661 | Steps: 83] [G loss: -0.004087 Steps: 42]\n",
      "[Epoch 29/200] [D loss: -0.000428 | Steps: 83] [G loss: -0.002654 Steps: 42]\n",
      "[Epoch 30/200] [D loss: -0.001219 | Steps: 83] [G loss: -0.006755 Steps: 42]\n",
      "[Epoch 31/200] [D loss: -0.000644 | Steps: 83] [G loss: -0.009124 Steps: 42]\n",
      "[Epoch 32/200] [D loss: -0.001405 | Steps: 83] [G loss: -0.012766 Steps: 42]\n",
      "[Epoch 33/200] [D loss: -0.000848 | Steps: 83] [G loss: -0.013842 Steps: 42]\n",
      "[Epoch 34/200] [D loss: -0.000704 | Steps: 83] [G loss: -0.015614 Steps: 42]\n",
      "[Epoch 35/200] [D loss: -0.001735 | Steps: 83] [G loss: -0.017076 Steps: 42]\n",
      "[Epoch 36/200] [D loss: -0.001666 | Steps: 83] [G loss: -0.014241 Steps: 42]\n",
      "[Epoch 37/200] [D loss: -0.000380 | Steps: 83] [G loss: -0.016259 Steps: 42]\n",
      "[Epoch 38/200] [D loss: -0.000894 | Steps: 83] [G loss: -0.013554 Steps: 42]\n",
      "[Epoch 39/200] [D loss: -0.001111 | Steps: 83] [G loss: -0.012947 Steps: 42]\n",
      "[Epoch 40/200] [D loss: -0.001399 | Steps: 83] [G loss: -0.015342 Steps: 42]\n",
      "[Epoch 41/200] [D loss: -0.000377 | Steps: 83] [G loss: -0.015636 Steps: 42]\n",
      "[Epoch 42/200] [D loss: -0.000947 | Steps: 83] [G loss: -0.017547 Steps: 42]\n",
      "[Epoch 43/200] [D loss: -0.000478 | Steps: 83] [G loss: -0.012912 Steps: 42]\n",
      "[Epoch 44/200] [D loss: -0.000313 | Steps: 83] [G loss: -0.006778 Steps: 42]\n",
      "[Epoch 45/200] [D loss: -0.002422 | Steps: 83] [G loss: -0.009449 Steps: 42]\n",
      "[Epoch 46/200] [D loss: -0.001795 | Steps: 83] [G loss: -0.004935 Steps: 42]\n",
      "[Epoch 47/200] [D loss: -0.000642 | Steps: 83] [G loss: -0.020333 Steps: 42]\n",
      "[Epoch 48/200] [D loss: -0.001399 | Steps: 83] [G loss: -0.011987 Steps: 42]\n",
      "[Epoch 49/200] [D loss: -0.000710 | Steps: 83] [G loss: -0.017271 Steps: 42]\n",
      "[Epoch 50/200] [D loss: -0.000944 | Steps: 83] [G loss: -0.015667 Steps: 42]\n",
      "[Epoch 51/200] [D loss: -0.003225 | Steps: 83] [G loss: -0.008442 Steps: 42]\n",
      "[Epoch 52/200] [D loss: -0.000346 | Steps: 83] [G loss: -0.013998 Steps: 42]\n",
      "[Epoch 53/200] [D loss: -0.003020 | Steps: 83] [G loss: -0.006344 Steps: 42]\n",
      "[Epoch 54/200] [D loss: -0.002065 | Steps: 83] [G loss: 0.000076 Steps: 42]\n",
      "[Epoch 55/200] [D loss: -0.002305 | Steps: 83] [G loss: 0.005821 Steps: 42]\n",
      "[Epoch 56/200] [D loss: -0.003218 | Steps: 83] [G loss: 0.002190 Steps: 42]\n",
      "[Epoch 57/200] [D loss: -0.001149 | Steps: 83] [G loss: -0.002015 Steps: 42]\n",
      "[Epoch 58/200] [D loss: -0.003295 | Steps: 83] [G loss: 0.022686 Steps: 42]\n",
      "[Epoch 59/200] [D loss: -0.001290 | Steps: 83] [G loss: 0.021120 Steps: 42]\n",
      "[Epoch 60/200] [D loss: -0.001317 | Steps: 83] [G loss: 0.032428 Steps: 42]\n",
      "[Epoch 61/200] [D loss: -0.002125 | Steps: 83] [G loss: 0.035045 Steps: 42]\n",
      "[Epoch 62/200] [D loss: -0.001983 | Steps: 83] [G loss: 0.039434 Steps: 42]\n",
      "[Epoch 63/200] [D loss: -0.000636 | Steps: 83] [G loss: 0.037780 Steps: 42]\n",
      "[Epoch 64/200] [D loss: -0.001037 | Steps: 83] [G loss: 0.033294 Steps: 42]\n",
      "[Epoch 65/200] [D loss: -0.002093 | Steps: 83] [G loss: 0.039604 Steps: 42]\n",
      "[Epoch 66/200] [D loss: -0.003222 | Steps: 83] [G loss: 0.058187 Steps: 42]\n",
      "[Epoch 67/200] [D loss: -0.002899 | Steps: 83] [G loss: 0.066660 Steps: 42]\n",
      "[Epoch 68/200] [D loss: -0.003401 | Steps: 83] [G loss: 0.070871 Steps: 42]\n",
      "[Epoch 69/200] [D loss: -0.003529 | Steps: 83] [G loss: 0.074380 Steps: 42]\n",
      "[Epoch 70/200] [D loss: -0.003447 | Steps: 83] [G loss: 0.098268 Steps: 42]\n",
      "[Epoch 71/200] [D loss: -0.002947 | Steps: 83] [G loss: 0.081418 Steps: 42]\n",
      "[Epoch 72/200] [D loss: -0.006355 | Steps: 83] [G loss: 0.063268 Steps: 42]\n",
      "[Epoch 73/200] [D loss: -0.003858 | Steps: 83] [G loss: 0.069417 Steps: 42]\n",
      "[Epoch 74/200] [D loss: -0.008639 | Steps: 83] [G loss: 0.080309 Steps: 42]\n",
      "[Epoch 75/200] [D loss: -0.009458 | Steps: 83] [G loss: 0.067450 Steps: 42]\n",
      "[Epoch 76/200] [D loss: -0.012204 | Steps: 83] [G loss: 0.041868 Steps: 42]\n",
      "[Epoch 77/200] [D loss: -0.012248 | Steps: 83] [G loss: 0.045255 Steps: 42]\n",
      "[Epoch 78/200] [D loss: -0.010768 | Steps: 83] [G loss: 0.070897 Steps: 42]\n",
      "[Epoch 79/200] [D loss: -0.012714 | Steps: 83] [G loss: 0.103025 Steps: 42]\n",
      "[Epoch 80/200] [D loss: -0.020290 | Steps: 83] [G loss: 0.094452 Steps: 42]\n",
      "[Epoch 81/200] [D loss: -0.012656 | Steps: 83] [G loss: 0.066617 Steps: 42]\n",
      "[Epoch 82/200] [D loss: -0.016593 | Steps: 83] [G loss: 0.118821 Steps: 42]\n",
      "[Epoch 83/200] [D loss: -0.014173 | Steps: 83] [G loss: 0.054616 Steps: 42]\n",
      "[Epoch 84/200] [D loss: -0.018744 | Steps: 83] [G loss: 0.011155 Steps: 42]\n",
      "[Epoch 85/200] [D loss: -0.017058 | Steps: 83] [G loss: -0.006034 Steps: 42]\n",
      "[Epoch 86/200] [D loss: -0.016809 | Steps: 83] [G loss: 0.050967 Steps: 42]\n",
      "[Epoch 87/200] [D loss: -0.020518 | Steps: 83] [G loss: 0.043743 Steps: 42]\n",
      "[Epoch 88/200] [D loss: -0.022825 | Steps: 83] [G loss: 0.039278 Steps: 42]\n",
      "[Epoch 89/200] [D loss: -0.028826 | Steps: 83] [G loss: 0.063293 Steps: 42]\n",
      "[Epoch 90/200] [D loss: -0.035241 | Steps: 83] [G loss: 0.099554 Steps: 42]\n",
      "[Epoch 91/200] [D loss: -0.027046 | Steps: 83] [G loss: 0.024272 Steps: 42]\n",
      "[Epoch 92/200] [D loss: -0.031847 | Steps: 83] [G loss: -0.003538 Steps: 42]\n",
      "[Epoch 93/200] [D loss: -0.024537 | Steps: 83] [G loss: -0.058129 Steps: 42]\n",
      "[Epoch 94/200] [D loss: -0.035480 | Steps: 83] [G loss: -0.055608 Steps: 42]\n",
      "[Epoch 95/200] [D loss: -0.030307 | Steps: 83] [G loss: -0.000579 Steps: 42]\n",
      "[Epoch 96/200] [D loss: -0.041927 | Steps: 83] [G loss: 0.105211 Steps: 42]\n",
      "[Epoch 97/200] [D loss: -0.035386 | Steps: 83] [G loss: 0.011098 Steps: 42]\n",
      "[Epoch 98/200] [D loss: -0.031665 | Steps: 83] [G loss: -0.125437 Steps: 42]\n",
      "[Epoch 99/200] [D loss: -0.021028 | Steps: 83] [G loss: -0.185520 Steps: 42]\n",
      "[Epoch 100/200] [D loss: -0.038229 | Steps: 83] [G loss: 0.082901 Steps: 42]\n",
      "[Epoch 101/200] [D loss: -0.030436 | Steps: 83] [G loss: 0.106668 Steps: 42]\n",
      "[Epoch 102/200] [D loss: -0.036704 | Steps: 83] [G loss: 0.068967 Steps: 42]\n",
      "[Epoch 103/200] [D loss: -0.058828 | Steps: 83] [G loss: -0.020165 Steps: 42]\n",
      "[Epoch 104/200] [D loss: -0.046954 | Steps: 83] [G loss: -0.119773 Steps: 42]\n",
      "[Epoch 105/200] [D loss: -0.058115 | Steps: 83] [G loss: -0.196304 Steps: 42]\n",
      "[Epoch 106/200] [D loss: -0.063431 | Steps: 83] [G loss: -0.024149 Steps: 42]\n",
      "[Epoch 107/200] [D loss: -0.069685 | Steps: 83] [G loss: 0.134858 Steps: 42]\n",
      "[Epoch 108/200] [D loss: -0.076013 | Steps: 83] [G loss: 0.172300 Steps: 42]\n",
      "[Epoch 109/200] [D loss: -0.083323 | Steps: 83] [G loss: 0.148554 Steps: 42]\n",
      "[Epoch 110/200] [D loss: -0.102738 | Steps: 83] [G loss: 0.018157 Steps: 42]\n",
      "[Epoch 111/200] [D loss: -0.088734 | Steps: 83] [G loss: -0.097001 Steps: 42]\n",
      "[Epoch 112/200] [D loss: -0.114245 | Steps: 83] [G loss: -0.265241 Steps: 42]\n",
      "[Epoch 113/200] [D loss: -0.152331 | Steps: 83] [G loss: -0.445043 Steps: 42]\n",
      "[Epoch 114/200] [D loss: -0.196226 | Steps: 83] [G loss: -0.177640 Steps: 42]\n",
      "[Epoch 115/200] [D loss: -0.138508 | Steps: 83] [G loss: -0.162332 Steps: 42]\n",
      "[Epoch 116/200] [D loss: -0.235703 | Steps: 83] [G loss: -0.093324 Steps: 42]\n",
      "[Epoch 117/200] [D loss: -0.238460 | Steps: 83] [G loss: -0.102929 Steps: 42]\n",
      "[Epoch 118/200] [D loss: -0.244081 | Steps: 83] [G loss: -0.446582 Steps: 42]\n",
      "[Epoch 119/200] [D loss: -0.245999 | Steps: 83] [G loss: -0.207627 Steps: 42]\n",
      "[Epoch 120/200] [D loss: -0.327812 | Steps: 83] [G loss: -0.194577 Steps: 42]\n",
      "[Epoch 121/200] [D loss: -0.340539 | Steps: 83] [G loss: 0.148018 Steps: 42]\n",
      "[Epoch 122/200] [D loss: -0.330842 | Steps: 83] [G loss: -0.104454 Steps: 42]\n",
      "[Epoch 123/200] [D loss: -0.386937 | Steps: 83] [G loss: 0.115467 Steps: 42]\n",
      "[Epoch 124/200] [D loss: -0.473655 | Steps: 83] [G loss: -0.095297 Steps: 42]\n",
      "[Epoch 125/200] [D loss: -0.423087 | Steps: 83] [G loss: -0.419884 Steps: 42]\n",
      "[Epoch 126/200] [D loss: -0.611251 | Steps: 83] [G loss: -0.708407 Steps: 42]\n",
      "[Epoch 127/200] [D loss: -0.512721 | Steps: 83] [G loss: -1.521782 Steps: 42]\n",
      "[Epoch 128/200] [D loss: -0.672573 | Steps: 83] [G loss: -0.921855 Steps: 42]\n",
      "[Epoch 129/200] [D loss: -0.506761 | Steps: 83] [G loss: -0.407426 Steps: 42]\n",
      "[Epoch 130/200] [D loss: -0.646205 | Steps: 83] [G loss: -0.961407 Steps: 42]\n",
      "[Epoch 131/200] [D loss: -0.899212 | Steps: 83] [G loss: -1.641218 Steps: 42]\n",
      "[Epoch 132/200] [D loss: -0.708994 | Steps: 83] [G loss: -1.976880 Steps: 42]\n",
      "[Epoch 133/200] [D loss: -1.182361 | Steps: 83] [G loss: -2.200936 Steps: 42]\n",
      "[Epoch 134/200] [D loss: -0.861425 | Steps: 83] [G loss: -3.164996 Steps: 42]\n",
      "[Epoch 135/200] [D loss: -1.042902 | Steps: 83] [G loss: -4.012981 Steps: 42]\n",
      "[Epoch 136/200] [D loss: -0.925603 | Steps: 83] [G loss: -2.588349 Steps: 42]\n",
      "[Epoch 137/200] [D loss: -1.251318 | Steps: 83] [G loss: -5.100828 Steps: 42]\n",
      "[Epoch 138/200] [D loss: -1.089468 | Steps: 83] [G loss: -2.670487 Steps: 42]\n",
      "[Epoch 139/200] [D loss: -1.336314 | Steps: 83] [G loss: -5.059859 Steps: 42]\n",
      "[Epoch 140/200] [D loss: -1.343191 | Steps: 83] [G loss: -3.026031 Steps: 42]\n",
      "[Epoch 141/200] [D loss: -1.265449 | Steps: 83] [G loss: -4.728501 Steps: 42]\n",
      "[Epoch 142/200] [D loss: -1.549999 | Steps: 83] [G loss: -0.787643 Steps: 42]\n",
      "[Epoch 143/200] [D loss: -1.204447 | Steps: 83] [G loss: -3.873115 Steps: 42]\n",
      "[Epoch 144/200] [D loss: -1.657726 | Steps: 83] [G loss: -3.509500 Steps: 42]\n",
      "[Epoch 145/200] [D loss: -1.400972 | Steps: 83] [G loss: -5.010742 Steps: 42]\n",
      "[Epoch 146/200] [D loss: -1.602536 | Steps: 83] [G loss: -4.934414 Steps: 42]\n",
      "[Epoch 147/200] [D loss: -1.569145 | Steps: 83] [G loss: -3.406319 Steps: 42]\n",
      "[Epoch 148/200] [D loss: -1.216905 | Steps: 83] [G loss: -7.518493 Steps: 42]\n",
      "[Epoch 149/200] [D loss: -1.204044 | Steps: 83] [G loss: -5.826062 Steps: 42]\n",
      "[Epoch 150/200] [D loss: -1.096931 | Steps: 83] [G loss: -10.512743 Steps: 42]\n",
      "[Epoch 151/200] [D loss: -1.290912 | Steps: 83] [G loss: -14.588897 Steps: 42]\n",
      "[Epoch 152/200] [D loss: -1.460145 | Steps: 83] [G loss: -9.785814 Steps: 42]\n",
      "[Epoch 153/200] [D loss: -0.922603 | Steps: 83] [G loss: -14.467007 Steps: 42]\n",
      "[Epoch 154/200] [D loss: -0.943224 | Steps: 83] [G loss: -16.948041 Steps: 42]\n",
      "[Epoch 155/200] [D loss: -0.908584 | Steps: 83] [G loss: -12.136878 Steps: 42]\n",
      "[Epoch 156/200] [D loss: -0.728652 | Steps: 83] [G loss: -12.493836 Steps: 42]\n",
      "[Epoch 157/200] [D loss: -0.537233 | Steps: 83] [G loss: -10.058498 Steps: 42]\n",
      "[Epoch 158/200] [D loss: -0.694424 | Steps: 83] [G loss: -12.851720 Steps: 42]\n",
      "[Epoch 159/200] [D loss: -1.010310 | Steps: 83] [G loss: -15.198644 Steps: 42]\n",
      "[Epoch 160/200] [D loss: -0.586107 | Steps: 83] [G loss: -16.157194 Steps: 42]\n",
      "[Epoch 161/200] [D loss: -0.678717 | Steps: 83] [G loss: -18.813740 Steps: 42]\n",
      "[Epoch 162/200] [D loss: -0.641192 | Steps: 83] [G loss: -25.143796 Steps: 42]\n",
      "[Epoch 163/200] [D loss: 0.116903 | Steps: 83] [G loss: -32.569619 Steps: 42]\n",
      "[Epoch 164/200] [D loss: 0.065419 | Steps: 83] [G loss: -29.108712 Steps: 42]\n",
      "[Epoch 165/200] [D loss: 0.522008 | Steps: 83] [G loss: -39.147102 Steps: 42]\n",
      "[Epoch 166/200] [D loss: -0.049546 | Steps: 83] [G loss: -38.498239 Steps: 42]\n",
      "[Epoch 167/200] [D loss: -0.473045 | Steps: 83] [G loss: -37.713658 Steps: 42]\n",
      "[Epoch 168/200] [D loss: -0.037735 | Steps: 83] [G loss: -37.899687 Steps: 42]\n",
      "[Epoch 169/200] [D loss: -0.055537 | Steps: 83] [G loss: -33.792796 Steps: 42]\n",
      "[Epoch 170/200] [D loss: -0.289515 | Steps: 83] [G loss: -32.820854 Steps: 42]\n",
      "[Epoch 171/200] [D loss: -0.012905 | Steps: 83] [G loss: -39.846517 Steps: 42]\n",
      "[Epoch 172/200] [D loss: -0.081117 | Steps: 83] [G loss: -40.468781 Steps: 42]\n",
      "[Epoch 173/200] [D loss: -0.481088 | Steps: 83] [G loss: -42.910604 Steps: 42]\n",
      "[Epoch 174/200] [D loss: 0.054677 | Steps: 83] [G loss: -38.603396 Steps: 42]\n",
      "[Epoch 175/200] [D loss: -0.266080 | Steps: 83] [G loss: -43.591304 Steps: 42]\n",
      "[Epoch 176/200] [D loss: -0.114449 | Steps: 83] [G loss: -46.726576 Steps: 42]\n",
      "[Epoch 177/200] [D loss: -0.290563 | Steps: 83] [G loss: -42.565813 Steps: 42]\n",
      "[Epoch 178/200] [D loss: -0.277278 | Steps: 83] [G loss: -39.929422 Steps: 42]\n",
      "[Epoch 179/200] [D loss: -0.526407 | Steps: 83] [G loss: -37.614668 Steps: 42]\n",
      "[Epoch 180/200] [D loss: -0.317886 | Steps: 83] [G loss: -19.769634 Steps: 42]\n",
      "[Epoch 181/200] [D loss: 0.391202 | Steps: 83] [G loss: -16.196461 Steps: 42]\n",
      "[Epoch 182/200] [D loss: -0.839952 | Steps: 83] [G loss: -22.718720 Steps: 42]\n",
      "[Epoch 183/200] [D loss: 0.690269 | Steps: 83] [G loss: -34.747641 Steps: 42]\n",
      "[Epoch 184/200] [D loss: -0.090733 | Steps: 83] [G loss: -38.979976 Steps: 42]\n",
      "[Epoch 185/200] [D loss: -0.507529 | Steps: 83] [G loss: -39.452651 Steps: 42]\n",
      "[Epoch 186/200] [D loss: -0.211472 | Steps: 83] [G loss: -33.386324 Steps: 42]\n",
      "[Epoch 187/200] [D loss: 0.018515 | Steps: 83] [G loss: -35.937876 Steps: 42]\n",
      "[Epoch 188/200] [D loss: -0.145405 | Steps: 83] [G loss: -37.506929 Steps: 42]\n",
      "[Epoch 189/200] [D loss: -0.509581 | Steps: 83] [G loss: -39.090611 Steps: 42]\n",
      "[Epoch 190/200] [D loss: -0.248961 | Steps: 83] [G loss: -36.523116 Steps: 42]\n",
      "[Epoch 191/200] [D loss: -0.025089 | Steps: 83] [G loss: -45.049944 Steps: 42]\n",
      "[Epoch 192/200] [D loss: -0.212378 | Steps: 83] [G loss: -38.805825 Steps: 42]\n",
      "[Epoch 193/200] [D loss: -0.458340 | Steps: 83] [G loss: -41.717976 Steps: 42]\n",
      "[Epoch 194/200] [D loss: -0.192237 | Steps: 83] [G loss: -41.388448 Steps: 42]\n",
      "[Epoch 195/200] [D loss: -0.070623 | Steps: 83] [G loss: -44.158934 Steps: 42]\n",
      "[Epoch 196/200] [D loss: -0.526451 | Steps: 83] [G loss: -45.002186 Steps: 42]\n",
      "[Epoch 197/200] [D loss: -0.155453 | Steps: 83] [G loss: -48.531315 Steps: 42]\n",
      "[Epoch 198/200] [D loss: -0.217123 | Steps: 83] [G loss: -51.431216 Steps: 42]\n",
      "[Epoch 199/200] [D loss: -0.060032 | Steps: 83] [G loss: -50.930574 Steps: 42]\n"
     ]
    }
   ],
   "source": [
    "run_epoch(models, optimizers, True, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    models[model].eval()\n",
    "\n",
    "batch = list(test_loader)[0]\n",
    "batch = SortByRefSlide(batch)\n",
    "x_slide_deck = batch[\"slide_deck\"].to(device)\n",
    "length_ref = batch[\"length_ref_types\"].to(device)\n",
    "ref_types = batch[\"ref_types\"].to(device)\n",
    "ref_slide = batch[\"ref_slide\"].to(device)\n",
    "\n",
    "x_slide_deck = torch.transpose(x_slide_deck, 0, 1)\n",
    "x_slide_deck = torch.transpose(x_slide_deck, 1, 2)\n",
    "lengths_slide_deck = torch.transpose(batch[\"lengths_slide_deck\"], 0, 1).to(device)\n",
    "\n",
    "optimizers[\"encoder\"].zero_grad()\n",
    "optimizers[\"discriminator\"].zero_grad()\n",
    "\n",
    "slide_deck_embedding = models['encoder'](x_slide_deck, lengths_slide_deck)\n",
    "\n",
    "batch_size, _ = ref_types.shape\n",
    "\n",
    "# Sample noise as generator input\n",
    "z = torch.autograd.Variable(Tensor(np.random.normal(0, 1, (batch_size, args.latent_vector_dim))))\n",
    "fake_layouts_bbs = models['generator'](ref_types, z, slide_deck_embedding, length_ref)[0].detach()\n",
    "real_layouts_bbs = ref_slide[:,:,:-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_bbs((1,1),real_layouts_bbs[4])\n",
    "draw_bbs((1,1), fake_layouts_bbs[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45dc61766396859fdffae760accc4b74216ea8fbbed6d1a9d5e8fb1914e35062"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
