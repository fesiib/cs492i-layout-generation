{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import pi\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "from matplotlib.figure import Figure\n",
    "import numpy as np\n",
    "\n",
    "fig = Figure()\n",
    "canvas = FigureCanvas(fig)\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.text(0.0,0.0,\"Test\", fontsize=45)\n",
    "ax.axis('off')\n",
    "\n",
    "canvas.draw()       # draw the canvas, cache the renderer\n",
    "\n",
    "image = np.frombuffer(canvas.tostring_rgb(), dtype='uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "from matplotlib.figure import Figure\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "fig = Figure(figsize=(5, 4), dpi=100)\n",
    "# A canvas must be manually attached to the figure (pyplot would automatically\n",
    "# do it).  This is done by instantiating the canvas with the figure as\n",
    "# argument.\n",
    "canvas = FigureCanvasAgg(fig)\n",
    "\n",
    "# Do some plotting.\n",
    "ax = fig.add_subplot()\n",
    "ax.plot([1, 2, 3])\n",
    "\n",
    "# Option 1: Save the figure to a file; can also be a file-like object (BytesIO,\n",
    "# etc.).\n",
    "# fig.savefig(\"test.png\")\n",
    "\n",
    "# Option 2: Retrieve a memoryview on the renderer buffer, and convert it to a\n",
    "# numpy array.\n",
    "canvas.draw()\n",
    "rgba = np.asarray(canvas.buffer_rgba())\n",
    "# ... and pass it to PIL.\n",
    "im = Image.fromarray(rgba)\n",
    "# This image can then be saved to any format supported by Pillow, e.g.:\n",
    "# im.save(\"test.bmp\")\n",
    "\n",
    "# Uncomment this line to display the image using ImageMagick's `display` tool.\n",
    "# im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=500x400 at 0x1F6CFE45070>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.fromarray(rgba[:,:,:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chika\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\skimage\\io\\_plugins\\matplotlib_plugin.py:150: UserWarning: Low image data range; displaying image with stretched contrast.\n",
      "  lo, hi, cmap = _get_display_range(image)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (921600,) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-78e0a411097a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\skimage\\io\\_io.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(arr, plugin, **plugin_args)\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'imread'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'imshow'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\skimage\\io\\manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[1;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m                                (plugin, kind))\n\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\skimage\\io\\_plugins\\matplotlib_plugin.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(image, ax, show_cbar, **kwargs)\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m     \u001b[0max_im\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcmap\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0m_default_colormap\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mshow_cbar\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mshow_cbar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mdivider\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_axes_locatable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36minner\u001b[1;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1438\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[0mbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[0;32m   5521\u001b[0m                               resample=resample, **kwargs)\n\u001b[0;32m   5522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5523\u001b[1;33m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5524\u001b[0m         \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5525\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\SlideStyleTransfer\\lib\\site-packages\\matplotlib\\image.py\u001b[0m in \u001b[0;36mset_data\u001b[1;34m(self, A)\u001b[0m\n\u001b[0;32m    707\u001b[0m         if not (self._A.ndim == 2\n\u001b[0;32m    708\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m--> 709\u001b[1;33m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0m\u001b[0;32m    710\u001b[0m                             .format(self._A.shape))\n\u001b[0;32m    711\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Invalid shape (921600,) for image data"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import skimage\n",
    "skimage.io.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 224])\n"
     ]
    }
   ],
   "source": [
    "input_embedding_num = 64\n",
    "slide_deck_embedding_num = 128\n",
    "output_label_num = 4\n",
    "hidden_state_num = 128\n",
    "content_num = 32\n",
    "batch_size = 2\n",
    "\n",
    "input_embedding = torch.randn((batch_size, input_embedding_num))\n",
    "slide_deck_embedding = torch.randn((batch_size, slide_deck_embedding_num))\n",
    "content = torch.randn((batch_size, content_num))\n",
    "\n",
    "x = torch.cat([input_embedding, slide_deck_embedding, content], 1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BoxGenerator(torch.nn.module):\n",
    "    def __init__(self, input_embedding_num, slide_deck_embedding_num, output_label_num, hidden_state_num, content_num, K=2):\n",
    "        self.lstm = nn.LSTM(input_size=input_embedding_num + slide_deck_embedding_num + content_num, hidden_size=hidden_state_num, batch_first=True)\n",
    "        self.W1 = nn.Linear(hidden_state_num + content_num, output_label_num)\n",
    "        self.W2 = nn.Linear(output_label_num + hidden_state_num, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_1 = self.lstm(x)\n",
    "        temp = torch.cat(W1,)\n",
    "        label = self.W1(output_1, x[:,])\n",
    "\n",
    "        \n",
    "\n",
    "class GaussianMixture(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Fits a mixture of k=1,..,K Gaussians to the input data (K is supplied via n_components).\n",
    "    Input tensors are expected to be flat with dimensions (n: number of samples, d: number of features).\n",
    "    The model then extends them to (n, 1, d).\n",
    "    The model parametrization (mu, sigma) is stored as (1, k, d),\n",
    "    probabilities are shaped (n, k, 1) if they relate to an individual sample,\n",
    "    or (1, k, 1) if they assign membership probabilities to one of the mixture components.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components, n_features, covariance_type=\"full\", eps=1.e-6, init_params=\"kmeans\", mu_init=None, var_init=None):\n",
    "        \"\"\"\n",
    "        Initializes the model and brings all tensors into their required shape.\n",
    "        The class expects data to be fed as a flat tensor in (n, d).\n",
    "        The class owns:\n",
    "            x:               torch.Tensor (n, 1, d)\n",
    "            mu:              torch.Tensor (1, k, d)\n",
    "            var:             torch.Tensor (1, k, d) or (1, k, d, d)\n",
    "            pi:              torch.Tensor (1, k, 1)\n",
    "            covariance_type: str\n",
    "            eps:             float\n",
    "            init_params:     str\n",
    "            log_likelihood:  float\n",
    "            n_components:    int\n",
    "            n_features:      int\n",
    "        args:\n",
    "            n_components:    int\n",
    "            n_features:      int\n",
    "        options:\n",
    "            mu_init:         torch.Tensor (1, k, d)\n",
    "            var_init:        torch.Tensor (1, k, d) or (1, k, d, d)\n",
    "            covariance_type: str\n",
    "            eps:             float\n",
    "            init_params:     str\n",
    "        \"\"\"\n",
    "        super(GaussianMixture, self).__init__()\n",
    "\n",
    "        self.n_components = n_components\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.mu_init = mu_init\n",
    "        self.var_init = var_init\n",
    "        self.eps = eps\n",
    "\n",
    "        self.log_likelihood = -np.inf\n",
    "\n",
    "        self.covariance_type = covariance_type\n",
    "        self.init_params = init_params\n",
    "\n",
    "        assert self.covariance_type in [\"full\", \"diag\"]\n",
    "        assert self.init_params in [\"kmeans\", \"random\"]\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "\n",
    "    def _init_params(self):\n",
    "        if self.mu_init is not None:\n",
    "            assert self.mu_init.size() == (1, self.n_components, self.n_features), \"Input mu_init does not have required tensor dimensions (1, %i, %i)\" % (self.n_components, self.n_features)\n",
    "            # (1, k, d)\n",
    "            self.mu = torch.nn.Parameter(self.mu_init, requires_grad=False)\n",
    "        else:\n",
    "            self.mu = torch.nn.Parameter(torch.randn(1, self.n_components, self.n_features), requires_grad=False)\n",
    "\n",
    "        if self.covariance_type == \"diag\":\n",
    "            if self.var_init is not None:\n",
    "                # (1, k, d)\n",
    "                assert self.var_init.size() == (1, self.n_components, self.n_features), \"Input var_init does not have required tensor dimensions (1, %i, %i)\" % (self.n_components, self.n_features)\n",
    "                self.var = torch.nn.Parameter(self.var_init, requires_grad=False)\n",
    "            else:\n",
    "                self.var = torch.nn.Parameter(torch.ones(1, self.n_components, self.n_features), requires_grad=False)\n",
    "        elif self.covariance_type == \"full\":\n",
    "            if self.var_init is not None:\n",
    "                # (1, k, d, d)\n",
    "                assert self.var_init.size() == (1, self.n_components, self.n_features, self.n_features), \"Input var_init does not have required tensor dimensions (1, %i, %i, %i)\" % (self.n_components, self.n_features, self.n_features)\n",
    "                self.var = torch.nn.Parameter(self.var_init, requires_grad=False,)\n",
    "            else:\n",
    "                self.var = torch.nn.Parameter(\n",
    "                    torch.eye(self.n_features,dtype=torch.float64).reshape(1, 1, self.n_features, self.n_features).repeat(1, self.n_components, 1, 1),\n",
    "                    requires_grad=False)\n",
    "\n",
    "        # (1, k, 1)\n",
    "        self.pi = torch.nn.Parameter(torch.Tensor(1, self.n_components, 1), requires_grad=False).fill_(1. / self.n_components)\n",
    "\n",
    "        self.params_fitted = False\n",
    "\n",
    "\n",
    "    def check_size(self, x):\n",
    "        if len(x.size()) == 2:\n",
    "            # (n, d) --> (n, 1, d)\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def bic(self, x):\n",
    "        \"\"\"\n",
    "        Bayesian information criterion for a batch of samples.\n",
    "        args:\n",
    "            x:      torch.Tensor (n, d) or (n, 1, d)\n",
    "        returns:\n",
    "            bic:    float\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Free parameters for covariance, means and mixture components\n",
    "        free_params = self.n_features * self.n_components + self.n_features + self.n_components - 1\n",
    "\n",
    "        bic = -2. * self.__score(x, as_average=False).mean() * n + free_params * np.log(n)\n",
    "\n",
    "        return bic\n",
    "\n",
    "\n",
    "    def fit(self, x, delta=1e-3, n_iter=100, warm_start=False):\n",
    "        \"\"\"\n",
    "        Fits model to the data.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, d) or (n, k, d)\n",
    "        options:\n",
    "            delta:      float\n",
    "            n_iter:     int\n",
    "            warm_start: bool\n",
    "        \"\"\"\n",
    "        if not warm_start and self.params_fitted:\n",
    "            self._init_params()\n",
    "\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        if self.init_params == \"kmeans\" and self.mu_init is None:\n",
    "            mu = self.get_kmeans_mu(x, n_centers=self.n_components)\n",
    "            self.mu.data = mu\n",
    "\n",
    "        i = 0\n",
    "        j = np.inf\n",
    "\n",
    "        while (i <= n_iter) and (j >= delta):\n",
    "\n",
    "            log_likelihood_old = self.log_likelihood\n",
    "            mu_old = self.mu\n",
    "            var_old = self.var\n",
    "\n",
    "            self.__em(x)\n",
    "            self.log_likelihood = self.__score(x)\n",
    "\n",
    "            if torch.isinf(self.log_likelihood.abs()) or torch.isnan(self.log_likelihood):\n",
    "                device = self.mu.device\n",
    "                # When the log-likelihood assumes inane values, reinitialize model\n",
    "                self.__init__(self.n_components,\n",
    "                    self.n_features,\n",
    "                    covariance_type=self.covariance_type,\n",
    "                    mu_init=self.mu_init,\n",
    "                    var_init=self.var_init,\n",
    "                    eps=self.eps)\n",
    "                for p in self.parameters():\n",
    "                    p.data = p.data.to(device)\n",
    "                if self.init_params == \"kmeans\":\n",
    "                    self.mu.data, = self.get_kmeans_mu(x, n_centers=self.n_components)\n",
    "\n",
    "            i += 1\n",
    "            j = self.log_likelihood - log_likelihood_old\n",
    "\n",
    "            if j <= delta:\n",
    "                # When score decreases, revert to old parameters\n",
    "                self.__update_mu(mu_old)\n",
    "                self.__update_var(var_old)\n",
    "\n",
    "        self.params_fitted = True\n",
    "\n",
    "\n",
    "    def predict(self, x, probs=False):\n",
    "        \"\"\"\n",
    "        Assigns input data to one of the mixture components by evaluating the likelihood under each.\n",
    "        If probs=True returns normalized probabilities of class membership.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, d) or (n, 1, d)\n",
    "            probs:      bool\n",
    "        returns:\n",
    "            p_k:        torch.Tensor (n, k)\n",
    "            (or)\n",
    "            y:          torch.LongTensor (n)\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        weighted_log_prob = self._estimate_log_prob(x) + torch.log(self.pi)\n",
    "\n",
    "        if probs:\n",
    "            p_k = torch.exp(weighted_log_prob)\n",
    "            return torch.squeeze(p_k / (p_k.sum(1, keepdim=True)))\n",
    "        else:\n",
    "            return torch.squeeze(torch.max(weighted_log_prob, 1)[1].type(torch.LongTensor))\n",
    "\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        Returns normalized probabilities of class membership.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, d) or (n, 1, d)\n",
    "        returns:\n",
    "            y:          torch.LongTensor (n)\n",
    "        \"\"\"\n",
    "        return self.predict(x, probs=True)\n",
    "\n",
    "\n",
    "    def score_samples(self, x):\n",
    "        \"\"\"\n",
    "        Computes log-likelihood of samples under the current model.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, d) or (n, 1, d)\n",
    "        returns:\n",
    "            score:      torch.LongTensor (n)\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        score = self.__score(x, as_average=False)\n",
    "        return score\n",
    "\n",
    "\n",
    "    def _estimate_log_prob(self, x):\n",
    "        \"\"\"\n",
    "        Returns a tensor with dimensions (n, k, 1), which indicates the log-likelihood that samples belong to the k-th Gaussian.\n",
    "        args:\n",
    "            x:            torch.Tensor (n, d) or (n, 1, d)\n",
    "        returns:\n",
    "            log_prob:     torch.Tensor (n, k, 1)\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        if self.covariance_type == \"full\":\n",
    "            mu = self.mu\n",
    "            var = self.var\n",
    "            precision = torch.inverse(var)\n",
    "            d = x.shape[-1]\n",
    "\n",
    "            log_2pi = d * np.log(2. * pi)\n",
    "\n",
    "            log_det = self._calculate_log_det(precision)\n",
    "\n",
    "            x = x.double() \n",
    "            mu = mu.double()\n",
    "            x_mu_T = (x - mu).unsqueeze(-2)\n",
    "            x_mu = (x - mu).unsqueeze(-1)\n",
    "\n",
    "            x_mu_T_precision = calculate_matmul_n_times(self.n_components, x_mu_T, precision)\n",
    "            x_mu_T_precision_x_mu = calculate_matmul(x_mu_T_precision, x_mu)\n",
    "\n",
    "            return -.5 * (log_2pi - log_det + x_mu_T_precision_x_mu)\n",
    "\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            mu = self.mu\n",
    "            prec = torch.rsqrt(self.var)\n",
    "\n",
    "            log_p = torch.sum((mu * mu + x * x - 2 * x * mu) * (prec ** 2), dim=2, keepdim=True)\n",
    "            log_det = torch.sum(torch.log(prec), dim=2, keepdim=True)\n",
    "\n",
    "            return -.5 * (self.n_features * np.log(2. * pi) + log_p) + log_det\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_log_det(self, var):\n",
    "        \"\"\"\n",
    "        Calculate log determinant in log space, to prevent overflow errors.\n",
    "        args:\n",
    "            var:            torch.Tensor (1, k, d, d)\n",
    "        \"\"\"\n",
    "        log_det = torch.empty(size=(self.n_components,)).to(var.device)\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            log_det[k] = 2 * torch.log(torch.diagonal(torch.linalg.cholesky(var[0,k]))).sum()\n",
    "\n",
    "        return log_det.unsqueeze(-1)\n",
    "\n",
    "\n",
    "    def _e_step(self, x):\n",
    "        \"\"\"\n",
    "        Computes log-responses that indicate the (logarithmic) posterior belief (sometimes called responsibilities) that a data point was generated by one of the k mixture components.\n",
    "        Also returns the mean of the mean of the logarithms of the probabilities (as is done in sklearn).\n",
    "        This is the so-called expectation step of the EM-algorithm.\n",
    "        args:\n",
    "            x:              torch.Tensor (n, d) or (n, 1, d)\n",
    "        returns:\n",
    "            log_prob_norm:  torch.Tensor (1)\n",
    "            log_resp:       torch.Tensor (n, k, 1)\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        weighted_log_prob = self._estimate_log_prob(x) + torch.log(self.pi)\n",
    "\n",
    "        log_prob_norm = torch.logsumexp(weighted_log_prob, dim=1, keepdim=True)\n",
    "        log_resp = weighted_log_prob - log_prob_norm\n",
    "\n",
    "        return torch.mean(log_prob_norm), log_resp\n",
    "\n",
    "\n",
    "    def _m_step(self, x, log_resp):\n",
    "        \"\"\"\n",
    "        From the log-probabilities, computes new parameters pi, mu, var (that maximize the log-likelihood). This is the maximization step of the EM-algorithm.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, d) or (n, 1, d)\n",
    "            log_resp:   torch.Tensor (n, k, 1)\n",
    "        returns:\n",
    "            pi:         torch.Tensor (1, k, 1)\n",
    "            mu:         torch.Tensor (1, k, d)\n",
    "            var:        torch.Tensor (1, k, d)\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        resp = torch.exp(log_resp)\n",
    "\n",
    "        pi = torch.sum(resp, dim=0, keepdim=True) + self.eps\n",
    "        mu = torch.sum(resp * x, dim=0, keepdim=True) / pi\n",
    "\n",
    "        if self.covariance_type == \"full\":\n",
    "            eps = (torch.eye(self.n_features) * self.eps).to(x.device)\n",
    "            var = torch.sum((x - mu).unsqueeze(-1).matmul((x - mu).unsqueeze(-2)) * resp.unsqueeze(-1), dim=0,\n",
    "                            keepdim=True) / torch.sum(resp, dim=0, keepdim=True).unsqueeze(-1) + eps\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            x2 = (resp * x * x).sum(0, keepdim=True) / pi\n",
    "            mu2 = mu * mu\n",
    "            xmu = (resp * mu * x).sum(0, keepdim=True) / pi\n",
    "            var = x2 - 2 * xmu + mu2 + self.eps\n",
    "\n",
    "        pi = pi / x.shape[0]\n",
    "\n",
    "        return pi, mu, var\n",
    "\n",
    "\n",
    "    def __em(self, x):\n",
    "        \"\"\"\n",
    "        Performs one iteration of the expectation-maximization algorithm by calling the respective subroutines.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, 1, d)\n",
    "        \"\"\"\n",
    "        _, log_resp = self._e_step(x)\n",
    "        pi, mu, var = self._m_step(x, log_resp)\n",
    "\n",
    "        self.__update_pi(pi)\n",
    "        self.__update_mu(mu)\n",
    "        self.__update_var(var)\n",
    "\n",
    "\n",
    "    def __score(self, x, as_average=True):\n",
    "        \"\"\"\n",
    "        Computes the log-likelihood of the data under the model.\n",
    "        args:\n",
    "            x:                  torch.Tensor (n, 1, d)\n",
    "            sum_data:           bool\n",
    "        returns:\n",
    "            score:              torch.Tensor (1)\n",
    "            (or)\n",
    "            per_sample_score:   torch.Tensor (n)\n",
    "        \"\"\"\n",
    "        weighted_log_prob = self._estimate_log_prob(x) + torch.log(self.pi)\n",
    "        per_sample_score = torch.logsumexp(weighted_log_prob, dim=1)\n",
    "\n",
    "        if as_average:\n",
    "            return per_sample_score.mean()\n",
    "        else:\n",
    "            return torch.squeeze(per_sample_score)\n",
    "\n",
    "\n",
    "    def __update_mu(self, mu):\n",
    "        \"\"\"\n",
    "        Updates mean to the provided value.\n",
    "        args:\n",
    "            mu:         torch.FloatTensor\n",
    "        \"\"\"\n",
    "        assert mu.size() in [(self.n_components, self.n_features), (1, self.n_components, self.n_features)], \"Input mu does not have required tensor dimensions (%i, %i) or (1, %i, %i)\" % (self.n_components, self.n_features, self.n_components, self.n_features)\n",
    "\n",
    "        if mu.size() == (self.n_components, self.n_features):\n",
    "            self.mu = mu.unsqueeze(0)\n",
    "        elif mu.size() == (1, self.n_components, self.n_features):\n",
    "            self.mu.data = mu\n",
    "\n",
    "\n",
    "    def __update_var(self, var):\n",
    "        \"\"\"\n",
    "        Updates variance to the provided value.\n",
    "        args:\n",
    "            var:        torch.FloatTensor\n",
    "        \"\"\"\n",
    "        if self.covariance_type == \"full\":\n",
    "            assert var.size() in [(self.n_components, self.n_features, self.n_features), (1, self.n_components, self.n_features, self.n_features)], \"Input var does not have required tensor dimensions (%i, %i, %i) or (1, %i, %i, %i)\" % (self.n_components, self.n_features, self.n_features, self.n_components, self.n_features, self.n_features)\n",
    "\n",
    "            if var.size() == (self.n_components, self.n_features, self.n_features):\n",
    "                self.var = var.unsqueeze(0)\n",
    "            elif var.size() == (1, self.n_components, self.n_features, self.n_features):\n",
    "                self.var.data = var\n",
    "\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            assert var.size() in [(self.n_components, self.n_features), (1, self.n_components, self.n_features)], \"Input var does not have required tensor dimensions (%i, %i) or (1, %i, %i)\" % (self.n_components, self.n_features, self.n_components, self.n_features)\n",
    "\n",
    "            if var.size() == (self.n_components, self.n_features):\n",
    "                self.var = var.unsqueeze(0)\n",
    "            elif var.size() == (1, self.n_components, self.n_features):\n",
    "                self.var.data = var\n",
    "\n",
    "\n",
    "    def __update_pi(self, pi):\n",
    "        \"\"\"\n",
    "        Updates pi to the provided value.\n",
    "        args:\n",
    "            pi:         torch.FloatTensor\n",
    "        \"\"\"\n",
    "        assert pi.size() in [(1, self.n_components, 1)], \"Input pi does not have required tensor dimensions (%i, %i, %i)\" % (1, self.n_components, 1)\n",
    "\n",
    "        self.pi.data = pi\n",
    "\n",
    "\n",
    "    def get_kmeans_mu(self, x, n_centers, init_times=50, min_delta=1e-3):\n",
    "        \"\"\"\n",
    "        Find an initial value for the mean. Requires a threshold min_delta for the k-means algorithm to stop iterating.\n",
    "        The algorithm is repeated init_times often, after which the best centerpoint is returned.\n",
    "        args:\n",
    "            x:            torch.FloatTensor (n, d) or (n, 1, d)\n",
    "            init_times:   init\n",
    "            min_delta:    int\n",
    "        \"\"\"\n",
    "        if len(x.size()) == 3:\n",
    "            x = x.squeeze(1)\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        x = (x - x_min) / (x_max - x_min)\n",
    "        \n",
    "        min_cost = np.inf\n",
    "\n",
    "        for i in range(init_times):\n",
    "            tmp_center = x[np.random.choice(np.arange(x.shape[0]), size=n_centers, replace=False), ...]\n",
    "            l2_dis = torch.norm((x.unsqueeze(1).repeat(1, n_centers, 1) - tmp_center), p=2, dim=2)\n",
    "            l2_cls = torch.argmin(l2_dis, dim=1)\n",
    "\n",
    "            cost = 0\n",
    "            for c in range(n_centers):\n",
    "                cost += torch.norm(x[l2_cls == c] - tmp_center[c], p=2, dim=1).mean()\n",
    "\n",
    "            if cost < min_cost:\n",
    "                min_cost = cost\n",
    "                center = tmp_center\n",
    "\n",
    "        delta = np.inf\n",
    "\n",
    "        while delta > min_delta:\n",
    "            l2_dis = torch.norm((x.unsqueeze(1).repeat(1, n_centers, 1) - center), p=2, dim=2)\n",
    "            l2_cls = torch.argmin(l2_dis, dim=1)\n",
    "            center_old = center.clone()\n",
    "\n",
    "            for c in range(n_centers):\n",
    "                center[c] = x[l2_cls == c].mean(dim=0)\n",
    "\n",
    "            delta = torch.norm((center_old - center), dim=1).max()\n",
    "\n",
    "        return (center.unsqueeze(0)*(x_max - x_min) + x_min)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e04026de45b6381e69cbe3b87200c3876b94cab4b646df382595102aa479834e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('SlideStyleTransfer': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
