{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../'\n",
    "\n",
    "import os, sys\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from functools import cmp_to_key\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import mask_rcnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "#!pip install easydict\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "args = edict()\n",
    "args.batch_size = 2\n",
    "args.nlayers = 2\n",
    "\n",
    "args.embedding_size = 4\n",
    "args.ninp = 4 + args.embedding_size\n",
    "args.nhid = 256 #512\n",
    "\n",
    "\n",
    "args.clip = 1\n",
    "args.lr_lstm = 0.001\n",
    "args.dropout = 0.2\n",
    "args.nhid_attn = 256\n",
    "args.epochs = 20\n",
    "\n",
    "##### Transformer\n",
    "args.nhid_tran = 256\n",
    "args.nhead = 8\n",
    "args.nlayers_transformer = 6\n",
    "args.attn_pdrop = 0.1\n",
    "args.resid_pdrop = 0.1\n",
    "args.embd_pdrop = 0.1\n",
    "args.nff = 4 * args.nhid_tran\n",
    "\n",
    "\n",
    "args.lr_transformer = 0.0001 #1.0\n",
    "args.betas = (0.9, 0.98)\n",
    "\n",
    "args.gpu = True\n",
    "\n",
    "args.tensorboard = False\n",
    "args.train_portion = 0.7\n",
    "args.slide_deck_N = 5\n",
    "args.slide_deck_embedding_size = 512\n",
    "args.padding_idx = 0\n",
    "args.max_seq_length = 8\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "# Create directory name.\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tensorboard:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"{str(result_dir)}\" --samples_per_plugin images=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_TYPES = [\n",
    "    '<pad>',\n",
    "    'title',\n",
    "    'header',\n",
    "    'text box',\n",
    "    'footer',\n",
    "    'picture',\n",
    "    'instructor',\n",
    "    'diagram',\n",
    "    'table',\n",
    "    'figure',\n",
    "    'handwriting',\n",
    "    'chart',\n",
    "    'schematic diagram',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbs(shape, bbs):\n",
    "    if (torch.is_tensor(bbs)):\n",
    "        bbs = np.array(bbs.tolist())\n",
    "    if (torch.is_tensor(shape)):\n",
    "        [h, w] = np.array(shape.tolist())\n",
    "        shape = (h, w)\n",
    "    \n",
    "    h, w = shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    background=patches.Rectangle((0, 0), w, h, linewidth=2, edgecolor='b', facecolor='black')\n",
    "    ax.add_patch(background)\n",
    "    for bb in bbs:\n",
    "        rect = patches.Rectangle((bb[0], bb[1]), bb[2], bb[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.autoscale(True, 'both')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_BB_types(bbs):\n",
    "    return bbs[:, 4]\n",
    "\n",
    "class BBSlideDeckDataset(Dataset):\n",
    "    \"\"\" Slide Deck Dataset but with Bounding Boxes\"\"\"\n",
    "    def __init__(self, slide_deck_data, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.slide_deck_data = slide_deck_data\n",
    "        self.slide_deck_ids = list(self.slide_deck_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slide_deck_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        slide_deck_id = self.slide_deck_ids[idx]\n",
    "        (h, w) = self.slide_deck_data[slide_deck_id][\"shape\"]\n",
    "        lengths_slide_deck = []\n",
    "        slides = []\n",
    "        max_len_bbs = 0\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            lengths_slide_deck.append(len(slide))\n",
    "            if len(slide) > max_len_bbs:\n",
    "                max_len_bbs = len(slide)\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            np_slide = np.zeros((max_len_bbs, 5), dtype=np.double)\n",
    "            for i, bb in enumerate(slide):\n",
    "                np_slide[i] = bb\n",
    "            slides.append(np_slide)\n",
    "\n",
    "        ref_slide = slides[0]\n",
    "        slide_deck = slides[1:]\n",
    "        length_ref_types = lengths_slide_deck.pop(0)\n",
    "        sample = {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": get_BB_types(ref_slide),\n",
    "            \"slide_deck\": np.asarray(slide_deck),\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleBB(object):\n",
    "    \"\"\"Rescale the bounding boxes in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def _resize_single_slide(self, slide, original_shape, new_shape):\n",
    "        h, w = original_shape\n",
    "        new_h, new_w = new_shape\n",
    "        slide = slide * np.array([new_w / w, new_h / h, new_w / w, new_h / h, 1]).T\n",
    "        return slide\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        ref_slide = self._resize_single_slide(ref_slide, (h, w), (new_h, new_w))\n",
    "        for i, slide in enumerate(slide_deck):\n",
    "            slide_deck[i] = self._resize_single_slide(slide, (h, w), (new_h, new_w))\n",
    "\n",
    "        return {\n",
    "            \"shape\": (new_h, new_w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class LeaveN(object):\n",
    "    def __init__ (self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if slide_deck.shape[0] > self.N:\n",
    "            slide_deck = np.delete(slide_deck, range(self.N, slide_deck.shape[0]), 0)\n",
    "            lengths_slide_deck = lengths_slide_deck[:self.N]\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ShuffleRefSlide(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        lengths_slide_deck.append(length_ref_types)\n",
    "        slide_deck = np.vstack((slide_deck, ref_slide[None, :]))\n",
    "\n",
    "        idxs = np.array([*range(0, len(lengths_slide_deck))], dtype=np.int32)\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        slide_deck = slide_deck[idxs]\n",
    "\n",
    "        lengths_slide_deck = np.array(lengths_slide_deck, dtype=np.int32)\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck.tolist()\n",
    "        \n",
    "        slide_deck = slide_deck.tolist()\n",
    "        ref_slide = np.asarray(slide_deck.pop())\n",
    "        length_ref_types = lengths_slide_deck.pop()\n",
    "        ref_types = get_BB_types(ref_slide)\n",
    "\n",
    "        slide_deck = np.asarray(slide_deck)\n",
    "        \n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ToTensorBB(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        idxs = [*range(0, len(lengths_slide_deck))]\n",
    "\n",
    "        def by_length(p1, p2):\n",
    "            return lengths_slide_deck[p2] - lengths_slide_deck[p1]\n",
    "        idxs = sorted(idxs, key=cmp_to_key(by_length))\n",
    "\n",
    "        shape = torch.tensor([h, w], dtype=torch.float64)\n",
    "        ref_slide = torch.from_numpy(ref_slide).float()\n",
    "        ref_types = torch.from_numpy(ref_types).float()\n",
    "        \n",
    "        slide_deck = torch.from_numpy(slide_deck).float()\n",
    "        lengths_slide_deck = torch.tensor(lengths_slide_deck, dtype=torch.int32)\n",
    "        \n",
    "        slide_deck = slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "\n",
    "        length_ref_types = torch.tensor(length_ref_types, dtype=torch.int32)\n",
    "\n",
    "        return {\n",
    "            \"shape\": shape,\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slide_deck_dataset(all_dataset):\n",
    "    slide_deck_data = {}\n",
    "    for entrance in all_dataset.iloc:\n",
    "        slide_deck_id = entrance['Slide Deck Id']\n",
    "        \n",
    "        slide_id = entrance[\"Slide Id\"]\n",
    "        if (slide_deck_id not in slide_deck_data):\n",
    "            slide_deck_data[slide_deck_id] = {\n",
    "                'slides': {},\n",
    "                'shape': (entrance['Image Height'], entrance['Image Width'])\n",
    "            }\n",
    "        \n",
    "        if slide_id not in slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            slide_deck_data[slide_deck_id][\"slides\"][slide_id] = []\n",
    "        bb_type = BB_TYPES.index(entrance['Type'])\n",
    "        if (bb_type < 0 or bb_type >= len(BB_TYPES)):\n",
    "            bb_type = len(BB_TYPES)\n",
    "\n",
    "        bb = np.array([\n",
    "            entrance['X'],\n",
    "            entrance['Y'],\n",
    "            entrance['BB Width'],\n",
    "            entrance['BB Height'],\n",
    "            bb_type\n",
    "        ]).T\n",
    "        slide_deck_data[slide_deck_id]['slides'][slide_id].append(bb)\n",
    "    for key in slide_deck_data.keys():\n",
    "        \n",
    "        # if key == 100:\n",
    "        #     for (id, value) in slide_deck_data[key][\"slides\"].items():\n",
    "        #         print(56, id)\n",
    "        #         draw_bbs(slide_deck_data[key][\"shape\"], value)\n",
    "\n",
    "        values = list(slide_deck_data[key][\"slides\"].values())\n",
    "        slide_deck_data[key][\"slides\"] = [np.asarray(value) for value in values]\n",
    "    return slide_deck_data\n",
    "\n",
    "def slice_dict(dictionary, l, r):\n",
    "    keys = list(dictionary.keys())\n",
    "    keys = keys[l:r]\n",
    "    ret_dictionary = {}\n",
    "    for key in keys:\n",
    "        ret_dictionary[key] = dictionary[key]\n",
    "    return ret_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"slide_deck_dataset.csv\")\n",
    "\n",
    "dataset = pd.read_csv(csv_file)\n",
    "slide_deck_data = process_slide_deck_dataset(dataset)\n",
    "\n",
    "division = int(args.train_portion * len(slide_deck_data))\n",
    "\n",
    "train_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, 0, division),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(args.slide_deck_N),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, division, len(slide_deck_data)),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(args.slide_deck_N),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 3., 3., 3., 0., 0., 0.])\n",
      "tensor([[0.2473, 0.1190, 0.5074, 0.1069, 1.0000],\n",
      "        [0.0578, 0.4118, 0.4015, 0.0685, 3.0000],\n",
      "        [0.0586, 0.5060, 0.4004, 0.2107, 3.0000],\n",
      "        [0.0575, 0.3216, 0.2919, 0.0670, 3.0000],\n",
      "        [0.5274, 0.3241, 0.4178, 0.4476, 7.0000],\n",
      "        [0.0575, 0.5025, 0.4026, 0.2923, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[ 0.0881,  0.1270,  0.8268,  0.0927,  1.0000],\n",
      "        [ 0.0597,  0.3165,  0.4216,  0.4592,  3.0000],\n",
      "        [ 0.5108,  0.3170,  0.4242,  0.5791, 11.0000],\n",
      "        [ 0.0571,  0.3110,  0.4265,  0.5927,  3.0000],\n",
      "        [ 0.0824,  0.8432,  0.2129,  0.0509,  3.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[0.2261, 0.1210, 0.5509, 0.1033, 1.0000],\n",
      "        [0.0514, 0.9138, 0.8594, 0.0418, 3.0000],\n",
      "        [0.0586, 0.6568, 0.8321, 0.2001, 9.0000],\n",
      "        [0.0597, 0.4229, 0.1728, 0.0600, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.2291, 0.3926, 0.5312, 0.3473, 5.0000],\n",
      "        [0.1509, 0.0585, 0.6960, 0.2263, 1.0000],\n",
      "        [0.2669, 0.7959, 0.4590, 0.0580, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0836, 0.3241, 0.7977, 0.3261, 3.0000],\n",
      "        [0.2242, 0.1174, 0.5539, 0.1089, 1.0000],\n",
      "        [0.0858, 0.7364, 0.6900, 0.0776, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([6, 5, 4, 3, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "single = train_slide_deck_dataset[0]\n",
    "draw_bbs(single[\"shape\"], single[\"ref_slide\"])\n",
    "\n",
    "print(single[\"ref_types\"])\n",
    "for i in range(5):\n",
    "    print(single[\"slide_deck\"][i])\n",
    "print(single[\"lengths_slide_deck\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_slide_deck_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_slide_deck_dataset, batch_size=args.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlideEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlideEncoder, self).__init__()\n",
    "        ninp = args.ninp\n",
    "        nhid = args.nhid\n",
    "        nlayers = args.nlayers\n",
    "        dropout = args.dropout\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, args.padding_idx)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, bias=True).float()\n",
    "\n",
    "    def forward(self, x, states, lengths=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor(B, L, 5)\n",
    "            states: List[Tuple(h_0, c_0), ..., Tuple(h_{B-1}, c_{B-1})]\n",
    "            lengths: tensor(B)\n",
    "        \"\"\"\n",
    "        idxs = [*range(0, len(lengths))]\n",
    "        def by_lengths(p1, p2):\n",
    "            return lengths[p2] - lengths[p1]\n",
    "\n",
    "        idxs = sorted(idxs, key=cmp_to_key(by_lengths))\n",
    "\n",
    "        x = x[:, idxs]\n",
    "        lengths = lengths[idxs]\n",
    "        \n",
    "        input = x[:, :, :-1]\n",
    "        types = x[:, :, -1:].long()\n",
    "        types = torch.squeeze(self.embed(types))\n",
    "        input = torch.cat((input, types), dim=-1)\n",
    "\n",
    "        output = self.dropout(input)\n",
    "        \n",
    "\n",
    "        output = torch.nn.utils.rnn.pack_padded_sequence(output, lengths)\n",
    "\n",
    "        h_0 = torch.stack([h for (h, _) in states], dim=0)\n",
    "        c_0 = torch.stack([c for (_, c) in states], dim=0)\n",
    "        \n",
    "        (output, context_vector) = self.lstm(output, (h_0, c_0))\n",
    "        output, lengths = torch.nn.utils.rnn.pad_packed_sequence(output, total_length = args.max_seq_length)\n",
    "        return (output, context_vector)\n",
    "\n",
    "class SlideDeckEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlideDeckEncoder, self).__init__()\n",
    "        self.slide_encoder = SlideEncoder()\n",
    "\n",
    "        input_size = args.nhid * args.slide_deck_N\n",
    "        output_size = args.slide_deck_embedding_size\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        return\n",
    "\n",
    "    def _get_init_states(self, x):\n",
    "        init_states = [\n",
    "            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n",
    "            torch.zeros((x.size(1), args.nhid)).to(x.device))\n",
    "            for _ in range(args.nlayers)\n",
    "        ]\n",
    "        return init_states\n",
    "    \n",
    "    def forward(self, xs, lengths):\n",
    "        states = None\n",
    "        embedding = []\n",
    "        for i, x in enumerate(xs):\n",
    "            if states is None:\n",
    "                states = self._get_init_states(x)\n",
    "            length = lengths[i]\n",
    "            output, states = self.slide_encoder(x, states, length)\n",
    "            output = output[length.long() - 1,:,:]\n",
    "            idxs = torch.arange(args.batch_size)\n",
    "            output = output[idxs, idxs, :]\n",
    "            embedding.append(output[-1:].squeeze())\n",
    "        \n",
    "        output = torch.cat(embedding, dim=-1)\n",
    "        output = self.relu(self.linear(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "model = SlideDeckEncoder().to(device)\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    for batch in train_loader:\n",
    "        xs = torch.transpose(batch[\"slide_deck\"], 0, 1)\n",
    "        xs = torch.transpose(xs, 1, 2)\n",
    "        lengths = torch.transpose(batch[\"lengths_slide_deck\"], 0, 1)\n",
    "        slide_deck_embedding = model(xs, lengths)\n",
    "        print(slide_deck_embedding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45dc61766396859fdffae760accc4b74216ea8fbbed6d1a9d5e8fb1914e35062"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
