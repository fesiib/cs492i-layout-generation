{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import pi\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14200), started 0:10:23 ago. (Use '!kill 14200' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d6b233c58f0146a5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d6b233c58f0146a5\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 224])\n"
     ]
    }
   ],
   "source": [
    "input_embedding_num = 64\n",
    "slide_deck_embedding_num = 128\n",
    "output_label_num = 4\n",
    "hidden_state_num = 128\n",
    "content_num = 32\n",
    "batch_size = 2\n",
    "\n",
    "input_embedding = torch.randn((batch_size, input_embedding_num))\n",
    "slide_deck_embedding = torch.randn((batch_size, slide_deck_embedding_num))\n",
    "content = torch.randn((batch_size, content_num))\n",
    "\n",
    "x = torch.cat([input_embedding, slide_deck_embedding, content], 1)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BoxGenerator(torch.nn.module):\n",
    "    def __init__(self, input_embedding_num, slide_deck_embedding_num, output_label_num, hidden_state_num, content_num, K=2):\n",
    "        self.lstm = nn.LSTM(input_size=input_embedding_num + slide_deck_embedding_num + content_num, hidden_size=hidden_state_num, batch_first=True)\n",
    "        self.W1 = nn.Linear(hidden_state_num + content_num, output_label_num)\n",
    "        self.W2 = nn.Linear(output_label_num + hidden_state_num, 4)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_1 = self.lstm(x)\n",
    "        temp = torch.cat(W1,)\n",
    "        label = self.W1(output_1, x[:,])\n",
    "\n",
    "        \n",
    "\n",
    "class GaussianMixture(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Fits a mixture of k=1,..,K Gaussians to the input data (K is supplied via n_components).\n",
    "    Input tensors are expected to be flat with dimensions (n: number of samples, d: number of features).\n",
    "    The model then extends them to (n, 1, d).\n",
    "    The model parametrization (mu, sigma) is stored as (1, k, d),\n",
    "    probabilities are shaped (n, k, 1) if they relate to an individual sample,\n",
    "    or (1, k, 1) if they assign membership probabilities to one of the mixture components.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components, n_features, covariance_type=\"full\", eps=1.e-6, init_params=\"kmeans\", mu_init=None, var_init=None):\n",
    "        \"\"\"\n",
    "        Initializes the model and brings all tensors into their required shape.\n",
    "        The class expects data to be fed as a flat tensor in (n, d).\n",
    "        The class owns:\n",
    "            x:               torch.Tensor (n, 1, d)\n",
    "            mu:              torch.Tensor (1, k, d)\n",
    "            var:             torch.Tensor (1, k, d) or (1, k, d, d)\n",
    "            pi:              torch.Tensor (1, k, 1)\n",
    "            covariance_type: str\n",
    "            eps:             float\n",
    "            init_params:     str\n",
    "            log_likelihood:  float\n",
    "            n_components:    int\n",
    "            n_features:      int\n",
    "        args:\n",
    "            n_components:    int\n",
    "            n_features:      int\n",
    "        options:\n",
    "            mu_init:         torch.Tensor (1, k, d)\n",
    "            var_init:        torch.Tensor (1, k, d) or (1, k, d, d)\n",
    "            covariance_type: str\n",
    "            eps:             float\n",
    "            init_params:     str\n",
    "        \"\"\"\n",
    "        super(GaussianMixture, self).__init__()\n",
    "\n",
    "        self.n_components = n_components\n",
    "        self.n_features = n_features\n",
    "\n",
    "        self.mu_init = mu_init\n",
    "        self.var_init = var_init\n",
    "        self.eps = eps\n",
    "\n",
    "        self.log_likelihood = -np.inf\n",
    "\n",
    "        self.covariance_type = covariance_type\n",
    "        self.init_params = init_params\n",
    "\n",
    "        assert self.covariance_type in [\"full\", \"diag\"]\n",
    "        assert self.init_params in [\"kmeans\", \"random\"]\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "\n",
    "    def _init_params(self):\n",
    "        if self.mu_init is not None:\n",
    "            assert self.mu_init.size() == (1, self.n_components, self.n_features), \"Input mu_init does not have required tensor dimensions (1, %i, %i)\" % (self.n_components, self.n_features)\n",
    "            # (1, k, d)\n",
    "            self.mu = torch.nn.Parameter(self.mu_init, requires_grad=False)\n",
    "        else:\n",
    "            self.mu = torch.nn.Parameter(torch.randn(1, self.n_components, self.n_features), requires_grad=False)\n",
    "\n",
    "        if self.covariance_type == \"diag\":\n",
    "            if self.var_init is not None:\n",
    "                # (1, k, d)\n",
    "                assert self.var_init.size() == (1, self.n_components, self.n_features), \"Input var_init does not have required tensor dimensions (1, %i, %i)\" % (self.n_components, self.n_features)\n",
    "                self.var = torch.nn.Parameter(self.var_init, requires_grad=False)\n",
    "            else:\n",
    "                self.var = torch.nn.Parameter(torch.ones(1, self.n_components, self.n_features), requires_grad=False)\n",
    "        elif self.covariance_type == \"full\":\n",
    "            if self.var_init is not None:\n",
    "                # (1, k, d, d)\n",
    "                assert self.var_init.size() == (1, self.n_components, self.n_features, self.n_features), \"Input var_init does not have required tensor dimensions (1, %i, %i, %i)\" % (self.n_components, self.n_features, self.n_features)\n",
    "                self.var = torch.nn.Parameter(self.var_init, requires_grad=False,)\n",
    "            else:\n",
    "                self.var = torch.nn.Parameter(\n",
    "                    torch.eye(self.n_features,dtype=torch.float64).reshape(1, 1, self.n_features, self.n_features).repeat(1, self.n_components, 1, 1),\n",
    "                    requires_grad=False)\n",
    "\n",
    "        # (1, k, 1)\n",
    "        self.pi = torch.nn.Parameter(torch.Tensor(1, self.n_components, 1), requires_grad=False).fill_(1. / self.n_components)\n",
    "\n",
    "        self.params_fitted = False\n",
    "\n",
    "\n",
    "    def check_size(self, x):\n",
    "        if len(x.size()) == 2:\n",
    "            # (n, d) --> (n, 1, d)\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def bic(self, x):\n",
    "        \"\"\"\n",
    "        Bayesian information criterion for a batch of samples.\n",
    "        args:\n",
    "            x:      torch.Tensor (n, d) or (n, 1, d)\n",
    "        returns:\n",
    "            bic:    float\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Free parameters for covariance, means and mixture components\n",
    "        free_params = self.n_features * self.n_components + self.n_features + self.n_components - 1\n",
    "\n",
    "        bic = -2. * self.__score(x, as_average=False).mean() * n + free_params * np.log(n)\n",
    "\n",
    "        return bic\n",
    "\n",
    "\n",
    "    def fit(self, x, delta=1e-3, n_iter=100, warm_start=False):\n",
    "        \"\"\"\n",
    "        Fits model to the data.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, d) or (n, k, d)\n",
    "        options:\n",
    "            delta:      float\n",
    "            n_iter:     int\n",
    "            warm_start: bool\n",
    "        \"\"\"\n",
    "        if not warm_start and self.params_fitted:\n",
    "            self._init_params()\n",
    "\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        if self.init_params == \"kmeans\" and self.mu_init is None:\n",
    "            mu = self.get_kmeans_mu(x, n_centers=self.n_components)\n",
    "            self.mu.data = mu\n",
    "\n",
    "        i = 0\n",
    "        j = np.inf\n",
    "\n",
    "        while (i <= n_iter) and (j >= delta):\n",
    "\n",
    "            log_likelihood_old = self.log_likelihood\n",
    "            mu_old = self.mu\n",
    "            var_old = self.var\n",
    "\n",
    "            self.__em(x)\n",
    "            self.log_likelihood = self.__score(x)\n",
    "\n",
    "            if torch.isinf(self.log_likelihood.abs()) or torch.isnan(self.log_likelihood):\n",
    "                device = self.mu.device\n",
    "                # When the log-likelihood assumes inane values, reinitialize model\n",
    "                self.__init__(self.n_components,\n",
    "                    self.n_features,\n",
    "                    covariance_type=self.covariance_type,\n",
    "                    mu_init=self.mu_init,\n",
    "                    var_init=self.var_init,\n",
    "                    eps=self.eps)\n",
    "                for p in self.parameters():\n",
    "                    p.data = p.data.to(device)\n",
    "                if self.init_params == \"kmeans\":\n",
    "                    self.mu.data, = self.get_kmeans_mu(x, n_centers=self.n_components)\n",
    "\n",
    "            i += 1\n",
    "            j = self.log_likelihood - log_likelihood_old\n",
    "\n",
    "            if j <= delta:\n",
    "                # When score decreases, revert to old parameters\n",
    "                self.__update_mu(mu_old)\n",
    "                self.__update_var(var_old)\n",
    "\n",
    "        self.params_fitted = True\n",
    "\n",
    "\n",
    "    def predict(self, x, probs=False):\n",
    "        \"\"\"\n",
    "        Assigns input data to one of the mixture components by evaluating the likelihood under each.\n",
    "        If probs=True returns normalized probabilities of class membership.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, d) or (n, 1, d)\n",
    "            probs:      bool\n",
    "        returns:\n",
    "            p_k:        torch.Tensor (n, k)\n",
    "            (or)\n",
    "            y:          torch.LongTensor (n)\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        weighted_log_prob = self._estimate_log_prob(x) + torch.log(self.pi)\n",
    "\n",
    "        if probs:\n",
    "            p_k = torch.exp(weighted_log_prob)\n",
    "            return torch.squeeze(p_k / (p_k.sum(1, keepdim=True)))\n",
    "        else:\n",
    "            return torch.squeeze(torch.max(weighted_log_prob, 1)[1].type(torch.LongTensor))\n",
    "\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        \"\"\"\n",
    "        Returns normalized probabilities of class membership.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, d) or (n, 1, d)\n",
    "        returns:\n",
    "            y:          torch.LongTensor (n)\n",
    "        \"\"\"\n",
    "        return self.predict(x, probs=True)\n",
    "\n",
    "\n",
    "    def score_samples(self, x):\n",
    "        \"\"\"\n",
    "        Computes log-likelihood of samples under the current model.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, d) or (n, 1, d)\n",
    "        returns:\n",
    "            score:      torch.LongTensor (n)\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        score = self.__score(x, as_average=False)\n",
    "        return score\n",
    "\n",
    "\n",
    "    def _estimate_log_prob(self, x):\n",
    "        \"\"\"\n",
    "        Returns a tensor with dimensions (n, k, 1), which indicates the log-likelihood that samples belong to the k-th Gaussian.\n",
    "        args:\n",
    "            x:            torch.Tensor (n, d) or (n, 1, d)\n",
    "        returns:\n",
    "            log_prob:     torch.Tensor (n, k, 1)\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        if self.covariance_type == \"full\":\n",
    "            mu = self.mu\n",
    "            var = self.var\n",
    "            precision = torch.inverse(var)\n",
    "            d = x.shape[-1]\n",
    "\n",
    "            log_2pi = d * np.log(2. * pi)\n",
    "\n",
    "            log_det = self._calculate_log_det(precision)\n",
    "\n",
    "            x = x.double() \n",
    "            mu = mu.double()\n",
    "            x_mu_T = (x - mu).unsqueeze(-2)\n",
    "            x_mu = (x - mu).unsqueeze(-1)\n",
    "\n",
    "            x_mu_T_precision = calculate_matmul_n_times(self.n_components, x_mu_T, precision)\n",
    "            x_mu_T_precision_x_mu = calculate_matmul(x_mu_T_precision, x_mu)\n",
    "\n",
    "            return -.5 * (log_2pi - log_det + x_mu_T_precision_x_mu)\n",
    "\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            mu = self.mu\n",
    "            prec = torch.rsqrt(self.var)\n",
    "\n",
    "            log_p = torch.sum((mu * mu + x * x - 2 * x * mu) * (prec ** 2), dim=2, keepdim=True)\n",
    "            log_det = torch.sum(torch.log(prec), dim=2, keepdim=True)\n",
    "\n",
    "            return -.5 * (self.n_features * np.log(2. * pi) + log_p) + log_det\n",
    "\n",
    "\n",
    "\n",
    "    def _calculate_log_det(self, var):\n",
    "        \"\"\"\n",
    "        Calculate log determinant in log space, to prevent overflow errors.\n",
    "        args:\n",
    "            var:            torch.Tensor (1, k, d, d)\n",
    "        \"\"\"\n",
    "        log_det = torch.empty(size=(self.n_components,)).to(var.device)\n",
    "        \n",
    "        for k in range(self.n_components):\n",
    "            log_det[k] = 2 * torch.log(torch.diagonal(torch.linalg.cholesky(var[0,k]))).sum()\n",
    "\n",
    "        return log_det.unsqueeze(-1)\n",
    "\n",
    "\n",
    "    def _e_step(self, x):\n",
    "        \"\"\"\n",
    "        Computes log-responses that indicate the (logarithmic) posterior belief (sometimes called responsibilities) that a data point was generated by one of the k mixture components.\n",
    "        Also returns the mean of the mean of the logarithms of the probabilities (as is done in sklearn).\n",
    "        This is the so-called expectation step of the EM-algorithm.\n",
    "        args:\n",
    "            x:              torch.Tensor (n, d) or (n, 1, d)\n",
    "        returns:\n",
    "            log_prob_norm:  torch.Tensor (1)\n",
    "            log_resp:       torch.Tensor (n, k, 1)\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        weighted_log_prob = self._estimate_log_prob(x) + torch.log(self.pi)\n",
    "\n",
    "        log_prob_norm = torch.logsumexp(weighted_log_prob, dim=1, keepdim=True)\n",
    "        log_resp = weighted_log_prob - log_prob_norm\n",
    "\n",
    "        return torch.mean(log_prob_norm), log_resp\n",
    "\n",
    "\n",
    "    def _m_step(self, x, log_resp):\n",
    "        \"\"\"\n",
    "        From the log-probabilities, computes new parameters pi, mu, var (that maximize the log-likelihood). This is the maximization step of the EM-algorithm.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, d) or (n, 1, d)\n",
    "            log_resp:   torch.Tensor (n, k, 1)\n",
    "        returns:\n",
    "            pi:         torch.Tensor (1, k, 1)\n",
    "            mu:         torch.Tensor (1, k, d)\n",
    "            var:        torch.Tensor (1, k, d)\n",
    "        \"\"\"\n",
    "        x = self.check_size(x)\n",
    "\n",
    "        resp = torch.exp(log_resp)\n",
    "\n",
    "        pi = torch.sum(resp, dim=0, keepdim=True) + self.eps\n",
    "        mu = torch.sum(resp * x, dim=0, keepdim=True) / pi\n",
    "\n",
    "        if self.covariance_type == \"full\":\n",
    "            eps = (torch.eye(self.n_features) * self.eps).to(x.device)\n",
    "            var = torch.sum((x - mu).unsqueeze(-1).matmul((x - mu).unsqueeze(-2)) * resp.unsqueeze(-1), dim=0,\n",
    "                            keepdim=True) / torch.sum(resp, dim=0, keepdim=True).unsqueeze(-1) + eps\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            x2 = (resp * x * x).sum(0, keepdim=True) / pi\n",
    "            mu2 = mu * mu\n",
    "            xmu = (resp * mu * x).sum(0, keepdim=True) / pi\n",
    "            var = x2 - 2 * xmu + mu2 + self.eps\n",
    "\n",
    "        pi = pi / x.shape[0]\n",
    "\n",
    "        return pi, mu, var\n",
    "\n",
    "\n",
    "    def __em(self, x):\n",
    "        \"\"\"\n",
    "        Performs one iteration of the expectation-maximization algorithm by calling the respective subroutines.\n",
    "        args:\n",
    "            x:          torch.Tensor (n, 1, d)\n",
    "        \"\"\"\n",
    "        _, log_resp = self._e_step(x)\n",
    "        pi, mu, var = self._m_step(x, log_resp)\n",
    "\n",
    "        self.__update_pi(pi)\n",
    "        self.__update_mu(mu)\n",
    "        self.__update_var(var)\n",
    "\n",
    "\n",
    "    def __score(self, x, as_average=True):\n",
    "        \"\"\"\n",
    "        Computes the log-likelihood of the data under the model.\n",
    "        args:\n",
    "            x:                  torch.Tensor (n, 1, d)\n",
    "            sum_data:           bool\n",
    "        returns:\n",
    "            score:              torch.Tensor (1)\n",
    "            (or)\n",
    "            per_sample_score:   torch.Tensor (n)\n",
    "        \"\"\"\n",
    "        weighted_log_prob = self._estimate_log_prob(x) + torch.log(self.pi)\n",
    "        per_sample_score = torch.logsumexp(weighted_log_prob, dim=1)\n",
    "\n",
    "        if as_average:\n",
    "            return per_sample_score.mean()\n",
    "        else:\n",
    "            return torch.squeeze(per_sample_score)\n",
    "\n",
    "\n",
    "    def __update_mu(self, mu):\n",
    "        \"\"\"\n",
    "        Updates mean to the provided value.\n",
    "        args:\n",
    "            mu:         torch.FloatTensor\n",
    "        \"\"\"\n",
    "        assert mu.size() in [(self.n_components, self.n_features), (1, self.n_components, self.n_features)], \"Input mu does not have required tensor dimensions (%i, %i) or (1, %i, %i)\" % (self.n_components, self.n_features, self.n_components, self.n_features)\n",
    "\n",
    "        if mu.size() == (self.n_components, self.n_features):\n",
    "            self.mu = mu.unsqueeze(0)\n",
    "        elif mu.size() == (1, self.n_components, self.n_features):\n",
    "            self.mu.data = mu\n",
    "\n",
    "\n",
    "    def __update_var(self, var):\n",
    "        \"\"\"\n",
    "        Updates variance to the provided value.\n",
    "        args:\n",
    "            var:        torch.FloatTensor\n",
    "        \"\"\"\n",
    "        if self.covariance_type == \"full\":\n",
    "            assert var.size() in [(self.n_components, self.n_features, self.n_features), (1, self.n_components, self.n_features, self.n_features)], \"Input var does not have required tensor dimensions (%i, %i, %i) or (1, %i, %i, %i)\" % (self.n_components, self.n_features, self.n_features, self.n_components, self.n_features, self.n_features)\n",
    "\n",
    "            if var.size() == (self.n_components, self.n_features, self.n_features):\n",
    "                self.var = var.unsqueeze(0)\n",
    "            elif var.size() == (1, self.n_components, self.n_features, self.n_features):\n",
    "                self.var.data = var\n",
    "\n",
    "        elif self.covariance_type == \"diag\":\n",
    "            assert var.size() in [(self.n_components, self.n_features), (1, self.n_components, self.n_features)], \"Input var does not have required tensor dimensions (%i, %i) or (1, %i, %i)\" % (self.n_components, self.n_features, self.n_components, self.n_features)\n",
    "\n",
    "            if var.size() == (self.n_components, self.n_features):\n",
    "                self.var = var.unsqueeze(0)\n",
    "            elif var.size() == (1, self.n_components, self.n_features):\n",
    "                self.var.data = var\n",
    "\n",
    "\n",
    "    def __update_pi(self, pi):\n",
    "        \"\"\"\n",
    "        Updates pi to the provided value.\n",
    "        args:\n",
    "            pi:         torch.FloatTensor\n",
    "        \"\"\"\n",
    "        assert pi.size() in [(1, self.n_components, 1)], \"Input pi does not have required tensor dimensions (%i, %i, %i)\" % (1, self.n_components, 1)\n",
    "\n",
    "        self.pi.data = pi\n",
    "\n",
    "\n",
    "    def get_kmeans_mu(self, x, n_centers, init_times=50, min_delta=1e-3):\n",
    "        \"\"\"\n",
    "        Find an initial value for the mean. Requires a threshold min_delta for the k-means algorithm to stop iterating.\n",
    "        The algorithm is repeated init_times often, after which the best centerpoint is returned.\n",
    "        args:\n",
    "            x:            torch.FloatTensor (n, d) or (n, 1, d)\n",
    "            init_times:   init\n",
    "            min_delta:    int\n",
    "        \"\"\"\n",
    "        if len(x.size()) == 3:\n",
    "            x = x.squeeze(1)\n",
    "        x_min, x_max = x.min(), x.max()\n",
    "        x = (x - x_min) / (x_max - x_min)\n",
    "        \n",
    "        min_cost = np.inf\n",
    "\n",
    "        for i in range(init_times):\n",
    "            tmp_center = x[np.random.choice(np.arange(x.shape[0]), size=n_centers, replace=False), ...]\n",
    "            l2_dis = torch.norm((x.unsqueeze(1).repeat(1, n_centers, 1) - tmp_center), p=2, dim=2)\n",
    "            l2_cls = torch.argmin(l2_dis, dim=1)\n",
    "\n",
    "            cost = 0\n",
    "            for c in range(n_centers):\n",
    "                cost += torch.norm(x[l2_cls == c] - tmp_center[c], p=2, dim=1).mean()\n",
    "\n",
    "            if cost < min_cost:\n",
    "                min_cost = cost\n",
    "                center = tmp_center\n",
    "\n",
    "        delta = np.inf\n",
    "\n",
    "        while delta > min_delta:\n",
    "            l2_dis = torch.norm((x.unsqueeze(1).repeat(1, n_centers, 1) - center), p=2, dim=2)\n",
    "            l2_cls = torch.argmin(l2_dis, dim=1)\n",
    "            center_old = center.clone()\n",
    "\n",
    "            for c in range(n_centers):\n",
    "                center[c] = x[l2_cls == c].mean(dim=0)\n",
    "\n",
    "            delta = torch.norm((center_old - center), dim=1).max()\n",
    "\n",
    "        return (center.unsqueeze(0)*(x_max - x_min) + x_min)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e04026de45b6381e69cbe3b87200c3876b94cab4b646df382595102aa479834e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('SlideStyleTransfer': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
