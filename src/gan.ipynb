{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../'\n",
    "\n",
    "import os, sys\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import mask_rcnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = []\n",
    "\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "BB_TYPES = [\n",
    "    'title',\n",
    "    'header',\n",
    "    'text box',\n",
    "    'footer',\n",
    "    'picture',\n",
    "    'instructor',\n",
    "    'diagram',\n",
    "    'table',\n",
    "    'figure',\n",
    "    'handwriting',\n",
    "    'chart',\n",
    "    'schematic diagram',\n",
    "]\n",
    "\n",
    "args = edict()\n",
    "args.batch_size = 1\n",
    "args.lr = 1e-4\n",
    "args.momentum = 0.9\n",
    "args.weight_decay = 5e-4\n",
    "args.epoch = 10\n",
    "args.tensorboard = False\n",
    "args.gpu = True\n",
    "args.train_portion = 0.7\n",
    "args.slide_deck_embedding_dim = 128\n",
    "args.bbtype_num = len(BB_TYPES)\n",
    "args.latent_dim = 64\n",
    "args.hidden_size = 64\n",
    "args.input_size = 64\n",
    "args.dropout_rate = 0.5\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "\n",
    "# Create directory name.\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tensorboard:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"{str(result_dir)}\" --samples_per_plugin images=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_image(image, bbs):\n",
    "    if (torch.is_tensor(image)):\n",
    "        image = np.array(image.tolist()).transpose((1, 2, 0))\n",
    "    if (torch.is_tensor(bbs)):\n",
    "        bbs = np.array(bbs.tolist())\n",
    "\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    for bb in bbs:\n",
    "        rect = patches.Rectangle((bb[0], bb[1]), bb[2], bb[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    plt.show()\n",
    "\n",
    "class FitVidDataset(Dataset):\n",
    "    \"\"\" FitVid Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, img_data, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_data = img_data\n",
    "        self.img_filenames = list(self.img_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_filenames)\n",
    "\n",
    "    def show_image(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        filename = self.img_filenames[idx]\n",
    "        img_dir = os.path.join(self.root_dir, filename)\n",
    "        \n",
    "        image = io.imread(img_dir)\n",
    "        draw_image(image, self.img_data[filename]['bbs'])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        filename = self.img_filenames[idx]\n",
    "        img_dir = os.path.join(self.root_dir, filename)\n",
    "        image = io.imread(img_dir)\n",
    "\n",
    "        bbs = self.img_data[filename]['bbs']\n",
    "\n",
    "        sample = {\n",
    "            'image' : image,\n",
    "            'labels' : bbs,\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbs(shape, bbs):\n",
    "    if (torch.is_tensor(bbs)):\n",
    "        bbs = np.array(bbs.tolist())\n",
    "    if (torch.is_tensor(shape)):\n",
    "        [h, w] = np.array(shape.tolist())\n",
    "        shape = (h, w)\n",
    "    \n",
    "    h, w = shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    background=patches.Rectangle((0, 0), w, h, linewidth=2, edgecolor='b', facecolor='black')\n",
    "    ax.add_patch(background)\n",
    "    for bb in bbs:\n",
    "        rect = patches.Rectangle((bb[0], bb[1]), bb[2], bb[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.autoscale(True, 'both')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "class BBSlideDeckDataset(Dataset):\n",
    "    \"\"\" Slide Deck Dataset but with Bounding Boxes\"\"\"\n",
    "    def __init__(self, slide_deck_data, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.slide_deck_data = slide_deck_data\n",
    "        self.slide_deck_ids = list(self.slide_deck_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slide_deck_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        slide_deck_id = self.slide_deck_ids[idx]\n",
    "        (h, w) = self.slide_deck_data[slide_deck_id][\"shape\"]\n",
    "        slides = []\n",
    "        max_len_bbs = 0\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            if len(slide) > max_len_bbs:\n",
    "                max_len_bbs = len(slide)\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            np_slide = np.zeros((max_len_bbs, 5))\n",
    "            for i, bb in enumerate(slide):\n",
    "                np_slide[i] = bb\n",
    "            slides.append(np_slide)\n",
    "\n",
    "        ref_slide = slides[0]\n",
    "        slide_deck = slides[1:]\n",
    "        sample = {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"slide_deck\": np.asarray(slide_deck)\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, labels = sample['image'], sample['labels']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        labels = labels * np.array([new_w / w, new_h / h, new_w / w, new_h / h, 1]).T\n",
    "        return {'image': img, 'labels': labels}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, labels = sample['image'], sample['labels']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "        labels = labels - np.array([left, top, 0, 0, 0]).T\n",
    "\n",
    "        return {'image': image, 'labels': labels}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, labels = sample['image'], sample['labels']\n",
    "\n",
    "        # swap color axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image),\n",
    "                'labels': torch.from_numpy(labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleBB(object):\n",
    "    \"\"\"Rescale the bounding boxes in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def _resize_single_slide(self, slide, original_shape, new_shape):\n",
    "        h, w = original_shape\n",
    "        new_h, new_w = new_shape\n",
    "        slide = slide * np.array([new_w / w, new_h / h, new_w / w, new_h / h, 1]).T\n",
    "        return slide\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        ref_slide = self._resize_single_slide(ref_slide, (h, w), (new_h, new_w))\n",
    "        for i, slide in enumerate(slide_deck):\n",
    "            slide_deck[i] = self._resize_single_slide(slide, (h, w), (new_h, new_w))\n",
    "\n",
    "        return {\n",
    "            \"shape\": (new_h, new_w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"slide_deck\": slide_deck\n",
    "        }\n",
    "\n",
    "class LeaveN(object):\n",
    "    def __init__ (self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "\n",
    "        np.random.shuffle(slide_deck)\n",
    "\n",
    "        if slide_deck.shape[0] > self.N:\n",
    "            slide_deck = np.delete(slide_deck, range(self.N, slide_deck.shape[0]), 0)\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"slide_deck\": slide_deck\n",
    "        }\n",
    "\n",
    "class ShuffleRefSlide(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        \n",
    "        slide_deck = np.vstack((slide_deck, ref_slide[None, :]))\n",
    "        np.random.shuffle(slide_deck)\n",
    "        slide_deck = slide_deck.tolist()\n",
    "        ref_slide = np.asarray(slide_deck.pop())\n",
    "        slide_deck = np.asarray(slide_deck)\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"slide_deck\": slide_deck\n",
    "        }\n",
    "\n",
    "class ShuffleSlideDeck(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        np.random.shuffle(slide_deck)\n",
    "        \n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"slide_deck\": slide_deck\n",
    "        }\n",
    "\n",
    "class ToTensorBB(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        return {\n",
    "            'shape': torch.tensor([h, w]),\n",
    "            'ref_slide': torch.from_numpy(ref_slide),\n",
    "            'slide_deck': torch.from_numpy(slide_deck)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_slide_deck_dataset(all_dataset):\n",
    "    slide_deck_data = {}\n",
    "    for entrance in all_dataset.iloc:\n",
    "        slide_deck_id = entrance['Slide Deck Id']\n",
    "        \n",
    "        slide_id = entrance[\"Slide Id\"]\n",
    "        if (slide_deck_id not in slide_deck_data):\n",
    "            slide_deck_data[slide_deck_id] = {\n",
    "                'slides': {},\n",
    "                'shape': (entrance['Image Height'], entrance['Image Width'])\n",
    "            }\n",
    "        \n",
    "        if slide_id not in slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            slide_deck_data[slide_deck_id][\"slides\"][slide_id] = []\n",
    "        bb_type = BB_TYPES.index(entrance['Type'])\n",
    "        if (bb_type < 0 or bb_type >= len(BB_TYPES)):\n",
    "            bb_type = len(BB_TYPES)\n",
    "\n",
    "        bb = np.array([\n",
    "            entrance['X'],\n",
    "            entrance['Y'],\n",
    "            entrance['BB Width'],\n",
    "            entrance['BB Height'],\n",
    "            bb_type + 1\n",
    "        ]).T\n",
    "        slide_deck_data[slide_deck_id]['slides'][slide_id].append(bb)\n",
    "    for key in slide_deck_data.keys():\n",
    "        \n",
    "        # if key == 100:\n",
    "        #     for (id, value) in slide_deck_data[key][\"slides\"].items():\n",
    "        #         print(56, id)\n",
    "        #         draw_bbs(slide_deck_data[key][\"shape\"], value)\n",
    "\n",
    "        values = list(slide_deck_data[key][\"slides\"].values())\n",
    "        slide_deck_data[key][\"slides\"] = [np.asarray(value) for value in values]\n",
    "    return slide_deck_data\n",
    "\n",
    "def process_fitvid_dataset(all_dataset, root_dir):\n",
    "    img_data = {}\n",
    "    for entrance in all_dataset.iloc:\n",
    "        filename = entrance['filename']\n",
    "        img_dir = os.path.join(root_dir, filename)\n",
    "        if (os.path.exists(img_dir) is False):\n",
    "            continue\n",
    "        if (filename not in img_data):\n",
    "            img_data[filename] = {\n",
    "                'bbs': [],\n",
    "                'img_w' : entrance['Image Width'],\n",
    "                'img_h' : entrance['Image Height'],\n",
    "            }\n",
    "        \n",
    "        bb_type = BB_TYPES.index(entrance['Type'])\n",
    "        if (bb_type < 0 or bb_type >= len(BB_TYPES)):\n",
    "            bb_type = len(BB_TYPES)\n",
    "\n",
    "        bb = np.array([\n",
    "            entrance['X'],\n",
    "            entrance['Y'],\n",
    "            entrance['BB Width'],\n",
    "            entrance['BB Height'],\n",
    "            bb_type + 1\n",
    "        ]).T\n",
    "        img_data[filename]['bbs'].append(bb)\n",
    "    return img_data\n",
    "\n",
    "def slice_dict(dictionary, l, r):\n",
    "    keys = list(dictionary.keys())\n",
    "    keys = keys[l:r]\n",
    "    ret_dictionary = {}\n",
    "    for key in keys:\n",
    "        ret_dictionary[key] = dictionary[key]\n",
    "    return ret_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"slide_deck_dataset.csv\")\n",
    "\n",
    "dataset = pd.read_csv(csv_file)\n",
    "slide_deck_data = process_slide_deck_dataset(dataset)\n",
    "\n",
    "division = int(args.train_portion * len(slide_deck_data))\n",
    "\n",
    "train_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, 0, division),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB(256),\n",
    "        ShuffleRefSlide(),\n",
    "        ShuffleSlideDeck(),\n",
    "        LeaveN(5),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, division, len(slide_deck_data)),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB(256),\n",
    "        #ShuffleSlideDeck(),\n",
    "        #ShuffleRefSlide(),\n",
    "        #LeaveN(5),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 5]) torch.Size([5, 7, 5])\n"
     ]
    }
   ],
   "source": [
    "single = train_slide_deck_dataset[0]\n",
    "draw_bbs(single[\"shape\"], single[\"ref_slide\"])\n",
    "print(single[\"ref_slide\"].shape, single[\"slide_deck\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, torch.autograd.Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "    return torch.autograd.Variable(y_one_hot) if isinstance(y, torch.autograd.Variable) else y_one_hot\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, embedding=False):\n",
    "        super(Generator, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=args.input_size, hidden_size=args.hidden_size, num_layers=2, \n",
    "            batch_first=True, dropout=args.dropout_rate, bias=True)\n",
    "        self.exist_embedding = embedding\n",
    "        if self.exist_embedding:\n",
    "            self.embed = nn.Embedding(args.bbtype_num + 1, args.bbtype_num, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # def block(in_feat, out_feat, normalize=True):\n",
    "        #     layers = [nn.Linear(in_feat, out_feat)]\n",
    "        #     if normalize:\n",
    "        #         layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "        #     layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "        #     return layers\n",
    "\n",
    "        # self.model = nn.Sequential(\n",
    "        #     *block(opt.latent_dim, 128, normalize=False),\n",
    "        #     *block(128, 256),\n",
    "        #     *block(256, 512),\n",
    "        #     *block(512, 1024),\n",
    "        #     nn.Linear(1024, int(np.prod(img_shape))),\n",
    "        #     nn.Tanh()\n",
    "        # )\n",
    "\n",
    "\n",
    "    def forward(self, x, z, slide_deck_embedding, length=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "            z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            length (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            bb sequence: (tensor), (Batch_size, Sequence_size, 5)\n",
    "        \"\"\"\n",
    "        (Batch_size, Sequence_size) = x.shape\n",
    "        if self.exist_embedding:\n",
    "            temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        else:\n",
    "            temp_input_1 = to_one_hot(x, n_dims=args.bbtype_num + 1)\n",
    "            temp_input_1[:, :, 0] = 0.0\n",
    "        temp_input_2 = z.unsqueeze(1).repeat((1, Sequence_size, 1))\n",
    "        input_1 = torch.cat((temp_input_1, z), dim=-1)\n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 5])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "input = torch.LongTensor([[0,2,4,0],[4,3,0,0]])\n",
    "embedding(input).shape\n",
    "\n",
    "a = to_one_hot(input)\n",
    "a[:,:,0] = 0\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = nn.LSTM(10, 20, 2) #input_size  = 10, hidden = 20, layer=2\n",
    "input = torch.randn(5, 3, 10) # 3 batch, \n",
    "h0 = torch.randn(2, 3, 20)\n",
    "c0 = torch.randn(2, 3, 20)\n",
    "output, (hn, cn) = rnn(input, (h0, c0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 3, 20]), torch.Size([2, 3, 20]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape, hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2,  3,  2],\n",
       "         [ 2,  3,  2]],\n",
       "\n",
       "        [[23,  2,  2],\n",
       "         [23,  2,  2]]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[2, 3,2],[23,2, 2]])\n",
    "print(x.shape)\n",
    "x.unsqueeze(1).repeat((1,2,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45dc61766396859fdffae760accc4b74216ea8fbbed6d1a9d5e8fb1914e35062"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
