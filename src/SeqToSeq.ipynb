{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../'\n",
    "\n",
    "import os, sys\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import mask_rcnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = []\n",
    "\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "BB_TYPES = [\n",
    "    'title',\n",
    "    'header',\n",
    "    'text box',\n",
    "    'footer',\n",
    "    'picture',\n",
    "    'instructor',\n",
    "    'diagram',\n",
    "    'table',\n",
    "    'figure',\n",
    "    'handwriting',\n",
    "    'chart',\n",
    "    'schematic diagram',\n",
    "]\n",
    "\n",
    "args = edict()\n",
    "args.batch_size = 2\n",
    "args.lr = 1e-4\n",
    "args.momentum = 0.9\n",
    "args.weight_decay = 5e-4\n",
    "args.epoch = 10\n",
    "args.tensorboard = False\n",
    "args.gpu = True\n",
    "args.train_portion = 0.7\n",
    "args.slide_deck_embedding_dim = 128\n",
    "args.bbtype_num = len(BB_TYPES)\n",
    "args.latent_dim = 32\n",
    "args.hidden_size = 64\n",
    "args.input_size = 64 \n",
    "args.dropout_rate = 0.5\n",
    "\n",
    "assert(args.latent_dim == args.input_size//2)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "\n",
    "# Create directory name.\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tensorboard:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"{str(result_dir)}\" --samples_per_plugin images=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbs(shape, bbs):\n",
    "    if (torch.is_tensor(bbs)):\n",
    "        bbs = np.array(bbs.tolist())\n",
    "    if (torch.is_tensor(shape)):\n",
    "        [h, w] = np.array(shape.tolist())\n",
    "        shape = (h, w)\n",
    "    \n",
    "    h, w = shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    background=patches.Rectangle((0, 0), w, h, linewidth=2, edgecolor='b', facecolor='black')\n",
    "    ax.add_patch(background)\n",
    "    for bb in bbs:\n",
    "        rect = patches.Rectangle((bb[0], bb[1]), bb[2], bb[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.autoscale(True, 'both')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_BB_types(bbs):\n",
    "    return bbs[:, 4]\n",
    "\n",
    "class BBSlideDeckDataset(Dataset):\n",
    "    \"\"\" Slide Deck Dataset but with Bounding Boxes\"\"\"\n",
    "    def __init__(self, slide_deck_data, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.slide_deck_data = slide_deck_data\n",
    "        self.slide_deck_ids = list(self.slide_deck_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slide_deck_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        slide_deck_id = self.slide_deck_ids[idx]\n",
    "        (h, w) = self.slide_deck_data[slide_deck_id][\"shape\"]\n",
    "        lengths_slide_deck = []\n",
    "        slides = []\n",
    "        max_len_bbs = 0\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            lengths_slide_deck.append(len(slide))\n",
    "            if len(slide) > max_len_bbs:\n",
    "                max_len_bbs = len(slide)\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            np_slide = np.zeros((max_len_bbs, 5))\n",
    "            for i, bb in enumerate(slide):\n",
    "                np_slide[i] = bb\n",
    "            slides.append(np_slide)\n",
    "\n",
    "        ref_slide = slides[0]\n",
    "        slide_deck = slides[1:]\n",
    "        length_ref_types = lengths_slide_deck.pop(0)\n",
    "        sample = {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": get_BB_types(ref_slide),\n",
    "            \"slide_deck\": np.asarray(slide_deck),\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleBB(object):\n",
    "    \"\"\"Rescale the bounding boxes in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def _resize_single_slide(self, slide, original_shape, new_shape):\n",
    "        h, w = original_shape\n",
    "        new_h, new_w = new_shape\n",
    "        slide = slide * np.array([new_w / w, new_h / h, new_w / w, new_h / h, 1]).T\n",
    "        return slide\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        ref_slide = self._resize_single_slide(ref_slide, (h, w), (new_h, new_w))\n",
    "        for i, slide in enumerate(slide_deck):\n",
    "            slide_deck[i] = self._resize_single_slide(slide, (h, w), (new_h, new_w))\n",
    "\n",
    "        return {\n",
    "            \"shape\": (new_h, new_w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class LeaveN(object):\n",
    "    def __init__ (self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if slide_deck.shape[0] > self.N:\n",
    "            slide_deck = np.delete(slide_deck, range(self.N, slide_deck.shape[0]), 0)\n",
    "            lengths_slide_deck = lengths_slide_deck[:self.N]\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ShuffleRefSlide(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        lengths_slide_deck.append(length_ref_types)\n",
    "        slide_deck = np.vstack((slide_deck, ref_slide[None, :]))\n",
    "\n",
    "        idxs = np.array([*range(0, len(lengths_slide_deck))], dtype=np.int32)\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        slide_deck = slide_deck[idxs]\n",
    "\n",
    "        lengths_slide_deck = np.array(lengths_slide_deck)\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck.tolist()\n",
    "        \n",
    "        slide_deck = slide_deck.tolist()\n",
    "        ref_slide = np.asarray(slide_deck.pop())\n",
    "        length_ref_types = lengths_slide_deck.pop()\n",
    "        ref_types = get_BB_types(ref_slide)\n",
    "\n",
    "        slide_deck = np.asarray(slide_deck)\n",
    "        \n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ToTensorBB(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "        return {\n",
    "            \"shape\": torch.tensor([h, w]),\n",
    "            \"ref_slide\": torch.from_numpy(ref_slide),\n",
    "            \"ref_types\": torch.from_numpy(ref_types),\n",
    "            \"slide_deck\": torch.from_numpy(slide_deck),\n",
    "            \"lengths_slide_deck\": torch.tensor(lengths_slide_deck),\n",
    "            \"length_ref_types\": torch.tensor(length_ref_types)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slide_deck_dataset(all_dataset):\n",
    "    slide_deck_data = {}\n",
    "    for entrance in all_dataset.iloc:\n",
    "        slide_deck_id = entrance['Slide Deck Id']\n",
    "        \n",
    "        slide_id = entrance[\"Slide Id\"]\n",
    "        if (slide_deck_id not in slide_deck_data):\n",
    "            slide_deck_data[slide_deck_id] = {\n",
    "                'slides': {},\n",
    "                'shape': (entrance['Image Height'], entrance['Image Width'])\n",
    "            }\n",
    "        \n",
    "        if slide_id not in slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            slide_deck_data[slide_deck_id][\"slides\"][slide_id] = []\n",
    "        bb_type = BB_TYPES.index(entrance['Type'])\n",
    "        if (bb_type < 0 or bb_type >= len(BB_TYPES)):\n",
    "            bb_type = len(BB_TYPES)\n",
    "\n",
    "        bb = np.array([\n",
    "            entrance['X'],\n",
    "            entrance['Y'],\n",
    "            entrance['BB Width'],\n",
    "            entrance['BB Height'],\n",
    "            bb_type + 1\n",
    "        ]).T\n",
    "        slide_deck_data[slide_deck_id]['slides'][slide_id].append(bb)\n",
    "    for key in slide_deck_data.keys():\n",
    "        \n",
    "        # if key == 100:\n",
    "        #     for (id, value) in slide_deck_data[key][\"slides\"].items():\n",
    "        #         print(56, id)\n",
    "        #         draw_bbs(slide_deck_data[key][\"shape\"], value)\n",
    "\n",
    "        values = list(slide_deck_data[key][\"slides\"].values())\n",
    "        slide_deck_data[key][\"slides\"] = [np.asarray(value) for value in values]\n",
    "    return slide_deck_data\n",
    "\n",
    "def slice_dict(dictionary, l, r):\n",
    "    keys = list(dictionary.keys())\n",
    "    keys = keys[l:r]\n",
    "    ret_dictionary = {}\n",
    "    for key in keys:\n",
    "        ret_dictionary[key] = dictionary[key]\n",
    "    return ret_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"slide_deck_dataset.csv\")\n",
    "\n",
    "dataset = pd.read_csv(csv_file)\n",
    "slide_deck_data = process_slide_deck_dataset(dataset)\n",
    "\n",
    "division = int(args.train_portion * len(slide_deck_data))\n",
    "\n",
    "train_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, 0, division),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(5),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, division, len(slide_deck_data)),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        #ShuffleSlideDeck(),\n",
    "        #ShuffleRefSlide(),\n",
    "        #LeaveN(5),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sl torch.Size([7, 5])\n",
      "sl torch.Size([7, 5])\n",
      "sl torch.Size([7, 5])\n",
      "sl torch.Size([7, 5])\n",
      "sl torch.Size([7, 5])\n",
      "tensor([5, 2, 1, 2, 6])\n",
      "torch.Size([7, 5]) tensor([11.,  1.,  3.,  3.,  0.,  0.,  0.], dtype=torch.float64) tensor(4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'shape': tensor([1, 1]),\n",
       " 'ref_slide': tensor([[ 0.4722,  0.3579,  0.5191,  0.5060, 11.0000],\n",
       "         [ 0.1924,  0.1215,  0.6178,  0.0927,  1.0000],\n",
       "         [ 0.0518,  0.3296,  0.3452,  0.2243,  3.0000],\n",
       "         [ 0.0507,  0.5680,  0.3951,  0.1588,  3.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]], dtype=torch.float64),\n",
       " 'ref_types': tensor([11.,  1.,  3.,  3.,  0.,  0.,  0.], dtype=torch.float64),\n",
       " 'slide_deck': tensor([[[ 0.0881,  0.1270,  0.8268,  0.0927,  1.0000],\n",
       "          [ 0.0597,  0.3165,  0.4216,  0.4592,  3.0000],\n",
       "          [ 0.5108,  0.3170,  0.4242,  0.5791, 11.0000],\n",
       "          [ 0.0571,  0.3110,  0.4265,  0.5927,  3.0000],\n",
       "          [ 0.0824,  0.8432,  0.2129,  0.0509,  3.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0578,  0.3256,  0.8306,  0.1749,  3.0000],\n",
       "          [ 0.2938,  0.1230,  0.4095,  0.0892,  1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.1864,  0.1190,  0.6287,  0.1074,  1.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.1376,  0.0595,  0.7263,  0.2303,  1.0000],\n",
       "          [ 0.0306,  0.4068,  0.8688,  0.5544,  7.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0560,  0.3231,  0.4246,  0.0655,  3.0000],\n",
       "          [ 0.2544,  0.1220,  0.4930,  0.1013,  1.0000],\n",
       "          [ 0.5512,  0.4052,  0.3524,  0.1346,  3.0000],\n",
       "          [ 0.8023,  0.8538,  0.1535,  0.0645,  3.0000],\n",
       "          [ 0.0590,  0.4083,  0.3376,  0.3246,  3.0000],\n",
       "          [ 0.5497,  0.8599,  0.0960,  0.0539,  3.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]], dtype=torch.float64),\n",
       " 'lengths_slide_deck': tensor([5, 2, 1, 2, 6]),\n",
       " 'length_ref_types': tensor(4)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single = train_slide_deck_dataset[0]\n",
    "draw_bbs(single[\"shape\"], single[\"ref_slide\"])\n",
    "#print(single)\n",
    "\n",
    "for slide in single[\"slide_deck\"]:\n",
    "    print('sl', slide.size())\n",
    "print(single[\"lengths_slide_deck\"])\n",
    "\n",
    "print(single[\"ref_slide\"].size(), single[\"ref_types\"], single[\"length_ref_types\"])\n",
    "single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_slide_deck_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_slide_deck_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, torch.autograd.Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "    return torch.autograd.Variable(y_one_hot) if isinstance(y, torch.autograd.Variable) else y_one_hot\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_weights=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed = nn.Embedding(args.bbtype_num + 1, args.input_size//2, padding_idx=0)\n",
    "        input_size = args.input_size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=args.hidden_size, num_layers=2, \n",
    "            batch_first=True, dropout=args.dropout_rate, bias=True)\n",
    "        self.linear1 = nn.Linear(args.slide_deck_embedding_dim, args.hidden_size)\n",
    "        self.linear2 = nn.Linear(args.hidden_size, 4)\n",
    "        if embed_weights:\n",
    "            self.embed.weight = embed_weights\n",
    "\n",
    "\n",
    "    def forward(self, x, z, slide_deck_embedding, length=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "            z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            length (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            bb sequence: (tensor), (Batch_size, Sequence_size, 5)\n",
    "        \"\"\"\n",
    "        print(x.shape, z.shape, slide_deck_embedding.shape)\n",
    "        x = x.int()\n",
    "        (Batch_size, Sequence_size) = x.shape\n",
    "        temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        print(\"1\",temp_input_1.shape)\n",
    "        temp_input_2 = z.unsqueeze(1).repeat((1, Sequence_size, 1))\n",
    "        print(\"2\",temp_input_2.shape)\n",
    "        input_1 = torch.cat((temp_input_2, temp_input_1), dim=-1)\n",
    "        print(\"3\",input_1.shape)\n",
    "        hidden_0 = self.dropout(self.linear1(slide_deck_embedding)).unsqueeze(0).repeat((2, 1, 1))\n",
    "        print(\"4\",hidden_0.shape)\n",
    "        c_0 = torch.zeros(size=(2,Batch_size, args.hidden_size)).to(device)\n",
    "        print(\"5\",c_0.shape)\n",
    "        output, (h_n, c_n) = self.lstm(input_1, (hidden_0, c_0))\n",
    "        print(\"6\",output.shape)\n",
    "        output = output.transpose(0, 1)\n",
    "        return self.linear2(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7]) torch.Size([2, 32]) torch.Size([2, 128])\n",
      "1 torch.Size([2, 7, 32])\n",
      "2 torch.Size([2, 7, 32])\n",
      "3 torch.Size([2, 7, 64])\n",
      "4 torch.Size([2, 2, 64])\n",
      "5 torch.Size([2, 2, 64])\n",
      "6 torch.Size([2, 7, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 9.0863e-02,  7.4051e-02, -6.4483e-02,  2.0179e-02],\n",
       "         [ 3.1125e-02,  5.5662e-02, -1.8351e-02,  7.4276e-04]],\n",
       "\n",
       "        [[ 6.7151e-02,  4.0819e-02, -4.6411e-02, -1.6982e-02],\n",
       "         [ 3.2093e-03,  4.4069e-02,  6.4309e-03,  2.3479e-02]],\n",
       "\n",
       "        [[ 3.7194e-02,  1.4526e-02, -2.1790e-02, -3.8534e-02],\n",
       "         [ 1.3342e-02,  4.6467e-02,  2.9348e-02, -1.7292e-02]],\n",
       "\n",
       "        [[ 6.2985e-02, -2.4184e-02, -4.5423e-03, -5.0321e-02],\n",
       "         [ 4.6401e-02,  1.0009e-01,  4.3048e-04, -3.8818e-02]],\n",
       "\n",
       "        [[ 5.3028e-02,  1.1962e-02, -2.6633e-02, -5.6699e-02],\n",
       "         [ 6.1051e-03,  1.0301e-01, -1.9393e-02, -5.5783e-02]],\n",
       "\n",
       "        [[ 3.7302e-02,  1.6299e-02,  1.9856e-02, -4.2367e-02],\n",
       "         [-9.4652e-03,  1.0492e-01, -1.0063e-02, -5.4050e-02]],\n",
       "\n",
       "        [[ 3.2184e-02,  3.4672e-02, -1.3484e-02, -5.4129e-02],\n",
       "         [-2.2046e-05,  1.1568e-01,  6.7779e-03, -3.0109e-02]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydec = Decoder()\n",
    "z = torch.randn((args.batch_size, args.input_size//2))\n",
    "s = list(train_loader)[0]\n",
    "mydec(x=s['ref_types'], z=z, slide_deck_embedding=torch.randn((args.batch_size, args.slide_deck_embedding_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45dc61766396859fdffae760accc4b74216ea8fbbed6d1a9d5e8fb1914e35062"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
