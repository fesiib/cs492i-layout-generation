{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../'\n",
    "\n",
    "import os, sys\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from functools import cmp_to_key\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn \n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import mask_rcnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "# Basic settings\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "#!pip install easydict\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "args = edict()\n",
    "args.batch_size = 2\n",
    "args.nlayers = 2\n",
    "\n",
    "args.embedding_size = 4\n",
    "args.ninp = 4 + args.embedding_size\n",
    "args.nhid = 256 #512\n",
    "\n",
    "\n",
    "args.clip = 1\n",
    "args.lr_lstm = 0.001\n",
    "args.dropout = 0.2\n",
    "args.nhid_attn = 256\n",
    "args.epochs = 20\n",
    "\n",
    "##### Transformer\n",
    "args.nhid_tran = 256\n",
    "args.nhead = 8\n",
    "args.nlayers_transformer = 6\n",
    "args.attn_pdrop = 0.1\n",
    "args.resid_pdrop = 0.1\n",
    "args.embd_pdrop = 0.1\n",
    "args.nff = 4 * args.nhid_tran\n",
    "\n",
    "\n",
    "args.lr_transformer = 0.0001 #1.0\n",
    "args.betas = (0.9, 0.98)\n",
    "\n",
    "args.gpu = True\n",
    "\n",
    "args.tensorboard = False\n",
    "args.train_portion = 0.7\n",
    "args.slide_deck_N = 5\n",
    "args.slide_deck_embedding_size = 512\n",
    "args.padding_idx = 0\n",
    "\n",
    "\n",
    "# Decoder\n",
    "args.latent_vector_dim = 28\n",
    "args.hidden_size = 64\n",
    "args.dropout_rate = 0.5\n",
    "args.max_seq_length = 8\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "# Create directory name.\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "BB_TYPES = [\n",
    "    '<pad>',\n",
    "    'title',\n",
    "    'header',\n",
    "    'text box',\n",
    "    'footer',\n",
    "    'picture',\n",
    "    'instructor',\n",
    "    'diagram',\n",
    "    'table',\n",
    "    'figure',\n",
    "    'handwriting',\n",
    "    'chart',\n",
    "    'schematic diagram',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tensorboard:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"{str(result_dir)}\" --samples_per_plugin images=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbs(shape, bbs):\n",
    "    if (torch.is_tensor(bbs)):\n",
    "        bbs = np.array(bbs.tolist())\n",
    "    if (torch.is_tensor(shape)):\n",
    "        [h, w] = np.array(shape.tolist())\n",
    "        shape = (h, w)\n",
    "    \n",
    "    h, w = shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    background=patches.Rectangle((0, 0), w, h, linewidth=2, edgecolor='b', facecolor='black')\n",
    "    ax.add_patch(background)\n",
    "    for bb in bbs:\n",
    "        rect = patches.Rectangle((bb[0], bb[1]), bb[2], bb[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.autoscale(True, 'both')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_BB_types(bbs):\n",
    "    return bbs[:, 4]\n",
    "\n",
    "class BBSlideDeckDataset(Dataset):\n",
    "    \"\"\" Slide Deck Dataset but with Bounding Boxes\"\"\"\n",
    "    def __init__(self, slide_deck_data, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.slide_deck_data = slide_deck_data\n",
    "        self.slide_deck_ids = list(self.slide_deck_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slide_deck_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        slide_deck_id = self.slide_deck_ids[idx]\n",
    "        (h, w) = self.slide_deck_data[slide_deck_id][\"shape\"]\n",
    "        lengths_slide_deck = []\n",
    "        slides = []\n",
    "        max_len_bbs = 0\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            lengths_slide_deck.append(len(slide))\n",
    "            if len(slide) > max_len_bbs:\n",
    "                max_len_bbs = len(slide)\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            np_slide = np.zeros((max_len_bbs, 5), dtype=np.double)\n",
    "            for i, bb in enumerate(slide):\n",
    "                np_slide[i] = bb\n",
    "            slides.append(np_slide)\n",
    "\n",
    "        ref_slide = slides[0]\n",
    "        slide_deck = slides[1:]\n",
    "        length_ref_types = lengths_slide_deck.pop(0)\n",
    "        sample = {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": get_BB_types(ref_slide),\n",
    "            \"slide_deck\": np.asarray(slide_deck),\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleBB(object):\n",
    "    \"\"\"Rescale the bounding boxes in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def _resize_single_slide(self, slide, original_shape, new_shape):\n",
    "        h, w = original_shape\n",
    "        new_h, new_w = new_shape\n",
    "        slide = slide * np.array([new_w / w, new_h / h, new_w / w, new_h / h, 1]).T\n",
    "        return slide\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        ref_slide = self._resize_single_slide(ref_slide, (h, w), (new_h, new_w))\n",
    "        for i, slide in enumerate(slide_deck):\n",
    "            slide_deck[i] = self._resize_single_slide(slide, (h, w), (new_h, new_w))\n",
    "\n",
    "        return {\n",
    "            \"shape\": (new_h, new_w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class LeaveN(object):\n",
    "    def __init__ (self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if slide_deck.shape[0] > self.N:\n",
    "            slide_deck = np.delete(slide_deck, range(self.N, slide_deck.shape[0]), 0)\n",
    "            lengths_slide_deck = lengths_slide_deck[:self.N]\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ShuffleRefSlide(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        lengths_slide_deck.append(length_ref_types)\n",
    "        slide_deck = np.vstack((slide_deck, ref_slide[None, :]))\n",
    "\n",
    "        idxs = np.array([*range(0, len(lengths_slide_deck))], dtype=np.int32)\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        slide_deck = slide_deck[idxs]\n",
    "\n",
    "        lengths_slide_deck = np.array(lengths_slide_deck, dtype=np.int32)\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck.tolist()\n",
    "        \n",
    "        slide_deck = slide_deck.tolist()\n",
    "        ref_slide = np.asarray(slide_deck.pop())\n",
    "        length_ref_types = lengths_slide_deck.pop()\n",
    "        ref_types = get_BB_types(ref_slide)\n",
    "\n",
    "        slide_deck = np.asarray(slide_deck)\n",
    "        \n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ToTensorBB(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        idxs = [*range(0, len(lengths_slide_deck))]\n",
    "\n",
    "        def by_length(p1, p2):\n",
    "            return lengths_slide_deck[p2] - lengths_slide_deck[p1]\n",
    "        idxs = sorted(idxs, key=cmp_to_key(by_length))\n",
    "\n",
    "        shape = torch.tensor([h, w], dtype=torch.float64)\n",
    "        ref_slide = torch.from_numpy(ref_slide).float()\n",
    "        ref_types = torch.from_numpy(ref_types).float()\n",
    "        \n",
    "        slide_deck = torch.from_numpy(slide_deck).float()\n",
    "        lengths_slide_deck = torch.tensor(lengths_slide_deck, dtype=torch.int32)\n",
    "        \n",
    "        slide_deck = slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "\n",
    "        length_ref_types = torch.tensor(length_ref_types, dtype=torch.int32)\n",
    "\n",
    "        return {\n",
    "            \"shape\": shape,\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slide_deck_dataset(all_dataset):\n",
    "    slide_deck_data = {}\n",
    "    for entrance in all_dataset.iloc:\n",
    "        slide_deck_id = entrance['Slide Deck Id']\n",
    "        \n",
    "        slide_id = entrance[\"Slide Id\"]\n",
    "        if (slide_deck_id not in slide_deck_data):\n",
    "            slide_deck_data[slide_deck_id] = {\n",
    "                'slides': {},\n",
    "                'shape': (entrance['Image Height'], entrance['Image Width'])\n",
    "            }\n",
    "        \n",
    "        if slide_id not in slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            slide_deck_data[slide_deck_id][\"slides\"][slide_id] = []\n",
    "        bb_type = BB_TYPES.index(entrance['Type'])\n",
    "        if (bb_type < 0 or bb_type >= len(BB_TYPES)):\n",
    "            bb_type = len(BB_TYPES)\n",
    "\n",
    "        bb = np.array([\n",
    "            entrance['X'],\n",
    "            entrance['Y'],\n",
    "            entrance['BB Width'],\n",
    "            entrance['BB Height'],\n",
    "            bb_type\n",
    "        ]).T\n",
    "        slide_deck_data[slide_deck_id]['slides'][slide_id].append(bb)\n",
    "    for key in slide_deck_data.keys():\n",
    "        \n",
    "        # if key == 100:\n",
    "        #     for (id, value) in slide_deck_data[key][\"slides\"].items():\n",
    "        #         print(56, id)\n",
    "        #         draw_bbs(slide_deck_data[key][\"shape\"], value)\n",
    "\n",
    "        values = list(slide_deck_data[key][\"slides\"].values())\n",
    "        slide_deck_data[key][\"slides\"] = [np.asarray(value) for value in values]\n",
    "    return slide_deck_data\n",
    "\n",
    "def slice_dict(dictionary, l, r):\n",
    "    keys = list(dictionary.keys())\n",
    "    keys = keys[l:r]\n",
    "    ret_dictionary = {}\n",
    "    for key in keys:\n",
    "        ret_dictionary[key] = dictionary[key]\n",
    "    return ret_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"slide_deck_dataset.csv\")\n",
    "\n",
    "dataset = pd.read_csv(csv_file)\n",
    "slide_deck_data = process_slide_deck_dataset(dataset)\n",
    "\n",
    "division = int(args.train_portion * len(slide_deck_data))\n",
    "\n",
    "train_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, 0, division),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(args.slide_deck_N),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, division, len(slide_deck_data)),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(args.slide_deck_N),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAOBUlEQVR4nO3dX4il9X3H8fenuxEakkbJTkK6q51t2fzZCy3J8Q+laU1C6669WAJeqCFSCSxSDblUCk0uvGkuCiGoWRZZJDfZi0aSTdlECiWxYE13Foy6ijJdjU434JiEFMyFrH57MSdyMp6Z88zuc+bs/Ob9goE95/nNzPfHbN77+GTOeVJVSJK2vj+Y9QCSpH4YdElqhEGXpEYYdElqhEGXpEbsnNU33rVrV83Pz8/q20vSlnT69OnXq2pu3LGZBX1+fp6FhYVZfXtJ2pKS/HytY15ykaRGGHRJaoRBl6RGGHRJaoRBl6RGTAx6kmNJXkvy7BrHk+SbSRaTPJ3kk/2PKUmapMsZ+iPAgXWOHwT2DT8OA9+6+LEkSRs18ffQq+rxJPPrLDkEfLtW3of3ySSXJ/lIVf2iryFHJdP4qpK0uabxzuV9XEPfDbw68nhp+Ny7JDmcZCHJwvLycg/fWpL0O328UnTcOfPYf3uq6ihwFGAwGFzkv0+eqkvaiqZ3U6E+ztCXgCtHHu8BzvXwdSVJG9BH0E8Adwx/2+UG4DfTun4uSVrbxEsuSb4D3AjsSrIEfA14D0BVHQFOAjcDi8BvgTunNawkaW1dfsvltgnHC7i7t4kkSRfEV4pKUiMMuiQ1YmY3uNgMLwHzsx5C0iXpZWDvrIfoWdNBn8ffVpc03vR+G3x2vOQiSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY0w6JLUCIMuSY3oFPQkB5K8kGQxyX1jjn8gyQ+S/CzJmSR39j+qJGk9E4OeZAfwIHAQ2A/clmT/qmV3A89V1TXAjcC/JLms51klSevocoZ+HbBYVWer6k3gOHBo1ZoC3p8kwPuAXwHne51UkrSuLkHfDbw68nhp+NyoB4BPAOeAZ4CvVNXbq79QksNJFpIsLC8vX+DIkqRxugQ9Y56rVY9vAp4C/hj4c+CBJH/0rk+qOlpVg6oazM3NbXBUSdJ6ugR9Cbhy5PEeVs7ER90JPForFoGXgI/3M6IkqYsuQT8F7Euyd/h/dN4KnFi15hXgcwBJPgx8DDjb56CSpPXtnLSgqs4nuQd4DNgBHKuqM0nuGh4/AtwPPJLkGVYu0dxbVa9PcW5J0ioTgw5QVSeBk6ueOzLy53PA3/Y7miRpI3ylqCQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiM6BT3JgSQvJFlMct8aa25M8lSSM0l+0u+YkqRJdk5akGQH8CDwN8AScCrJiap6bmTN5cBDwIGqeiXJh6Y0ryRpDV3O0K8DFqvqbFW9CRwHDq1aczvwaFW9AlBVr/U7piRpki5B3w28OvJ4afjcqI8CVyT5cZLTSe4Y94WSHE6ykGRheXn5wiaWJI3VJegZ81yterwT+BTwd8BNwD8l+ei7PqnqaFUNqmowNze34WElSWubeA2dlTPyK0ce7wHOjVnzelW9AbyR5HHgGuDFXqaUJE3U5Qz9FLAvyd4klwG3AidWrfk+8OkkO5O8F7geeL7fUSVJ65l4hl5V55PcAzwG7ACOVdWZJHcNjx+pqueT/Ah4GngbeLiqnp3m4JKk35eq1ZfDN8dgMKiFhYUNf17euaI/7tL+76tOqyRtR7Prw0pzLzS9SU5X1WDcMV8pKkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmNMOiS1AiDLkmN6BT0JAeSvJBkMcl966y7NslbSW7pb0RJUhcTg55kB/AgcBDYD9yWZP8a674OPNb3kJKkyXZ2WHMdsFhVZwGSHAcOAc+tWvdl4LvAtb1OeBFeBmrWQ0i6JL086wGmoEvQdwOvjjxeAq4fXZBkN/B54LOsE/Qkh4HDAFddddVGZ92wvVP/DpJ06ehyDT1jnlt94vsN4N6qemu9L1RVR6tqUFWDubm5jiNKkrrocoa+BFw58ngPcG7VmgFwPAnALuDmJOer6nt9DClJmqxL0E8B+5LsBf4XuBW4fXRBVb1zdSPJI8C/GXNJ2lwTg15V55Pcw8pvr+wAjlXVmSR3DY8fmfKMkqQOupyhU1UngZOrnhsb8qr6+4sfS5K0Ub5SVJIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIaYdAlqRGdbnChC/MSMD/rIaRt5GVg76RFDTPoUzQPZNZDSNtIzXqAGfOSiyQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiM6BT3JgSQvJFlMct+Y419I8vTw44kk1/Q/qiRpPRODnmQH8CBwENgP3JZk/6plLwF/XVVXA/cDR/seVJK0vi5n6NcBi1V1tqreBI4Dh0YXVNUTVfXr4cMngT39jilJmqRL0HcDr448Xho+t5YvAT8cdyDJ4SQLSRaWl5e7TylJmqhL0Mfdo2Hs+8gn+QwrQb933PGqOlpVg6oazM3NdZ9SkjRRlzsWLQFXjjzeA5xbvSjJ1cDDwMGq+mU/40mSuupyhn4K2Jdkb5LLgFuBE6MLklwFPAp8sape7H9MSdIkE8/Qq+p8knuAx4AdwLGqOpPkruHxI8BXgQ8CDyUBOF9Vg+mNLUlaLVWzua3qYDCohYWFDX9e3rmif+nffrnYClNK7dga/5tbae6FpjfJ6bVOmH2lqCQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiMMuiQ1wqBLUiN2znqAlr0M1KyHkLaRl2c9wIwZ9CnaO+sBJG0rXnKRpEYYdElqhEGXpEYYdElqhEGXpEYYdElqhEGXpEZ0CnqSA0leSLKY5L4xx5Pkm8PjTyf5ZP+jSpLWMzHoSXYADwIHgf3AbUn2r1p2ENg3/DgMfKvnOSVJE3R5peh1wGJVnQVIchw4BDw3suYQ8O2qKuDJJJcn+UhV/aL3id/hi+olaVSXSy67gVdHHi8Nn9voGpIcTrKQZGF5eXmjs0qS1tHlDD1jnlt9etxlDVV1FDgKMBgMLugUuzwxl6SxupyhLwFXjjzeA5y7gDWSpCnqEvRTwL4ke5NcBtwKnFi15gRwx/C3XW4AfjPd6+eSpNUmXnKpqvNJ7gEeA3YAx6rqTJK7hsePACeBm4FF4LfAndMbWZI0Tqf3Q6+qk6xEe/S5IyN/LuDufkeTJG2ErxSVpEYYdElqhEGXpEYYdElqRGpGr9RJsgz8/AI/fRfweo/jbAXueXtwz9vDxez5T6pqbtyBmQX9YiRZqKrBrOfYTO55e3DP28O09uwlF0lqhEGXpEZs1aAfnfUAM+Cetwf3vD1MZc9b8hq6JOndtuoZuiRpFYMuSY24pIO+HW9O3WHPXxju9ekkTyS5ZhZz9mnSnkfWXZvkrSS3bOZ809Blz0luTPJUkjNJfrLZM/atw9/tDyT5QZKfDfe8pd+1NcmxJK8leXaN4/33q6ouyQ9W3qr3f4A/BS4DfgbsX7XmZuCHrNwx6Qbgp7OeexP2/BfAFcM/H9wOex5Z9x+svOvnLbOeexN+zpezct/eq4aPPzTruTdhz/8IfH345zngV8Bls579Ivb8V8AngWfXON57vy7lM/R3bk5dVW8Cv7s59ah3bk5dVU8Clyf5yGYP2qOJe66qJ6rq18OHT7Jyd6itrMvPGeDLwHeB1zZzuCnpsufbgUer6hWAqtrq++6y5wLenyTA+1gJ+vnNHbM/VfU4K3tYS+/9upSD3tvNqbeQje7nS6z8C7+VTdxzkt3A54EjtKHLz/mjwBVJfpzkdJI7Nm266eiy5weAT7By+8pngK9U1dubM95M9N6vTje4mJHebk69hXTeT5LPsBL0v5zqRNPXZc/fAO6tqrdWTt62vC573gl8Cvgc8IfAfyV5sqpenPZwU9JlzzcBTwGfBf4M+Pck/1lV/zfl2Wal935dykHfjjen7rSfJFcDDwMHq+qXmzTbtHTZ8wA4Poz5LuDmJOer6nubMmH/uv7dfr2q3gDeSPI4cA2wVYPeZc93Av9cKxeYF5O8BHwc+O/NGXHT9d6vS/mSy3a8OfXEPSe5CngU+OIWPlsbNXHPVbW3quarah74V+AftnDModvf7e8Dn06yM8l7geuB5zd5zj512fMrrPwXCUk+DHwMOLupU26u3vt1yZ6h1za8OXXHPX8V+CDw0PCM9Xxt4Xeq67jnpnTZc1U9n+RHwNPA28DDVTX219+2go4/5/uBR5I8w8rliHurasu+rW6S7wA3AruSLAFfA94D0+uXL/2XpEZcypdcJEkbYNAlqREGXZIaYdAlqREGXZIaYdAlqREGXZIa8f9UtJOlZ+JASQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 7., 0., 0., 0., 0., 0.])\n",
      "tensor([[0.0560, 0.3231, 0.4246, 0.0655, 3.0000],\n",
      "        [0.2544, 0.1220, 0.4930, 0.1013, 1.0000],\n",
      "        [0.5512, 0.4052, 0.3524, 0.1346, 3.0000],\n",
      "        [0.8023, 0.8538, 0.1535, 0.0645, 3.0000],\n",
      "        [0.0590, 0.4083, 0.3376, 0.3246, 3.0000],\n",
      "        [0.5497, 0.8599, 0.0960, 0.0539, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.4083, 0.5307, 0.2234, 0.3105, 5.0000],\n",
      "        [0.7093, 0.5333, 0.1792, 0.2974, 5.0000],\n",
      "        [0.0964, 0.5307, 0.2253, 0.3034, 5.0000],\n",
      "        [0.0575, 0.4617, 0.8662, 0.0595, 3.0000],\n",
      "        [0.0665, 0.0746, 0.8635, 0.2828, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[ 0.2166,  0.2465,  0.5777,  0.7440, 11.0000],\n",
      "        [ 0.0741,  0.1245,  0.8541,  0.0963,  1.0000],\n",
      "        [ 0.4057,  0.9622,  0.2110,  0.0262,  3.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[0.2291, 0.3926, 0.5312, 0.3473, 5.0000],\n",
      "        [0.1509, 0.0585, 0.6960, 0.2263, 1.0000],\n",
      "        [0.2669, 0.7959, 0.4590, 0.0580, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0836, 0.3241, 0.7977, 0.3261, 3.0000],\n",
      "        [0.2242, 0.1174, 0.5539, 0.1089, 1.0000],\n",
      "        [0.0858, 0.7364, 0.6900, 0.0776, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([6, 5, 3, 3, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "single = train_slide_deck_dataset[0]\n",
    "draw_bbs(single[\"shape\"], single[\"ref_slide\"])\n",
    "\n",
    "print(single[\"ref_types\"])\n",
    "for i in range(5):\n",
    "    print(single[\"slide_deck\"][i])\n",
    "print(single[\"lengths_slide_deck\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_slide_deck_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_slide_deck_dataset, batch_size=args.batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlideEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlideEncoder, self).__init__()\n",
    "        ninp = args.ninp\n",
    "        nhid = args.nhid\n",
    "        nlayers = args.nlayers\n",
    "        dropout = args.dropout\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, args.padding_idx)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, bias=True).float()\n",
    "\n",
    "    def forward(self, x, states, lengths=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor(B, L, 5)\n",
    "            states: List[Tuple(h_0, c_0), ..., Tuple(h_{B-1}, c_{B-1})]\n",
    "            lengths: tensor(B)\n",
    "        \"\"\"\n",
    "        idxs = [*range(0, len(lengths))]\n",
    "        def by_lengths(p1, p2):\n",
    "            return lengths[p2] - lengths[p1]\n",
    "\n",
    "        idxs = sorted(idxs, key=cmp_to_key(by_lengths))\n",
    "\n",
    "        x = x[:, idxs]\n",
    "        lengths = lengths[idxs]\n",
    "        \n",
    "        input = x[:, :, :-1]\n",
    "        types = x[:, :, -1:].long()\n",
    "        types = torch.squeeze(self.embed(types))\n",
    "        input = torch.cat((input, types), dim=-1)\n",
    "\n",
    "        h_0 = torch.stack([h for (h, _) in states], dim=0)\n",
    "        c_0 = torch.stack([c for (_, c) in states], dim=0)\n",
    "        \n",
    "        output = self.dropout(input)\n",
    "        (output, context_vector) = self.lstm(output, (h_0, c_0))\n",
    "        return (output, context_vector)\n",
    "\n",
    "class SlideDeckEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlideDeckEncoder, self).__init__()\n",
    "        self.slide_encoder = SlideEncoder()\n",
    "\n",
    "        input_size = args.nhid * args.slide_deck_N\n",
    "        output_size = args.slide_deck_embedding_size\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        return\n",
    "\n",
    "    def _get_init_states(self, x):\n",
    "        init_states = [\n",
    "            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n",
    "            torch.zeros((x.size(1), args.nhid)).to(x.device))\n",
    "            for _ in range(args.nlayers)\n",
    "        ]\n",
    "        return init_states\n",
    "    \n",
    "    def forward(self, xs, lengths):\n",
    "        states = None\n",
    "        embedding = []\n",
    "        for i, x in enumerate(xs):\n",
    "            if states is None:\n",
    "                states = self._get_init_states(x)\n",
    "            length = lengths[i]\n",
    "            output, states = self.slide_encoder(x, states, length)\n",
    "            embedding.append(output[-1:].squeeze())\n",
    "        \n",
    "        output = torch.cat(embedding, dim=-1)\n",
    "        output = self.relu(self.linear(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, torch.autograd.Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "    return torch.autograd.Variable(y_one_hot) if isinstance(y, torch.autograd.Variable) else y_one_hot\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, embed_weights=None, ganlike=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ganlike = ganlike\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size=args.latent_vector_dim + args.embedding_size, hidden_size=args.hidden_size, num_layers=2, \n",
    "            batch_first=True, dropout=args.dropout_rate, bias=True)\n",
    "        self.linear1 = nn.Linear(args.slide_deck_embedding_size, args.hidden_size)\n",
    "        self.linear2 = nn.Linear(args.hidden_size, 4)\n",
    "        if embed_weights:\n",
    "            self.embed.weight.data = embed_weights\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            # if normalize:\n",
    "            #     layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.gen_model = nn.Sequential(\n",
    "            *block(args.hidden_size, 32, normalize=False),\n",
    "            *block(32, 32),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, z, slide_deck_embedding, lengths=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "            z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            lengths (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            bb sequence: (tensor), (Batch_size, Sequence_size, 5)\n",
    "        \"\"\"\n",
    "        # print(x.shape, z.shape, slide_deck_embedding.shape, lengths)\n",
    "        x = x.int()\n",
    "        (Batch_size, Sequence_size) = x.shape\n",
    "        temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        # print(temp_input_1)\n",
    "        # print(\"1\",temp_input_1.shape)\n",
    "        temp_input_2 = z.unsqueeze(1).repeat((1, Sequence_size, 1))\n",
    "        # print(\"2\",temp_input_2.shape)\n",
    "        input_1 = torch.cat((temp_input_2, temp_input_1), dim=-1)\n",
    "        # print(input_1.shape)\n",
    "        input_1 = torch.nn.utils.rnn.pack_padded_sequence(input_1, lengths, batch_first=True)\n",
    "        # print(input_1.data.shape)\n",
    "        # print(\"3\",input_1.shape)\n",
    "        hidden_0 = self.dropout(self.linear1(slide_deck_embedding)).unsqueeze(0).repeat((2, 1, 1))\n",
    "        # print(\"4\",hidden_0.shape)\n",
    "        c_0 = torch.zeros(size=(2, Batch_size, args.hidden_size))\n",
    "        # print(\"5\",c_0.shape)\n",
    "        output, (h_n, c_n) = self.lstm(input_1, (hidden_0, c_0))\n",
    "        # print(output.data.shape)\n",
    "        output, length = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=7)\n",
    "        print(output.shape, length.shape)\n",
    "\n",
    "        print(\"6\",output.shape)\n",
    "        # output = output.transpose(0, 1)\n",
    "        if self.ganlike:\n",
    "            # output = output.transpose(1, 2)\n",
    "            # print(output.shape)\n",
    "            output = self.gen_model(output)\n",
    "        else:\n",
    "            output = self.linear2(output)\n",
    "\n",
    "        return output, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "tensor([1, 0])\n",
      "tensor([[7., 3., 1., 3., 3., 7., 0.],\n",
      "        [1., 5., 5., 0., 0., 0., 0.]]) tensor([6, 3], dtype=torch.int32)\n",
      "torch.Size([2, 7, 64]) torch.Size([2])\n",
      "6 torch.Size([2, 7, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 4])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(args.batch_size)\n",
    "mydec = Generator()\n",
    "z = torch.randn((args.batch_size, args.latent_vector_dim))\n",
    "s = list(train_loader)[0]\n",
    "# print(len(s))\n",
    "# for sl in s:\n",
    "#     print(sl)\n",
    "#     print(s[sl].shape)\n",
    "\n",
    "idx = [i for i in range(s['ref_types'].size(0)-1, -1, -1)]\n",
    "idx = torch.LongTensor(idx)\n",
    "print(idx)\n",
    "t = s['ref_types']\n",
    "l = s['length_ref_types']\n",
    "print(t,l)\n",
    "v = mydec(x=t, z=z, slide_deck_embedding=torch.randn((args.batch_size, args.slide_deck_embedding_size)), lengths=s['length_ref_types'])\n",
    "# draw_bbs(,)\n",
    "v[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1025e-02, -7.1683e-02,  3.4754e-03, -4.3029e-02],\n",
       "         [-1.0758e-02, -7.7055e-02, -7.3799e-03, -4.0994e-02],\n",
       "         [-6.0653e-03, -7.7796e-02, -1.1775e-02, -3.8574e-02],\n",
       "         [-4.1654e-03, -7.9758e-02, -1.3114e-02, -3.9935e-02],\n",
       "         [-7.7862e-03, -8.0171e-02, -1.9412e-02, -4.0557e-02],\n",
       "         [-4.3906e-03, -8.2020e-02, -2.0880e-02, -4.0455e-02],\n",
       "         [ 5.4721e-05, -7.9216e-02, -2.8606e-02, -4.1513e-02]],\n",
       "\n",
       "        [[-2.0010e-02, -5.1413e-02,  4.6737e-04, -3.0619e-02],\n",
       "         [-1.0795e-02, -6.6156e-02, -1.6023e-02, -3.7351e-02],\n",
       "         [ 2.4069e-04, -8.1476e-02, -3.3473e-02, -3.9035e-02],\n",
       "         [ 5.4721e-05, -7.9216e-02, -2.8606e-02, -4.1513e-02],\n",
       "         [ 5.4721e-05, -7.9216e-02, -2.8606e-02, -4.1513e-02],\n",
       "         [ 5.4725e-05, -7.9216e-02, -2.8606e-02, -4.1513e-02],\n",
       "         [ 5.4725e-05, -7.9216e-02, -2.8606e-02, -4.1513e-02]]],\n",
       "       grad_fn=<TanhBackward>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 7, 2, 5])\n",
      "torch.Size([2, 512])\n",
      "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0285, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0253, 0.0000, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = SlideDeckEncoder()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    for batch in train_loader:\n",
    "        xs = torch.transpose(batch[\"slide_deck\"], 0, 1)\n",
    "        xs = torch.transpose(xs, 1, 2)\n",
    "        print(xs.shape)\n",
    "        lengths = torch.transpose(batch[\"lengths_slide_deck\"], 0, 1)\n",
    "        slide_deck_embedding = model(xs, lengths)\n",
    "        print(slide_deck_embedding.shape)\n",
    "        print(slide_deck_embedding)\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_weights=None):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size = 4 + args.embedding_size, hidden_size=args.hidden_size, num_layers=2, \n",
    "            batch_first=True, dropout=args.dropout_rate, bias=True)\n",
    "        self.linear1 = nn.Linear(args.slide_deck_embedding_size, args.hidden_size)\n",
    "        self.d_model = nn.Sequential(\n",
    "            nn.Linear(args.hidden_size, args.hidden_size//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(args.hidden_size//2, args.hidden_size//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(args.hidden_size//2, 1)\n",
    "        )\n",
    "        if embed_weights:\n",
    "            self.embed.weight.data = embed_weights\n",
    "\n",
    "\n",
    "    def forward(self, x, bb, slide_deck_embedding, lengths=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): type labels, (Batch_size, Sequence_size)\n",
    "            bb (tensor): (Batch_size, Sequence_size, 4)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            length (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            \n",
    "        \"\"\"\n",
    "        print(x.shape, bb.shape, slide_deck_embedding.shape, x)\n",
    "        print(\"0\", x)\n",
    "        x = x.int()\n",
    "        (Batch_size, Sequence_size) = x.shape\n",
    "        temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        print(\"1\",temp_input_1.shape)\n",
    "        # temp_input_2 = z.unsqueeze(1).repeat((1, Sequence_size, 1))\n",
    "        input_1 = torch.cat((bb, temp_input_1), dim=-1)\n",
    "        print(input_1.shape)\n",
    "        input_1 = torch.nn.utils.rnn.pack_padded_sequence(input_1, lengths, batch_first=True)\n",
    "        print(\"2\", input_1.data.shape)\n",
    "        # print(\"3\",input_1.shape)\n",
    "        hidden_0 = self.dropout(self.linear1(slide_deck_embedding)).unsqueeze(0).repeat((2, 1, 1))\n",
    "        print(\"4\",hidden_0.shape, hidden_0)\n",
    "        c_0 = torch.zeros(size=(2,Batch_size, args.hidden_size))\n",
    "        # print(\"5\",c_0.shape)\n",
    "        output, (h_n, c_n) = self.lstm(input_1, (hidden_0, c_0))\n",
    "        output, length = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=8)\n",
    "        \n",
    "        print(\"6\",output.shape, length)\n",
    "        print(output)\n",
    "        print(output[:,-1,:])\n",
    "        idx = length - 1 \n",
    "        output = output[:,idx,:].squeeze()\n",
    "        # output = output.transpose(0, 1)\n",
    "        output = self.d_model(output[:,-1,:].squeeze())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7]) torch.Size([2, 7, 4]) torch.Size([2, 512]) tensor([[7., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]])\n",
      "0 tensor([[7., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0.]])\n",
      "1 torch.Size([2, 7, 4])\n",
      "torch.Size([2, 7, 8])\n",
      "2 torch.Size([3, 8])\n",
      "4 torch.Size([2, 2, 64]) tensor([[[ 1.0275, -0.0000, -0.1246,  0.0000,  0.0000,  0.2237,  0.0000,\n",
      "          -0.0000, -0.0000,  0.7183,  0.0000, -0.0000, -0.1357,  0.4598,\n",
      "           1.2323, -0.0000, -0.0000, -0.0000,  0.7344, -0.3467,  0.0000,\n",
      "           0.7701, -0.0000,  1.5187,  0.5315,  0.1335,  0.2635,  0.1886,\n",
      "           0.0000,  1.0565, -2.5709,  1.1317,  0.4148,  0.0000,  0.0000,\n",
      "          -0.0000,  0.0000,  0.2626,  0.0636,  1.1265, -0.0000, -0.6807,\n",
      "          -1.4642,  0.0000,  0.0000,  0.0000, -0.7075, -1.2314, -0.0000,\n",
      "          -0.4988,  0.0000,  2.0654, -0.0000,  0.0000, -0.0000,  1.2825,\n",
      "          -0.0000,  0.0000,  0.0000,  0.0000, -0.5294,  0.0000, -0.0000,\n",
      "           0.5518],\n",
      "         [-0.0000,  0.0000, -0.0000,  1.1735,  0.0000,  0.0000,  0.4736,\n",
      "           0.5759, -0.0141, -0.0000,  0.0000,  1.3789,  0.0000, -0.4228,\n",
      "           0.0000, -0.6134,  0.1104,  0.0000,  0.7403, -0.2955,  1.3278,\n",
      "           0.0000, -0.0000, -1.3156,  1.1378, -0.4931,  0.0000,  1.9330,\n",
      "           0.0000, -0.1286,  0.2998, -1.3675,  1.4062, -0.0000, -0.0000,\n",
      "           0.5186,  1.9055,  0.8333, -0.1303,  0.5027, -0.1606, -0.1918,\n",
      "          -0.3231,  0.0000,  0.0000,  0.0000,  0.5493,  0.0000, -0.4804,\n",
      "           0.6488,  0.0000, -0.0000,  0.2653,  0.0000,  0.9730,  0.0000,\n",
      "           0.8939,  0.0000,  0.0000,  1.0336, -0.4541,  0.0000,  0.1628,\n",
      "           0.0000]],\n",
      "\n",
      "        [[ 1.0275, -0.0000, -0.1246,  0.0000,  0.0000,  0.2237,  0.0000,\n",
      "          -0.0000, -0.0000,  0.7183,  0.0000, -0.0000, -0.1357,  0.4598,\n",
      "           1.2323, -0.0000, -0.0000, -0.0000,  0.7344, -0.3467,  0.0000,\n",
      "           0.7701, -0.0000,  1.5187,  0.5315,  0.1335,  0.2635,  0.1886,\n",
      "           0.0000,  1.0565, -2.5709,  1.1317,  0.4148,  0.0000,  0.0000,\n",
      "          -0.0000,  0.0000,  0.2626,  0.0636,  1.1265, -0.0000, -0.6807,\n",
      "          -1.4642,  0.0000,  0.0000,  0.0000, -0.7075, -1.2314, -0.0000,\n",
      "          -0.4988,  0.0000,  2.0654, -0.0000,  0.0000, -0.0000,  1.2825,\n",
      "          -0.0000,  0.0000,  0.0000,  0.0000, -0.5294,  0.0000, -0.0000,\n",
      "           0.5518],\n",
      "         [-0.0000,  0.0000, -0.0000,  1.1735,  0.0000,  0.0000,  0.4736,\n",
      "           0.5759, -0.0141, -0.0000,  0.0000,  1.3789,  0.0000, -0.4228,\n",
      "           0.0000, -0.6134,  0.1104,  0.0000,  0.7403, -0.2955,  1.3278,\n",
      "           0.0000, -0.0000, -1.3156,  1.1378, -0.4931,  0.0000,  1.9330,\n",
      "           0.0000, -0.1286,  0.2998, -1.3675,  1.4062, -0.0000, -0.0000,\n",
      "           0.5186,  1.9055,  0.8333, -0.1303,  0.5027, -0.1606, -0.1918,\n",
      "          -0.3231,  0.0000,  0.0000,  0.0000,  0.5493,  0.0000, -0.4804,\n",
      "           0.6488,  0.0000, -0.0000,  0.2653,  0.0000,  0.9730,  0.0000,\n",
      "           0.8939,  0.0000,  0.0000,  1.0336, -0.4541,  0.0000,  0.1628,\n",
      "           0.0000]]], grad_fn=<RepeatBackward>)\n",
      "6 torch.Size([2, 8, 64]) tensor([2, 1])\n",
      "tensor([[[ 0.0632, -0.0064,  0.1491,  ...,  0.0247, -0.0954,  0.0472],\n",
      "         [-0.0029, -0.0614,  0.0988,  ...,  0.0512, -0.0125,  0.0475],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-0.2769, -0.1186,  0.0639,  ..., -0.0281, -0.0156,  0.0291],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         ...,\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0597],\n",
       "        [0.0717]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Discriminator()\n",
    "\n",
    "z = torch.randn((args.batch_size, args.latent_vector_dim))\n",
    "s = list(train_loader)[0]\n",
    "# print(len(s))\n",
    "# for sl in s:\n",
    "#     print(sl)\n",
    "#     print(s[sl].shape)\n",
    "# print(v[0])\n",
    "sss = c(x=s['ref_types'], bb = v[0], slide_deck_embedding=torch.randn((args.batch_size, args.slide_deck_embedding_size)), lengths=s['length_ref_types'])\n",
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch=\"last\"):\n",
    "    torch.save(model.state_dict(),  result_dir / f'{type(model).__name__}_{mode}.ckpt')\n",
    "\n",
    "def load_model(model, epoch=\"last\"):\n",
    "    if os.path.exists(result_dir / f'{type(model).__name__}_{mode}.ckpt'):\n",
    "        model.load_state_dict(torch.load(result_dir / f'{type(model).__name__}_{mode}.ckpt'))\n",
    "\n",
    "def load_model(model, epoch=\"last\"):\n",
    "    if os.path.exists(result_dir / f'{type(model).__name__}_{mode}.ckpt'):\n",
    "        model.load_state_dict(torch.load(result_dir / f'{type(model).__name__}_{mode}.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs and ckpts will be saved in : ..\\results\\trial_0\n"
     ]
    }
   ],
   "source": [
    "num_trial=0\n",
    "result_dir= Path(root) / 'results'\n",
    "parent_dir = result_dir / f'trial_{num_trial}'\n",
    "while parent_dir.is_dir():\n",
    "    num_trial = int(parent_dir.name.replace('trial_',''))\n",
    "    parent_dir = result_dir / f'trial_{num_trial+1}'\n",
    "\n",
    "# Modify parent_dir here if you want to resume from a checkpoint, or to rename directory.\n",
    "# parent_dir = result_dir / 'trial_99'\n",
    "print(f'Logs and ckpts will be saved in : {parent_dir}')\n",
    "\n",
    "log_dir = parent_dir\n",
    "ckpt_dir = parent_dir\n",
    "encoder_ckpt_path = parent_dir / 'encoder.pt'\n",
    "generator_ckpt_path = parent_dir / 'generator.pt'\n",
    "discriminator_ckpt_path = parent_dir / 'discriminator.pt'\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "models = {'discriminator': Discriminator().to(device), 'encoder':}\n",
    "run_epoch(epoch, models, optimizers, is_train=True, dataloader=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([2, 2])\n",
      "ref_slide torch.Size([2, 7, 5])\n",
      "ref_types torch.Size([2, 7])\n",
      "slide_deck torch.Size([2, 5, 7, 5])\n",
      "lengths_slide_deck torch.Size([2, 5])\n",
      "length_ref_types torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for k in list(train_loader)[0].keys():\n",
    "    print(k,list(train_loader)[0][k].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(epoch, models, optimizers, is_train=True, dataloader=None):\n",
    "    # total_loss = 0\n",
    "    # n_correct = 0\n",
    "    # n_total = 0\n",
    "    loss_G = 0\n",
    "    loss_D = 0\n",
    "    if is_train:\n",
    "        for model in models:\n",
    "            models[model].train()\n",
    "    else:\n",
    "        for model in models:\n",
    "            models[model].eval()\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        # ['shape', 'ref_slide', 'ref_types', 'slide_deck', 'lengths_slide_deck', 'length_ref_types']\n",
    "        x_slide_deck = batch[\"slide_deck\"].to(device)\n",
    "        x_slide_deck = torch.transpose(x_slide_deck, 0, 1)\n",
    "        x_slide_deck = torch.transpose(x_slide_deck, 1, 2)\n",
    "        lengths_slide_deck = torch.transpose(batch[\"lengths_slide_deck\"], 0, 1).to(device)\n",
    "        length_ref = batch[\"length_ref_types\"].to(device)\n",
    "        slide_deck_embedding = model['encoder'](xs, lengths_slide_deck)\n",
    "        ref_types = batch[\"ref_types\"].to(device)\n",
    "        batch_size, sequence_size = ref_types.shape\n",
    "        # Sample noise as generator input\n",
    "        z = torch.autograd.Variable(torch.FloatTensor(np.random.normal(0, 1, (batch_size, args.latent_vector_dim))))\n",
    "        \n",
    "        #   x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "        #     z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "        #     slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "        #     length (tensor): (Batch_size,)\n",
    "        # (batch_size, seq, 4)\n",
    "        \n",
    "        # Configure input\n",
    "        # both have ref_types\n",
    "        real_layouts_bbs = batch[\"ref_slide\"][:,:,:-1].to(device)\n",
    "        optimizers['generator'].zero_grad()\n",
    "        fake_layouts_bbs = models['generator'](ref_types, z, slide_deck_embedding, length_ref).detach()\n",
    "        # loss_D = -torch.mean(discriminator(real_imgs)) + torch.mean(discriminator(fake_imgs))\n",
    "        #     x (tensor): type labels, (Batch_size, Sequence_size)\n",
    "        #     bb (tensor): (Batch_size, Sequence_size, 4)\n",
    "        #     slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "        #     length (tensor): (Batch_size,)\n",
    "        out_discriminator_ = models['discriminator'](ref_types, real_layouts_bbs, slide_deck_embedding, length_ref)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        if isinstance(model, Transformer):\n",
    "            x, y = x.transpose(0, 1), y.transpose(0, 1)\n",
    "            target = target.transpose(0, 1) #y[:, 1:]\n",
    "        pred = model(x, y, length)\n",
    "        loss = criterion(pred.reshape(-1, trg_ntoken), target.reshape(-1))\n",
    "        n_targets = (target != pad_id).long().sum().item() \n",
    "        n_total += n_targets \n",
    "        n_correct += (pred.argmax(-1) == target)[target != pad_id].long().sum().item()\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n",
    "            optimizer.step()\n",
    "           \n",
    "            \n",
    "        total_loss += loss.item() * n_targets\n",
    "    total_loss /= n_total\n",
    "    print(\"Epoch\", epoch, 'Train' if is_train else 'Valid', \n",
    "          \"Loss\", np.mean(total_loss), \n",
    "          \"Acc\", n_correct / n_total, \n",
    "          \"PPL\", np.exp(total_loss))\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [3, 3],\n",
       "        [4, 4],\n",
       "        [5, 5],\n",
       "        [6, 6],\n",
       "        [7, 7],\n",
       "        [8, 8],\n",
       "        [9, 9]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(10).reshape(-1,1).tile(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45dc61766396859fdffae760accc4b74216ea8fbbed6d1a9d5e8fb1914e35062"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
