{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../'\n",
    "\n",
    "import os, sys\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from functools import cmp_to_key\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import mask_rcnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "#!pip install easydict\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "args = edict()\n",
    "args.batch_size = 2\n",
    "args.nlayers = 2\n",
    "\n",
    "args.embedding_size = 4\n",
    "args.ninp = 4 + args.embedding_size\n",
    "args.nhid = 256 #512\n",
    "\n",
    "args.dropout = 0.2\n",
    "args.gpu = True\n",
    "\n",
    "args.tensorboard = False\n",
    "args.train_portion = 0.7\n",
    "args.slide_deck_N = 5\n",
    "args.slide_deck_embedding_size = 512\n",
    "args.padding_idx = 0\n",
    "args.max_seq_length = 8\n",
    "\n",
    "# Decoder\n",
    "args.latent_vector_dim = 28\n",
    "args.hidden_size = 64\n",
    "args.dropout_rate = 0.5\n",
    "\n",
    "# GAN\n",
    "args.n_epochs = 200\n",
    "args.lr = 0.00005\n",
    "args.n_cpu = 4\n",
    "args.latent_dim = 100\n",
    "args.channels = 1\n",
    "args.clip_value = 0.01\n",
    "args.sample_interval = 400\n",
    "args.n_critic = 20\n",
    "args.b1 = 0.5\n",
    "args.b2 = 0.999\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "# Create directory name.\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tensorboard:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"{str(result_dir)}\" --samples_per_plugin images=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_TYPES = [\n",
    "    '<pad>',\n",
    "    'title',\n",
    "    'header',\n",
    "    'text box',\n",
    "    'footer',\n",
    "    'picture',\n",
    "    'instructor',\n",
    "    'diagram',\n",
    "    'table',\n",
    "    'figure',\n",
    "    'handwriting',\n",
    "    'chart',\n",
    "    'schematic diagram',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbs(shape, bbs):\n",
    "    if (torch.is_tensor(bbs)):\n",
    "        bbs = np.array(bbs.tolist())\n",
    "    if (torch.is_tensor(shape)):\n",
    "        [h, w] = np.array(shape.tolist())\n",
    "        shape = (h, w)\n",
    "    \n",
    "    h, w = shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    background=patches.Rectangle((0, 0), w, h, linewidth=2, edgecolor='b', facecolor='black')\n",
    "    ax.add_patch(background)\n",
    "    for bb in bbs:\n",
    "        rect = patches.Rectangle((bb[0], bb[1]), bb[2], bb[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.autoscale(True, 'both')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_BB_types(bbs):\n",
    "    return bbs[:, 4]\n",
    "\n",
    "class BBSlideDeckDataset(Dataset):\n",
    "    \"\"\" Slide Deck Dataset but with Bounding Boxes\"\"\"\n",
    "    def __init__(self, slide_deck_data, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.slide_deck_data = slide_deck_data\n",
    "        self.slide_deck_ids = list(self.slide_deck_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slide_deck_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        slide_deck_id = self.slide_deck_ids[idx]\n",
    "        (h, w) = self.slide_deck_data[slide_deck_id][\"shape\"]\n",
    "        lengths_slide_deck = []\n",
    "        \n",
    "        slides = []\n",
    "        max_len_bbs = args.max_seq_length\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            lengths_slide_deck.append(len(slide))\n",
    "            np_slide = np.zeros((max_len_bbs, 5), dtype=np.double)\n",
    "            for i, bb in enumerate(slide):\n",
    "                if (i > max_len_bbs):\n",
    "                    break\n",
    "                np_slide[i] = bb\n",
    "            slides.append(np_slide)\n",
    "        ref_slide = slides[0]\n",
    "        slide_deck = slides[1:]\n",
    "        length_ref_types = lengths_slide_deck.pop(0)\n",
    "        sample = {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": get_BB_types(ref_slide),\n",
    "            \"slide_deck\": np.asarray(slide_deck),\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleBB(object):\n",
    "    \"\"\"Rescale the bounding boxes in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def _resize_single_slide(self, slide, original_shape, new_shape):\n",
    "        h, w = original_shape\n",
    "        new_h, new_w = new_shape\n",
    "        slide = slide * np.array([new_w / w, new_h / h, new_w / w, new_h / h, 1]).T\n",
    "        return slide\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        ref_slide = self._resize_single_slide(ref_slide, (h, w), (new_h, new_w))\n",
    "        for i, slide in enumerate(slide_deck):\n",
    "            slide_deck[i] = self._resize_single_slide(slide, (h, w), (new_h, new_w))\n",
    "\n",
    "        return {\n",
    "            \"shape\": (new_h, new_w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class LeaveN(object):\n",
    "    def __init__ (self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if slide_deck.shape[0] > self.N:\n",
    "            slide_deck = np.delete(slide_deck, range(self.N, slide_deck.shape[0]), 0)\n",
    "            lengths_slide_deck = lengths_slide_deck[:self.N]\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ShuffleRefSlide(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        lengths_slide_deck.append(length_ref_types)\n",
    "        slide_deck = np.vstack((slide_deck, ref_slide[None, :]))\n",
    "\n",
    "        idxs = np.array([*range(0, len(lengths_slide_deck))], dtype=np.int32)\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        slide_deck = slide_deck[idxs]\n",
    "\n",
    "        lengths_slide_deck = np.array(lengths_slide_deck, dtype=np.int32)\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck.tolist()\n",
    "        \n",
    "        slide_deck = slide_deck.tolist()\n",
    "        ref_slide = np.asarray(slide_deck.pop())\n",
    "        length_ref_types = lengths_slide_deck.pop()\n",
    "        ref_types = get_BB_types(ref_slide)\n",
    "\n",
    "        slide_deck = np.asarray(slide_deck)\n",
    "        \n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ToTensorBB(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        idxs = [*range(0, len(lengths_slide_deck))]\n",
    "\n",
    "        def by_length(p1, p2):\n",
    "            return lengths_slide_deck[p2] - lengths_slide_deck[p1]\n",
    "        idxs = sorted(idxs, key=cmp_to_key(by_length))\n",
    "\n",
    "        shape = torch.tensor([h, w], dtype=torch.float64)\n",
    "        ref_slide = torch.from_numpy(ref_slide).float()\n",
    "        ref_types = torch.from_numpy(ref_types).float()\n",
    "        \n",
    "        slide_deck = torch.from_numpy(slide_deck).float()\n",
    "        lengths_slide_deck = torch.tensor(lengths_slide_deck, dtype=torch.int32)\n",
    "        \n",
    "        slide_deck = slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "\n",
    "        length_ref_types = torch.tensor(length_ref_types, dtype=torch.int32)\n",
    "\n",
    "        return {\n",
    "            \"shape\": shape,\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slide_deck_dataset(all_dataset):\n",
    "    slide_deck_data = {}\n",
    "    for entrance in all_dataset.iloc:\n",
    "        slide_deck_id = entrance['Slide Deck Id']\n",
    "        \n",
    "        slide_id = entrance[\"Slide Id\"]\n",
    "        if (slide_deck_id not in slide_deck_data):\n",
    "            slide_deck_data[slide_deck_id] = {\n",
    "                'slides': {},\n",
    "                'shape': (entrance['Image Height'], entrance['Image Width'])\n",
    "            }\n",
    "        \n",
    "        if slide_id not in slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            slide_deck_data[slide_deck_id][\"slides\"][slide_id] = []\n",
    "        bb_type = BB_TYPES.index(entrance['Type'])\n",
    "        if (bb_type < 0 or bb_type >= len(BB_TYPES)):\n",
    "            bb_type = len(BB_TYPES)\n",
    "\n",
    "        bb = np.array([\n",
    "            entrance['X'],\n",
    "            entrance['Y'],\n",
    "            entrance['BB Width'],\n",
    "            entrance['BB Height'],\n",
    "            bb_type\n",
    "        ]).T\n",
    "        slide_deck_data[slide_deck_id]['slides'][slide_id].append(bb)\n",
    "    for key in slide_deck_data.keys():\n",
    "        \n",
    "        # if key == 100:\n",
    "        #     for (id, value) in slide_deck_data[key][\"slides\"].items():\n",
    "        #         print(56, id)\n",
    "        #         draw_bbs(slide_deck_data[key][\"shape\"], value)\n",
    "\n",
    "        values = list(slide_deck_data[key][\"slides\"].values())\n",
    "        slide_deck_data[key][\"slides\"] = [np.asarray(value) for value in values]\n",
    "    return slide_deck_data\n",
    "\n",
    "def slice_dict(dictionary, l, r):\n",
    "    keys = list(dictionary.keys())\n",
    "    keys = keys[l:r]\n",
    "    ret_dictionary = {}\n",
    "    for key in keys:\n",
    "        ret_dictionary[key] = dictionary[key]\n",
    "    return ret_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"slide_deck_dataset.csv\")\n",
    "\n",
    "dataset = pd.read_csv(csv_file)\n",
    "slide_deck_data = process_slide_deck_dataset(dataset)\n",
    "\n",
    "division = int(args.train_portion * len(slide_deck_data))\n",
    "\n",
    "train_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, 0, division),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(args.slide_deck_N),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, division, len(slide_deck_data)),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(args.slide_deck_N),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 1., 3., 3., 3., 3., 0., 0.])\n",
      "tensor([[0.2473, 0.1190, 0.5074, 0.1069, 1.0000],\n",
      "        [0.0578, 0.4118, 0.4015, 0.0685, 3.0000],\n",
      "        [0.0586, 0.5060, 0.4004, 0.2107, 3.0000],\n",
      "        [0.0575, 0.3216, 0.2919, 0.0670, 3.0000],\n",
      "        [0.5274, 0.3241, 0.4178, 0.4476, 7.0000],\n",
      "        [0.0575, 0.5025, 0.4026, 0.2923, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.2291, 0.3926, 0.5312, 0.3473, 5.0000],\n",
      "        [0.1509, 0.0585, 0.6960, 0.2263, 1.0000],\n",
      "        [0.2669, 0.7959, 0.4590, 0.0580, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0824, 0.6295, 0.7762, 0.1028, 3.0000],\n",
      "        [0.0711, 0.2939, 0.8537, 0.2132, 3.0000],\n",
      "        [0.3043, 0.1210, 0.3898, 0.0867, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0836, 0.3241, 0.7977, 0.3261, 3.0000],\n",
      "        [0.2242, 0.1174, 0.5539, 0.1089, 1.0000],\n",
      "        [0.0858, 0.7364, 0.6900, 0.0776, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.1856, 0.0585, 0.6272, 0.2268, 1.0000],\n",
      "        [0.0798, 0.3800, 0.3898, 0.3926, 5.0000],\n",
      "        [0.5248, 0.3836, 0.3921, 0.3891, 5.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([6, 3, 3, 3, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "single = train_slide_deck_dataset[0]\n",
    "draw_bbs(single[\"shape\"], single[\"ref_slide\"])\n",
    "\n",
    "print(single[\"ref_types\"])\n",
    "for i in range(5):\n",
    "    print(single[\"slide_deck\"][i])\n",
    "print(single[\"lengths_slide_deck\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortByRefSlide(batch):\n",
    "    idx = [*range(batch[\"ref_slide\"].shape[0])]\n",
    "\n",
    "    def by_length(p1, p2):\n",
    "        return batch[\"length_ref_types\"][p2] - batch[\"length_ref_types\"][p1]\n",
    "    idx = sorted(idx, key=cmp_to_key(by_length))\n",
    "\n",
    "    idx = torch.tensor(idx).to(device).long()\n",
    "    for prop in batch.keys():\n",
    "        batch[prop] = batch[prop][idx]\n",
    "    \n",
    "    return batch\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_slide_deck_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_slide_deck_dataset, batch_size=args.batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlideEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlideEncoder, self).__init__()\n",
    "        ninp = args.ninp\n",
    "        nhid = args.nhid\n",
    "        nlayers = args.nlayers\n",
    "        dropout = args.dropout\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, args.padding_idx)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, bias=True).float()\n",
    "\n",
    "    def forward(self, x, states, lengths=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor(B, L, 5)\n",
    "            states: List[Tuple(h_0, c_0), ..., Tuple(h_{B-1}, c_{B-1})]\n",
    "            lengths: tensor(B)\n",
    "        \"\"\"\n",
    "        idxs = [*range(0, len(lengths))]\n",
    "        def by_lengths(p1, p2):\n",
    "            return lengths[p2] - lengths[p1]\n",
    "\n",
    "        idxs = sorted(idxs, key=cmp_to_key(by_lengths))\n",
    "\n",
    "        x = x[:, idxs]\n",
    "        lengths = lengths[idxs]\n",
    "        \n",
    "        input = x[:, :, :-1]\n",
    "        types = x[:, :, -1:].long()\n",
    "        types = torch.squeeze(self.embed(types))\n",
    "        input = torch.cat((input, types), dim=-1)\n",
    "\n",
    "        output = self.dropout(input)\n",
    "        \n",
    "\n",
    "        output = torch.nn.utils.rnn.pack_padded_sequence(output, lengths)\n",
    "\n",
    "        h_0 = torch.stack([h for (h, _) in states], dim=0)\n",
    "        c_0 = torch.stack([c for (_, c) in states], dim=0)\n",
    "        \n",
    "        (output, context_vector) = self.lstm(output, (h_0, c_0))\n",
    "        output, lengths = torch.nn.utils.rnn.pad_packed_sequence(output, total_length = args.max_seq_length)\n",
    "        return (output, context_vector)\n",
    "\n",
    "class SlideDeckEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlideDeckEncoder, self).__init__()\n",
    "        self.slide_encoder = SlideEncoder()\n",
    "\n",
    "        input_size = args.nhid * args.slide_deck_N\n",
    "        output_size = args.slide_deck_embedding_size\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        return\n",
    "\n",
    "    def _get_init_states(self, x):\n",
    "        init_states = [\n",
    "            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n",
    "            torch.zeros((x.size(1), args.nhid)).to(x.device))\n",
    "            for _ in range(args.nlayers)\n",
    "        ]\n",
    "        return init_states\n",
    "    \n",
    "    def forward(self, xs, lengths):\n",
    "        states = None\n",
    "        embedding = []\n",
    "        for i, x in enumerate(xs):\n",
    "            if states is None:\n",
    "                states = self._get_init_states(x)\n",
    "            length = lengths[i]\n",
    "            output, states = self.slide_encoder(x, states, length)\n",
    "            output = output[length.long() - 1,:,:]\n",
    "            idxs = torch.arange(args.batch_size)\n",
    "            output = output[idxs, idxs, :]\n",
    "            embedding.append(output.squeeze())\n",
    "        \n",
    "        output = torch.cat(embedding, dim=-1)\n",
    "        output = self.relu(self.linear(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SlideDeckEncoder().to(device)\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "    for batch in train_loader:\n",
    "        batch = SortByRefSlide(batch)\n",
    "        break\n",
    "        xs = torch.transpose(batch[\"slide_deck\"], 0, 1)\n",
    "        xs = torch.transpose(xs, 1, 2)\n",
    "        lengths = torch.transpose(batch[\"lengths_slide_deck\"], 0, 1)\n",
    "        slide_deck_embedding = model(xs, lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, torch.autograd.Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "    return torch.autograd.Variable(y_one_hot) if isinstance(y, torch.autograd.Variable) else y_one_hot\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, embed_weights=None, ganlike=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ganlike = ganlike\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size=args.latent_vector_dim + args.embedding_size, hidden_size=args.hidden_size, num_layers=2, \n",
    "            batch_first=True, dropout=args.dropout_rate, bias=True)\n",
    "        self.linear1 = nn.Linear(args.slide_deck_embedding_size, args.hidden_size)\n",
    "        self.linear2 = nn.Linear(args.hidden_size, 4)\n",
    "        if embed_weights:\n",
    "            self.embed.weight.data = embed_weights\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            # if normalize:\n",
    "            #     layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.gen_model = nn.Sequential(\n",
    "            *block(args.hidden_size, 32, normalize=False),\n",
    "            *block(32, 32),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, z, slide_deck_embedding, lengths=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "            z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            lengths (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            bb sequence: (tensor), (Batch_size, Sequence_size, 5)\n",
    "        \"\"\"\n",
    "        # print(x.shape, z.shape, slide_deck_embedding.shape, lengths)\n",
    "        x = x.int()\n",
    "        (Batch_size, Sequence_size) = x.shape\n",
    "        temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        # print(temp_input_1)\n",
    "        # print(\"1\",temp_input_1.shape)\n",
    "        temp_input_2 = z.unsqueeze(1).repeat((1, Sequence_size, 1))\n",
    "        # print(\"2\",temp_input_2.shape)\n",
    "        input_1 = torch.cat((temp_input_2, temp_input_1), dim=-1)\n",
    "        # print(input_1.shape)\n",
    "        input_1 = torch.nn.utils.rnn.pack_padded_sequence(input_1, lengths, batch_first=True)\n",
    "        # print(input_1.data.shape)\n",
    "        # print(\"3\",input_1.shape)\n",
    "        hidden_0 = self.dropout(self.linear1(slide_deck_embedding)).unsqueeze(0).repeat((2, 1, 1))\n",
    "        # print(\"4\",hidden_0.shape)\n",
    "        c_0 = torch.zeros(size=(2, Batch_size, args.hidden_size))\n",
    "        # print(\"5\",c_0.shape)\n",
    "        output, (h_n, c_n) = self.lstm(input_1, (hidden_0, c_0))\n",
    "        # print(output.data.shape)\n",
    "        output, length = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=args.max_seq_length)\n",
    "\n",
    "        # output = output.transpose(0, 1)\n",
    "        if self.ganlike:\n",
    "            # output = output.transpose(1, 2)\n",
    "            # print(output.shape)\n",
    "            output = self.gen_model(output)\n",
    "        else:\n",
    "            output = self.linear2(output)\n",
    "\n",
    "        return output, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 4])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(args.batch_size)\n",
    "mydec = Generator()\n",
    "z = torch.randn((args.batch_size, args.latent_vector_dim))\n",
    "s = SortByRefSlide(list(train_loader)[0])\n",
    "# print(len(s))\n",
    "# for sl in s:\n",
    "#     print(sl)\n",
    "#     print(s[sl].shape)\n",
    "\n",
    "idx = [i for i in range(s['ref_types'].size(0)-1, -1, -1)]\n",
    "idx = torch.LongTensor(idx)\n",
    "#print(idx)\n",
    "t = s['ref_types']\n",
    "l = s['length_ref_types']\n",
    "#print(t,l)\n",
    "v = mydec(x=t, z=z, slide_deck_embedding=torch.randn((args.batch_size, args.slide_deck_embedding_size)), lengths=s['length_ref_types'])\n",
    "# draw_bbs(,)\n",
    "v[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_weights=None):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size = 4 + args.embedding_size, hidden_size=args.hidden_size, num_layers=2, \n",
    "            batch_first=True, dropout=args.dropout_rate, bias=True)\n",
    "        self.linear1 = nn.Linear(args.slide_deck_embedding_size, args.hidden_size)\n",
    "        self.d_model = nn.Sequential(\n",
    "            nn.Linear(args.hidden_size, args.hidden_size//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(args.hidden_size//2, args.hidden_size//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(args.hidden_size//2, 1)\n",
    "        )\n",
    "        if embed_weights:\n",
    "            self.embed.weight.data = embed_weights\n",
    "\n",
    "\n",
    "    def forward(self, x, bb, slide_deck_embedding, lengths=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): type labels, (Batch_size, Sequence_size)\n",
    "            bb (tensor): (Batch_size, Sequence_size, 4)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            length (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            \n",
    "        \"\"\"\n",
    "        # print(x.shape, bb.shape, slide_deck_embedding.shape, x)\n",
    "        # print(\"0\", x)\n",
    "        x = x.int()\n",
    "        (Batch_size, Sequence_size) = x.shape\n",
    "        temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        #print(\"1\",temp_input_1.shape)\n",
    "        # temp_input_2 = z.unsqueeze(1).repeat((1, Sequence_size, 1))\n",
    "        input_1 = torch.cat((bb, temp_input_1), dim=-1)\n",
    "        input_1 = torch.nn.utils.rnn.pack_padded_sequence(input_1, lengths, batch_first=True)\n",
    "        #print(\"2\", input_1.data.shape)\n",
    "        # print(\"3\",input_1.shape)\n",
    "        hidden_0 = self.dropout(self.linear1(slide_deck_embedding)).unsqueeze(0).repeat((2, 1, 1))\n",
    "        #print(\"4\",hidden_0.shape, hidden_0)\n",
    "        c_0 = torch.zeros(size=(2,Batch_size, args.hidden_size))\n",
    "        # print(\"5\",c_0.shape)\n",
    "        output, (h_n, c_n) = self.lstm(input_1, (hidden_0, c_0))\n",
    "        output, length = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=8)\n",
    "        \n",
    "        #print(\"6\",output.shape, length)\n",
    "        #print(output)\n",
    "        #print(output[:,-1,:])\n",
    "        idx = length - 1 \n",
    "        output = output[:,idx,:].squeeze()\n",
    "        # output = output.transpose(0, 1)\n",
    "        output = self.d_model(output[:,-1,:].squeeze())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0669],\n",
       "        [0.0749]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Discriminator()\n",
    "\n",
    "z = torch.randn((args.batch_size, args.latent_vector_dim))\n",
    "s = SortByRefSlide(list(train_loader)[0])\n",
    "# print(len(s))\n",
    "# for sl in s:\n",
    "#     print(sl)\n",
    "#     print(s[sl].shape)\n",
    "# print(v[0])\n",
    "sss = c(x=s['ref_types'], bb = v[0], slide_deck_embedding=torch.randn((args.batch_size, args.slide_deck_embedding_size)), lengths=s['length_ref_types'])\n",
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch=\"last\"):\n",
    "    torch.save(model.state_dict(),  result_dir / f'{type(model).__name__}_{mode}.ckpt')\n",
    "\n",
    "def load_model(model, epoch=\"last\"):\n",
    "    if os.path.exists(result_dir / f'{type(model).__name__}_{mode}.ckpt'):\n",
    "        model.load_state_dict(torch.load(result_dir / f'{type(model).__name__}_{mode}.ckpt'))\n",
    "\n",
    "def load_model(model, epoch=\"last\"):\n",
    "    if os.path.exists(result_dir / f'{type(model).__name__}_{mode}.ckpt'):\n",
    "        model.load_state_dict(torch.load(result_dir / f'{type(model).__name__}_{mode}.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs and ckpts will be saved in : ../results/trial_7\n"
     ]
    }
   ],
   "source": [
    "num_trial=0\n",
    "result_dir= Path(root) / 'results'\n",
    "parent_dir = result_dir / f'trial_{num_trial}'\n",
    "while parent_dir.is_dir():\n",
    "    num_trial = int(parent_dir.name.replace('trial_',''))\n",
    "    parent_dir = result_dir / f'trial_{num_trial+1}'\n",
    "\n",
    "# Modify parent_dir here if you want to resume from a checkpoint, or to rename directory.\n",
    "# parent_dir = result_dir / 'trial_99'\n",
    "print(f'Logs and ckpts will be saved in : {parent_dir}')\n",
    "\n",
    "log_dir = parent_dir\n",
    "ckpt_dir = parent_dir\n",
    "encoder_ckpt_path = parent_dir / 'encoder.pt'\n",
    "generator_ckpt_path = parent_dir / 'generator.pt'\n",
    "discriminator_ckpt_path = parent_dir / 'discriminator.pt'\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([2, 2])\n",
      "ref_slide torch.Size([2, 8, 5])\n",
      "ref_types torch.Size([2, 8])\n",
      "slide_deck torch.Size([2, 5, 8, 5])\n",
      "lengths_slide_deck torch.Size([2, 5])\n",
      "length_ref_types torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "for k in list(train_loader)[0].keys():\n",
    "    print(k,list(train_loader)[0][k].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"discriminator\": Discriminator().to(device),\n",
    "    \"encoder\" : SlideDeckEncoder().to(device),\n",
    "    \"generator\" : Generator().to(device),\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    \"discriminator\": torch.optim.RMSprop(models[\"generator\"].parameters(), lr=args.lr),\n",
    "    \"generator\": torch.optim.RMSprop(models[\"discriminator\"].parameters(), lr=args.lr),\n",
    "    \"encoder\" : torch.optim.RMSprop(models[\"encoder\"].parameters(), lr=args.lr)\n",
    "}\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if device == 'cuda:0' else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(models, optimizers, is_train=True, dataloader=None):\n",
    "    batches_done = 0\n",
    "    for epoch in range(args.n_epochs):\n",
    "        total_loss_G = 0\n",
    "        total_loss_D = 0\n",
    "        G_num = 0\n",
    "        D_num = 0\n",
    "        loss_G = 0\n",
    "        loss_D = 0\n",
    "        if is_train:\n",
    "            for model in models:\n",
    "                models[model].train()\n",
    "        else:\n",
    "            for model in models:\n",
    "                models[model].eval()\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch = SortByRefSlide(batch)\n",
    "\n",
    "            # ['shape', 'ref_slide', 'ref_types', 'slide_deck', 'lengths_slide_deck', 'length_ref_types']\n",
    "\n",
    "            # conditioning\n",
    "            x_slide_deck = batch[\"slide_deck\"].to(device)\n",
    "            length_ref = batch[\"length_ref_types\"].to(device)\n",
    "            ref_types = batch[\"ref_types\"].to(device)\n",
    "            ref_slide = batch[\"ref_slide\"].to(device)\n",
    "\n",
    "            x_slide_deck = torch.transpose(x_slide_deck, 0, 1)\n",
    "            x_slide_deck = torch.transpose(x_slide_deck, 1, 2)\n",
    "            lengths_slide_deck = torch.transpose(batch[\"lengths_slide_deck\"], 0, 1).to(device)\n",
    "            \n",
    "            optimizers[\"encoder\"].zero_grad()\n",
    "            optimizers[\"discriminator\"].zero_grad()\n",
    "\n",
    "            slide_deck_embedding = models['encoder'](x_slide_deck, lengths_slide_deck)\n",
    "            \n",
    "            batch_size, _ = ref_types.shape\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            z = torch.autograd.Variable(Tensor(np.random.normal(0, 1, (batch_size, args.latent_vector_dim))))\n",
    "            \n",
    "            #   x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "            #     z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "            #     slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            #     length (tensor): (Batch_size,)\n",
    "            # (batch_size, seq, 4)\n",
    "            \n",
    "            # Configure input\n",
    "            # both have ref_types\n",
    "            \n",
    "            real_layouts_bbs = ref_slide[:,:,:-1]\n",
    "\n",
    "            fake_layouts_bbs = models['generator'](ref_types, z, slide_deck_embedding, length_ref)[0].detach()\n",
    "\n",
    "            print(\"true: \", models[\"discriminator\"](ref_types, real_layouts_bbs, slide_deck_embedding, length_ref))\n",
    "            print(\"false: \", models[\"discriminator\"](ref_types, fake_layouts_bbs, slide_deck_embedding, length_ref))\n",
    "\n",
    "            loss_D = (-torch.mean(models[\"discriminator\"](ref_types, real_layouts_bbs, slide_deck_embedding, length_ref))\n",
    "                + torch.mean(models[\"discriminator\"](ref_types, fake_layouts_bbs, slide_deck_embedding, length_ref)))\n",
    "            #     x (tensor): type labels, (Batch_size, Sequence_size)\n",
    "            #     bb (tensor): (Batch_size, Sequence_size, 4)\n",
    "            #     slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            #     length (tensor): (Batch_size,)\n",
    "            print(loss_D)\n",
    "            total_loss_D += loss_D.item()\n",
    "            D_num += 1\n",
    "            loss_D.backward()\n",
    "            optimizers[\"discriminator\"].step()\n",
    "            optimizers[\"encoder\"].step()\n",
    "\n",
    "            # # Clip weights of discriminator\n",
    "            # for p in models[\"discriminator\"].parameters():\n",
    "            #     p.data.clamp_(-args.clip_value, args.clip_value)\n",
    "\n",
    "            # Train the generator every n_critic iterations\n",
    "            if i % args.n_critic == 0:\n",
    "                optimizers[\"encoder\"].zero_grad()\n",
    "                optimizers[\"generator\"].zero_grad()\n",
    "\n",
    "                slide_deck_embedding = models['encoder'](x_slide_deck, lengths_slide_deck)\n",
    "                layouts_bbs = models['generator'](ref_types, z, slide_deck_embedding, length_ref)[0]\n",
    "                \n",
    "                # Adversarial loss\n",
    "                loss_G = -torch.mean(models[\"discriminator\"](ref_types, layouts_bbs, slide_deck_embedding, length_ref))\n",
    "                total_loss_G += loss_G.item()\n",
    "                G_num += 1\n",
    "                loss_G.backward()\n",
    "                optimizers[\"generator\"].step()\n",
    "                optimizers[\"encoder\"].step()\n",
    "        \n",
    "            #if batches_done % args.sample_interval == 0:\n",
    "            batches_done += 1\n",
    "           \n",
    "        print(\n",
    "            \"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, args.n_epochs, total_loss_D/D_num, total_loss_G/G_num)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true:  tensor([[2.6854],\n",
      "        [2.6465]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.6756],\n",
      "        [2.6559]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0052, grad_fn=<AddBackward0>)\n",
      "[Epoch 0/200] [D loss: 0.005195] [G loss: -2.663949]\n",
      "true:  tensor([[2.5559],\n",
      "        [2.5740]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.5221],\n",
      "        [2.5342]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0121, grad_fn=<AddBackward0>)\n",
      "[Epoch 1/200] [D loss: -0.012094] [G loss: -2.560165]\n",
      "true:  tensor([[2.5730],\n",
      "        [2.5256]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.5922],\n",
      "        [2.5644]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0092, grad_fn=<AddBackward0>)\n",
      "[Epoch 2/200] [D loss: 0.009213] [G loss: -2.577099]\n",
      "true:  tensor([[2.5549],\n",
      "        [2.5363]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.5952],\n",
      "        [2.5267]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0172, grad_fn=<AddBackward0>)\n",
      "[Epoch 3/200] [D loss: 0.017179] [G loss: -2.579963]\n",
      "true:  tensor([[2.1638],\n",
      "        [2.1638]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.1638],\n",
      "        [2.1638]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 4/200] [D loss: 0.000000] [G loss: -2.163818]\n",
      "true:  tensor([[2.5662],\n",
      "        [2.5997]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.5929],\n",
      "        [2.5919]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0090, grad_fn=<AddBackward0>)\n",
      "[Epoch 5/200] [D loss: -0.008970] [G loss: -2.545279]\n",
      "true:  tensor([[2.7327],\n",
      "        [2.7336]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7494],\n",
      "        [2.7308]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0088, grad_fn=<AddBackward0>)\n",
      "[Epoch 6/200] [D loss: 0.008766] [G loss: -2.741445]\n",
      "true:  tensor([[2.6029],\n",
      "        [2.5883]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.6450],\n",
      "        [2.6418]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0251, grad_fn=<AddBackward0>)\n",
      "[Epoch 7/200] [D loss: -0.025075] [G loss: -2.629330]\n",
      "true:  tensor([[2.7524],\n",
      "        [2.7560]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7711],\n",
      "        [2.7801]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0088, grad_fn=<AddBackward0>)\n",
      "[Epoch 8/200] [D loss: 0.008759] [G loss: -2.752386]\n",
      "true:  tensor([[2.7949],\n",
      "        [2.7959]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7954],\n",
      "        [2.8118]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0063, grad_fn=<AddBackward0>)\n",
      "[Epoch 9/200] [D loss: 0.006258] [G loss: -2.784945]\n",
      "true:  tensor([[2.7870],\n",
      "        [2.7428]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7909],\n",
      "        [2.7826]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0014, grad_fn=<AddBackward0>)\n",
      "[Epoch 10/200] [D loss: 0.001353] [G loss: -2.767253]\n",
      "true:  tensor([[2.8023],\n",
      "        [0.8283]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7982],\n",
      "        [1.1563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0093, grad_fn=<AddBackward0>)\n",
      "[Epoch 11/200] [D loss: -0.009261] [G loss: -2.003034]\n",
      "true:  tensor([[2.6719],\n",
      "        [2.6771]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.6574],\n",
      "        [2.6639]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0145, grad_fn=<AddBackward0>)\n",
      "[Epoch 12/200] [D loss: -0.014527] [G loss: -2.698752]\n",
      "true:  tensor([[2.8770],\n",
      "        [2.8650]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.8471],\n",
      "        [2.8562]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0003, grad_fn=<AddBackward0>)\n",
      "[Epoch 13/200] [D loss: -0.000280] [G loss: -2.845829]\n",
      "true:  tensor([[2.8280],\n",
      "        [2.8267]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.8370],\n",
      "        [2.8431]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0200, grad_fn=<AddBackward0>)\n",
      "[Epoch 14/200] [D loss: -0.020022] [G loss: -2.848166]\n",
      "true:  tensor([[2.2651],\n",
      "        [2.2651]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.2651],\n",
      "        [2.2651]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 15/200] [D loss: 0.000000] [G loss: -2.265089]\n",
      "true:  tensor([[2.7292],\n",
      "        [2.6701]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7207],\n",
      "        [2.7098]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0525, grad_fn=<AddBackward0>)\n",
      "[Epoch 16/200] [D loss: -0.052497] [G loss: -2.718816]\n",
      "true:  tensor([[2.7185],\n",
      "        [2.7246]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.6704],\n",
      "        [2.7319]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0094, grad_fn=<AddBackward0>)\n",
      "[Epoch 17/200] [D loss: -0.009364] [G loss: -2.733429]\n",
      "true:  tensor([[2.8747],\n",
      "        [2.9054]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.8708],\n",
      "        [2.9018]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0135, grad_fn=<AddBackward0>)\n",
      "[Epoch 18/200] [D loss: 0.013497] [G loss: -2.879014]\n",
      "true:  tensor([[2.6932],\n",
      "        [2.7387]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7577],\n",
      "        [2.7376]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0095, grad_fn=<AddBackward0>)\n",
      "[Epoch 19/200] [D loss: 0.009456] [G loss: -2.757727]\n",
      "true:  tensor([[2.7822],\n",
      "        [2.7979]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7519],\n",
      "        [2.7908]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0262, grad_fn=<AddBackward0>)\n",
      "[Epoch 20/200] [D loss: -0.026237] [G loss: -2.754570]\n",
      "true:  tensor([[2.9329],\n",
      "        [2.9269]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.8959],\n",
      "        [2.9117]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0259, grad_fn=<AddBackward0>)\n",
      "[Epoch 21/200] [D loss: 0.025882] [G loss: -2.920012]\n",
      "true:  tensor([[2.9449],\n",
      "        [2.9563]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.9372],\n",
      "        [2.9102]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0279, grad_fn=<AddBackward0>)\n",
      "[Epoch 22/200] [D loss: 0.027851] [G loss: -2.945209]\n",
      "true:  tensor([[2.7504],\n",
      "        [2.7671]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7954],\n",
      "        [2.7703]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0101, grad_fn=<AddBackward0>)\n",
      "[Epoch 23/200] [D loss: -0.010082] [G loss: -2.811837]\n",
      "true:  tensor([[2.9584],\n",
      "        [2.9653]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.9512],\n",
      "        [2.9747]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0108, grad_fn=<AddBackward0>)\n",
      "[Epoch 24/200] [D loss: 0.010773] [G loss: -2.946341]\n",
      "true:  tensor([[2.3604],\n",
      "        [2.3604]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.3604],\n",
      "        [2.3604]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 25/200] [D loss: 0.000000] [G loss: -2.360434]\n",
      "true:  tensor([[2.9901],\n",
      "        [2.9623]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.9816],\n",
      "        [2.9810]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0094, grad_fn=<AddBackward0>)\n",
      "[Epoch 26/200] [D loss: 0.009351] [G loss: -2.986900]\n",
      "true:  tensor([[3.0325],\n",
      "        [3.0434]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.0333],\n",
      "        [3.0336]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0004, grad_fn=<AddBackward0>)\n",
      "[Epoch 27/200] [D loss: 0.000362] [G loss: -3.041489]\n",
      "true:  tensor([[3.0198],\n",
      "        [1.0784]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.0149],\n",
      "        [1.0530]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.1054, grad_fn=<AddBackward0>)\n",
      "[Epoch 28/200] [D loss: 0.105357] [G loss: -2.070318]\n",
      "true:  tensor([[3.0161],\n",
      "        [3.0172]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.0254],\n",
      "        [3.0054]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0019, grad_fn=<AddBackward0>)\n",
      "[Epoch 29/200] [D loss: -0.001853] [G loss: -3.017717]\n",
      "true:  tensor([[2.8622],\n",
      "        [2.9021]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.8705],\n",
      "        [2.8173]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0339, grad_fn=<AddBackward0>)\n",
      "[Epoch 30/200] [D loss: -0.033889] [G loss: -2.851555]\n",
      "true:  tensor([[2.9384],\n",
      "        [2.8814]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.8961],\n",
      "        [2.9181]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0098, grad_fn=<AddBackward0>)\n",
      "[Epoch 31/200] [D loss: 0.009832] [G loss: -2.917475]\n",
      "true:  tensor([[2.4254],\n",
      "        [2.4254]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.4254],\n",
      "        [2.4254]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 32/200] [D loss: 0.000000] [G loss: -2.425425]\n",
      "true:  tensor([[3.0529],\n",
      "        [3.0481]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.0422],\n",
      "        [3.0366]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0197, grad_fn=<AddBackward0>)\n",
      "[Epoch 33/200] [D loss: 0.019680] [G loss: -3.059881]\n",
      "true:  tensor([[3.1406],\n",
      "        [3.1033]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.1178],\n",
      "        [3.1171]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0153, grad_fn=<AddBackward0>)\n",
      "[Epoch 34/200] [D loss: -0.015330] [G loss: -3.100910]\n",
      "true:  tensor([[2.9361],\n",
      "        [2.9022]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.9312],\n",
      "        [2.9564]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0243, grad_fn=<AddBackward0>)\n",
      "[Epoch 35/200] [D loss: 0.024325] [G loss: -2.901117]\n",
      "true:  tensor([[3.1309],\n",
      "        [3.1493]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.1497],\n",
      "        [3.1560]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0018, grad_fn=<AddBackward0>)\n",
      "[Epoch 36/200] [D loss: 0.001826] [G loss: -3.149558]\n",
      "true:  tensor([[2.4743],\n",
      "        [2.4743]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.4743],\n",
      "        [2.4743]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 37/200] [D loss: 0.000000] [G loss: -2.474325]\n",
      "true:  tensor([[3.1751],\n",
      "        [3.1616]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.1586],\n",
      "        [3.1629]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0182, grad_fn=<AddBackward0>)\n",
      "[Epoch 38/200] [D loss: 0.018214] [G loss: -3.160114]\n",
      "true:  tensor([[2.4930],\n",
      "        [0.4173]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.4930],\n",
      "        [0.6287]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0048, grad_fn=<AddBackward0>)\n",
      "[Epoch 39/200] [D loss: 0.004801] [G loss: -1.479365]\n",
      "true:  tensor([[3.1854],\n",
      "        [3.1711]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.1916],\n",
      "        [3.1663]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0020, grad_fn=<AddBackward0>)\n",
      "[Epoch 40/200] [D loss: 0.002037] [G loss: -3.179663]\n",
      "true:  tensor([[3.0169],\n",
      "        [3.0093]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.0125],\n",
      "        [3.0230]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0334, grad_fn=<AddBackward0>)\n",
      "[Epoch 41/200] [D loss: -0.033422] [G loss: -3.020667]\n",
      "true:  tensor([[2.5184],\n",
      "        [2.5184]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.5184],\n",
      "        [2.5184]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 42/200] [D loss: 0.000000] [G loss: -2.518447]\n",
      "true:  tensor([[3.0077],\n",
      "        [3.0327]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.0625],\n",
      "        [3.0226]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0003, grad_fn=<AddBackward0>)\n",
      "[Epoch 43/200] [D loss: -0.000325] [G loss: -3.043036]\n",
      "true:  tensor([[3.1807],\n",
      "        [3.2056]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2056],\n",
      "        [3.2161]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0049, grad_fn=<AddBackward0>)\n",
      "[Epoch 44/200] [D loss: 0.004920] [G loss: -3.181711]\n",
      "true:  tensor([[3.0975],\n",
      "        [3.0746]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.9630],\n",
      "        [3.0529]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0286, grad_fn=<AddBackward0>)\n",
      "[Epoch 45/200] [D loss: -0.028580] [G loss: -3.076039]\n",
      "true:  tensor([[3.2292],\n",
      "        [3.2267]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2297],\n",
      "        [3.2380]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0159, grad_fn=<AddBackward0>)\n",
      "[Epoch 46/200] [D loss: 0.015866] [G loss: -3.247263]\n",
      "true:  tensor([[3.2824],\n",
      "        [3.2928]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2884],\n",
      "        [3.2851]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0119, grad_fn=<AddBackward0>)\n",
      "[Epoch 47/200] [D loss: -0.011873] [G loss: -3.268086]\n",
      "true:  tensor([[3.2709],\n",
      "        [3.2459]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2788],\n",
      "        [3.2557]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0037, grad_fn=<AddBackward0>)\n",
      "[Epoch 48/200] [D loss: 0.003725] [G loss: -3.248384]\n",
      "true:  tensor([[3.2403],\n",
      "        [3.2669]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2720],\n",
      "        [3.2451]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0104, grad_fn=<AddBackward0>)\n",
      "[Epoch 49/200] [D loss: 0.010372] [G loss: -3.269134]\n",
      "true:  tensor([[3.1723],\n",
      "        [3.1129]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.1403],\n",
      "        [3.1397]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0255, grad_fn=<AddBackward0>)\n",
      "[Epoch 50/200] [D loss: -0.025511] [G loss: -3.118923]\n",
      "true:  tensor([[2.6100],\n",
      "        [2.6100]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.6100],\n",
      "        [2.6100]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 51/200] [D loss: 0.000000] [G loss: -2.610045]\n",
      "true:  tensor([[3.2993],\n",
      "        [3.3174]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.3214],\n",
      "        [3.3059]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0127, grad_fn=<AddBackward0>)\n",
      "[Epoch 52/200] [D loss: 0.012671] [G loss: -3.293089]\n",
      "true:  tensor([[3.3212],\n",
      "        [3.2919]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.3165],\n",
      "        [3.3281]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0216, grad_fn=<AddBackward0>)\n",
      "[Epoch 53/200] [D loss: 0.021585] [G loss: -3.341755]\n",
      "true:  tensor([[3.1496],\n",
      "        [3.1641]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2059],\n",
      "        [3.2214]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0016, grad_fn=<AddBackward0>)\n",
      "[Epoch 54/200] [D loss: -0.001585] [G loss: -3.175584]\n",
      "true:  tensor([[3.3683],\n",
      "        [3.3447]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.3670],\n",
      "        [3.3489]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0183, grad_fn=<AddBackward0>)\n",
      "[Epoch 55/200] [D loss: 0.018303] [G loss: -3.356655]\n",
      "true:  tensor([[3.3729],\n",
      "        [3.3729]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.3662],\n",
      "        [3.3612]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0177, grad_fn=<AddBackward0>)\n",
      "[Epoch 56/200] [D loss: -0.017673] [G loss: -3.356777]\n",
      "true:  tensor([[3.4099],\n",
      "        [1.3675]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.3995],\n",
      "        [2.0010]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0505, grad_fn=<AddBackward0>)\n",
      "[Epoch 57/200] [D loss: 0.050500] [G loss: -2.896033]\n",
      "true:  tensor([[3.2439],\n",
      "        [3.2353]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2306],\n",
      "        [3.2324]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0151, grad_fn=<AddBackward0>)\n",
      "[Epoch 58/200] [D loss: -0.015079] [G loss: -3.260390]\n",
      "true:  tensor([[2.6913],\n",
      "        [2.6913]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.6913],\n",
      "        [2.6913]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 59/200] [D loss: 0.000000] [G loss: -2.691323]\n",
      "true:  tensor([[3.4639],\n",
      "        [3.4526]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.4445],\n",
      "        [3.4445]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0072, grad_fn=<AddBackward0>)\n",
      "[Epoch 60/200] [D loss: -0.007166] [G loss: -3.453009]\n",
      "true:  tensor([[3.4248],\n",
      "        [3.4395]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.4253],\n",
      "        [3.4211]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0048, grad_fn=<AddBackward0>)\n",
      "[Epoch 61/200] [D loss: 0.004839] [G loss: -3.437481]\n",
      "true:  tensor([[3.4509],\n",
      "        [3.4440]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.4524],\n",
      "        [3.4608]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0020, grad_fn=<AddBackward0>)\n",
      "[Epoch 62/200] [D loss: -0.001981] [G loss: -3.445132]\n",
      "true:  tensor([[3.4942],\n",
      "        [3.5014]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.5078],\n",
      "        [3.4914]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0103, grad_fn=<AddBackward0>)\n",
      "[Epoch 63/200] [D loss: 0.010278] [G loss: -3.479118]\n",
      "true:  tensor([[3.2936],\n",
      "        [0.8131]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2740],\n",
      "        [1.1325]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0092, grad_fn=<AddBackward0>)\n",
      "[Epoch 64/200] [D loss: -0.009224] [G loss: -2.093941]\n",
      "true:  tensor([[3.4542],\n",
      "        [1.5512]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.4927],\n",
      "        [1.4682]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0874, grad_fn=<AddBackward0>)\n",
      "[Epoch 65/200] [D loss: -0.087365] [G loss: -2.549284]\n",
      "true:  tensor([[2.7584],\n",
      "        [2.7584]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7584],\n",
      "        [2.7584]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 66/200] [D loss: 0.000000] [G loss: -2.758373]\n",
      "true:  tensor([[3.5244],\n",
      "        [3.5041]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.4966],\n",
      "        [3.5083]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0065, grad_fn=<AddBackward0>)\n",
      "[Epoch 67/200] [D loss: -0.006484] [G loss: -3.500608]\n",
      "true:  tensor([[2.7782],\n",
      "        [2.7782]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.7782],\n",
      "        [2.7782]], grad_fn=<AddmmBackward0>)\n",
      "tensor(2.3842e-07, grad_fn=<AddBackward0>)\n",
      "[Epoch 68/200] [D loss: 0.000000] [G loss: -2.778170]\n",
      "true:  tensor([[3.5278],\n",
      "        [3.5410]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.5303],\n",
      "        [3.5345]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0080, grad_fn=<AddBackward0>)\n",
      "[Epoch 69/200] [D loss: -0.008007] [G loss: -3.522443]\n",
      "true:  tensor([[3.5452],\n",
      "        [3.5193]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.5544],\n",
      "        [3.5451]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0164, grad_fn=<AddBackward0>)\n",
      "[Epoch 70/200] [D loss: 0.016359] [G loss: -3.542102]\n",
      "true:  tensor([[3.5721],\n",
      "        [3.5362]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.5712],\n",
      "        [3.5266]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0024, grad_fn=<AddBackward0>)\n",
      "[Epoch 71/200] [D loss: 0.002426] [G loss: -3.554817]\n",
      "true:  tensor([[3.6056],\n",
      "        [3.5913]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.5954],\n",
      "        [3.5804]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0062, grad_fn=<AddBackward0>)\n",
      "[Epoch 72/200] [D loss: -0.006187] [G loss: -3.600821]\n",
      "true:  tensor([[3.4415],\n",
      "        [3.4522]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.4141],\n",
      "        [3.4298]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0286, grad_fn=<AddBackward0>)\n",
      "[Epoch 73/200] [D loss: -0.028641] [G loss: -3.391793]\n",
      "true:  tensor([[3.4199],\n",
      "        [3.4107]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.3976],\n",
      "        [3.4346]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0080, grad_fn=<AddBackward0>)\n",
      "[Epoch 74/200] [D loss: -0.008031] [G loss: -3.450855]\n",
      "true:  tensor([[3.4644],\n",
      "        [3.4469]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.4614],\n",
      "        [3.4811]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0182, grad_fn=<AddBackward0>)\n",
      "[Epoch 75/200] [D loss: -0.018204] [G loss: -3.448767]\n",
      "true:  tensor([[3.6654],\n",
      "        [3.6634]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.6582],\n",
      "        [3.6891]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0079, grad_fn=<AddBackward0>)\n",
      "[Epoch 76/200] [D loss: -0.007863] [G loss: -3.668599]\n",
      "true:  tensor([[3.6908],\n",
      "        [3.6731]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.6780],\n",
      "        [3.7054]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0103, grad_fn=<AddBackward0>)\n",
      "[Epoch 77/200] [D loss: -0.010314] [G loss: -3.683260]\n",
      "true:  tensor([[3.4690],\n",
      "        [3.4874]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.4644],\n",
      "        [3.5125]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0182, grad_fn=<AddBackward0>)\n",
      "[Epoch 78/200] [D loss: -0.018183] [G loss: -3.504688]\n",
      "true:  tensor([[3.7149],\n",
      "        [2.3477]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.7008],\n",
      "        [2.5292]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.1332, grad_fn=<AddBackward0>)\n",
      "[Epoch 79/200] [D loss: 0.133219] [G loss: -3.369641]\n",
      "true:  tensor([[3.6787],\n",
      "        [3.7038]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.6774],\n",
      "        [3.6969]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0200, grad_fn=<AddBackward0>)\n",
      "[Epoch 80/200] [D loss: 0.020009] [G loss: -3.665824]\n",
      "true:  tensor([[2.9189],\n",
      "        [2.9189]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[2.9189],\n",
      "        [2.9189]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 81/200] [D loss: 0.000000] [G loss: -2.918852]\n",
      "true:  tensor([[3.7194],\n",
      "        [3.7174]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.7069],\n",
      "        [3.7313]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0050, grad_fn=<AddBackward0>)\n",
      "[Epoch 82/200] [D loss: -0.005033] [G loss: -3.719797]\n",
      "true:  tensor([[3.7354],\n",
      "        [3.7297]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.7058],\n",
      "        [3.7284]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0130, grad_fn=<AddBackward0>)\n",
      "[Epoch 83/200] [D loss: -0.012993] [G loss: -3.730888]\n",
      "true:  tensor([[3.5564],\n",
      "        [3.5705]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.5447],\n",
      "        [3.5848]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0234, grad_fn=<AddBackward0>)\n",
      "[Epoch 84/200] [D loss: 0.023439] [G loss: -3.595912]\n",
      "true:  tensor([[3.5656],\n",
      "        [3.6072]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.5790],\n",
      "        [3.6205]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0420, grad_fn=<AddBackward0>)\n",
      "[Epoch 85/200] [D loss: -0.041952] [G loss: -3.605934]\n",
      "true:  tensor([[3.6274],\n",
      "        [3.6213]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.6365],\n",
      "        [3.5949]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0392, grad_fn=<AddBackward0>)\n",
      "[Epoch 86/200] [D loss: 0.039240] [G loss: -3.635501]\n",
      "true:  tensor([[3.7578],\n",
      "        [3.7923]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.7966],\n",
      "        [3.7911]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0045, grad_fn=<AddBackward0>)\n",
      "[Epoch 87/200] [D loss: 0.004502] [G loss: -3.786919]\n",
      "true:  tensor([[3.6490],\n",
      "        [3.6553]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.6588],\n",
      "        [3.6342]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0134, grad_fn=<AddBackward0>)\n",
      "[Epoch 88/200] [D loss: 0.013391] [G loss: -3.633616]\n",
      "true:  tensor([[3.0061],\n",
      "        [3.0061]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.0061],\n",
      "        [3.0061]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 89/200] [D loss: 0.000000] [G loss: -3.006091]\n",
      "true:  tensor([[3.0152],\n",
      "        [0.7840]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.0152],\n",
      "        [0.7212]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0368, grad_fn=<AddBackward0>)\n",
      "[Epoch 90/200] [D loss: -0.036761] [G loss: -1.840083]\n",
      "true:  tensor([[3.8231],\n",
      "        [3.8202]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.8410],\n",
      "        [3.8549]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0042, grad_fn=<AddBackward0>)\n",
      "[Epoch 91/200] [D loss: 0.004182] [G loss: -3.830224]\n",
      "true:  tensor([[3.6803],\n",
      "        [3.6867]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.6942],\n",
      "        [3.6875]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0384, grad_fn=<AddBackward0>)\n",
      "[Epoch 92/200] [D loss: 0.038395] [G loss: -3.698364]\n",
      "true:  tensor([[3.7059],\n",
      "        [3.6977]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.7028],\n",
      "        [3.7435]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0121, grad_fn=<AddBackward0>)\n",
      "[Epoch 93/200] [D loss: -0.012104] [G loss: -3.666745]\n",
      "true:  tensor([[3.9056],\n",
      "        [3.9028]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9032],\n",
      "        [3.9251]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0101, grad_fn=<AddBackward0>)\n",
      "[Epoch 94/200] [D loss: -0.010131] [G loss: -3.897639]\n",
      "true:  tensor([[3.6891],\n",
      "        [3.6951]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.7124],\n",
      "        [3.7242]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0178, grad_fn=<AddBackward0>)\n",
      "[Epoch 95/200] [D loss: -0.017778] [G loss: -3.720618]\n",
      "true:  tensor([[3.0775],\n",
      "        [0.6117]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.0775],\n",
      "        [0.7011]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0392, grad_fn=<AddBackward0>)\n",
      "[Epoch 96/200] [D loss: -0.039160] [G loss: -1.868740]\n",
      "true:  tensor([[3.0831],\n",
      "        [3.0831]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.0831],\n",
      "        [3.0831]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 97/200] [D loss: 0.000000] [G loss: -3.083090]\n",
      "true:  tensor([[3.7428],\n",
      "        [3.7488]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.7584],\n",
      "        [3.7638]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0106, grad_fn=<AddBackward0>)\n",
      "[Epoch 98/200] [D loss: 0.010577] [G loss: -3.739711]\n",
      "true:  tensor([[3.9448],\n",
      "        [3.9152]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9592],\n",
      "        [3.9389]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0311, grad_fn=<AddBackward0>)\n",
      "[Epoch 99/200] [D loss: 0.031066] [G loss: -3.947105]\n",
      "true:  tensor([[3.1155],\n",
      "        [3.1154]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.1155],\n",
      "        [3.1154]], grad_fn=<AddmmBackward0>)\n",
      "tensor(9.5367e-07, grad_fn=<AddBackward0>)\n",
      "[Epoch 100/200] [D loss: 0.000001] [G loss: -3.115482]\n",
      "true:  tensor([[4.0012],\n",
      "        [3.9948]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9864],\n",
      "        [4.0060]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0093, grad_fn=<AddBackward0>)\n",
      "[Epoch 101/200] [D loss: 0.009309] [G loss: -4.005291]\n",
      "true:  tensor([[4.0374],\n",
      "        [4.0327]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.0187],\n",
      "        [4.0298]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0015, grad_fn=<AddBackward0>)\n",
      "[Epoch 102/200] [D loss: -0.001477] [G loss: -4.018067]\n",
      "true:  tensor([[4.0090],\n",
      "        [2.2111]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.0054],\n",
      "        [2.2066]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.3455, grad_fn=<AddBackward0>)\n",
      "[Epoch 103/200] [D loss: 0.345503] [G loss: -2.994958]\n",
      "true:  tensor([[4.0193],\n",
      "        [4.0101]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.0167],\n",
      "        [4.0131]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0021, grad_fn=<AddBackward0>)\n",
      "[Epoch 104/200] [D loss: 0.002128] [G loss: -4.012720]\n",
      "true:  tensor([[4.0522],\n",
      "        [4.0351]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.0128],\n",
      "        [4.0281]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0038, grad_fn=<AddBackward0>)\n",
      "[Epoch 105/200] [D loss: -0.003777] [G loss: -4.025894]\n",
      "true:  tensor([[4.0162],\n",
      "        [4.0385]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.0393],\n",
      "        [4.0563]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0139, grad_fn=<AddBackward0>)\n",
      "[Epoch 106/200] [D loss: 0.013878] [G loss: -4.055413]\n",
      "true:  tensor([[3.8638],\n",
      "        [3.9072]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.8787],\n",
      "        [3.8970]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0162, grad_fn=<AddBackward0>)\n",
      "[Epoch 107/200] [D loss: 0.016163] [G loss: -3.882145]\n",
      "true:  tensor([[3.8927],\n",
      "        [3.9041]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9167],\n",
      "        [3.9007]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0065, grad_fn=<AddBackward0>)\n",
      "[Epoch 108/200] [D loss: -0.006520] [G loss: -3.888183]\n",
      "true:  tensor([[3.9144],\n",
      "        [3.9652]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.8547],\n",
      "        [3.8361]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0102, grad_fn=<AddBackward0>)\n",
      "[Epoch 109/200] [D loss: -0.010247] [G loss: -3.911803]\n",
      "true:  tensor([[4.1032],\n",
      "        [4.1046]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.1040],\n",
      "        [4.1055]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0039, grad_fn=<AddBackward0>)\n",
      "[Epoch 110/200] [D loss: -0.003921] [G loss: -4.114645]\n",
      "true:  tensor([[3.9375],\n",
      "        [3.9432]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9571],\n",
      "        [3.9189]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0333, grad_fn=<AddBackward0>)\n",
      "[Epoch 111/200] [D loss: -0.033267] [G loss: -3.946226]\n",
      "true:  tensor([[3.2519],\n",
      "        [0.7891]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2519],\n",
      "        [0.7164]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0450, grad_fn=<AddBackward0>)\n",
      "[Epoch 112/200] [D loss: -0.044979] [G loss: -2.033696]\n",
      "true:  tensor([[3.9587],\n",
      "        [3.9611]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9683],\n",
      "        [3.9071]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0456, grad_fn=<AddBackward0>)\n",
      "[Epoch 113/200] [D loss: -0.045595] [G loss: -3.970964]\n",
      "true:  tensor([[4.1619],\n",
      "        [4.1805]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.1488],\n",
      "        [4.1502]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0013, grad_fn=<AddBackward0>)\n",
      "[Epoch 114/200] [D loss: -0.001295] [G loss: -4.164958]\n",
      "true:  tensor([[3.2818],\n",
      "        [0.8734]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2818],\n",
      "        [0.8804]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.1070, grad_fn=<AddBackward0>)\n",
      "[Epoch 115/200] [D loss: 0.106982] [G loss: -2.034446]\n",
      "true:  tensor([[4.0308],\n",
      "        [4.0159]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9847],\n",
      "        [3.9630]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0162, grad_fn=<AddBackward0>)\n",
      "[Epoch 116/200] [D loss: 0.016190] [G loss: -3.991797]\n",
      "true:  tensor([[3.2994],\n",
      "        [3.2994]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.2994],\n",
      "        [3.2994]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 117/200] [D loss: 0.000000] [G loss: -3.299423]\n",
      "true:  tensor([[3.3091],\n",
      "        [3.3091]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.3091],\n",
      "        [3.3091]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 118/200] [D loss: 0.000000] [G loss: -3.309102]\n",
      "true:  tensor([[4.0587],\n",
      "        [4.0732]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.0440],\n",
      "        [4.0558]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0176, grad_fn=<AddBackward0>)\n",
      "[Epoch 119/200] [D loss: -0.017607] [G loss: -4.026547]\n",
      "true:  tensor([[4.2846],\n",
      "        [4.2144]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.2782],\n",
      "        [4.2591]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0067, grad_fn=<AddBackward0>)\n",
      "[Epoch 120/200] [D loss: 0.006680] [G loss: -4.277704]\n",
      "true:  tensor([[4.2686],\n",
      "        [4.2653]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.2796],\n",
      "        [4.2397]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0040, grad_fn=<AddBackward0>)\n",
      "[Epoch 121/200] [D loss: 0.004029] [G loss: -4.247847]\n",
      "true:  tensor([[4.3299],\n",
      "        [4.2945]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.3285],\n",
      "        [4.3124]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0013, grad_fn=<AddBackward0>)\n",
      "[Epoch 122/200] [D loss: 0.001310] [G loss: -4.318233]\n",
      "true:  tensor([[4.2784],\n",
      "        [4.2897]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.3004],\n",
      "        [4.3015]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0344, grad_fn=<AddBackward0>)\n",
      "[Epoch 123/200] [D loss: 0.034402] [G loss: -4.291812]\n",
      "true:  tensor([[4.3587],\n",
      "        [4.3487]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.3517],\n",
      "        [4.3490]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0108, grad_fn=<AddBackward0>)\n",
      "[Epoch 124/200] [D loss: 0.010803] [G loss: -4.348048]\n",
      "true:  tensor([[4.3429],\n",
      "        [4.1265]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.3775],\n",
      "        [4.1235]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0268, grad_fn=<AddBackward0>)\n",
      "[Epoch 125/200] [D loss: -0.026760] [G loss: -4.232552]\n",
      "true:  tensor([[4.1224],\n",
      "        [4.1832]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.1751],\n",
      "        [4.1639]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0067, grad_fn=<AddBackward0>)\n",
      "[Epoch 126/200] [D loss: -0.006735] [G loss: -4.149695]\n",
      "true:  tensor([[4.3832],\n",
      "        [4.3907]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.3901],\n",
      "        [4.3870]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0009, grad_fn=<AddBackward0>)\n",
      "[Epoch 127/200] [D loss: 0.000860] [G loss: -4.384898]\n",
      "true:  tensor([[3.4299],\n",
      "        [3.4299]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.4299],\n",
      "        [3.4299]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 128/200] [D loss: 0.000000] [G loss: -3.429885]\n",
      "true:  tensor([[3.4397],\n",
      "        [3.4397]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.4397],\n",
      "        [3.4397]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 129/200] [D loss: 0.000000] [G loss: -3.439730]\n",
      "true:  tensor([[4.4146],\n",
      "        [4.3930]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.3626],\n",
      "        [4.4079]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0063, grad_fn=<AddBackward0>)\n",
      "[Epoch 130/200] [D loss: 0.006261] [G loss: -4.398355]\n",
      "true:  tensor([[4.4346],\n",
      "        [4.4039]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.4072],\n",
      "        [4.3682]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0168, grad_fn=<AddBackward0>)\n",
      "[Epoch 131/200] [D loss: 0.016770] [G loss: -4.416444]\n",
      "true:  tensor([[4.4577],\n",
      "        [4.4727]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.4593],\n",
      "        [4.4714]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0084, grad_fn=<AddBackward0>)\n",
      "[Epoch 132/200] [D loss: -0.008419] [G loss: -4.454865]\n",
      "true:  tensor([[4.4910],\n",
      "        [4.4938]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.4905],\n",
      "        [4.4849]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0139, grad_fn=<AddBackward0>)\n",
      "[Epoch 133/200] [D loss: -0.013910] [G loss: -4.494733]\n",
      "true:  tensor([[4.2685],\n",
      "        [4.2933]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.2889],\n",
      "        [4.2535]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0192, grad_fn=<AddBackward0>)\n",
      "[Epoch 134/200] [D loss: -0.019182] [G loss: -4.292774]\n",
      "true:  tensor([[4.4665],\n",
      "        [4.4673]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.4835],\n",
      "        [4.4816]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0091, grad_fn=<AddBackward0>)\n",
      "[Epoch 135/200] [D loss: -0.009086] [G loss: -4.487257]\n",
      "true:  tensor([[4.5188],\n",
      "        [4.5137]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.5086],\n",
      "        [4.4946]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "[Epoch 136/200] [D loss: 0.002940] [G loss: -4.509009]\n",
      "true:  tensor([[4.3121],\n",
      "        [4.3295]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.3520],\n",
      "        [4.3198]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0029, grad_fn=<AddBackward0>)\n",
      "[Epoch 137/200] [D loss: -0.002943] [G loss: -4.290826]\n",
      "true:  tensor([[4.3069],\n",
      "        [4.3121]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.3643],\n",
      "        [4.3407]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0073, grad_fn=<AddBackward0>)\n",
      "[Epoch 138/200] [D loss: -0.007287] [G loss: -4.337592]\n",
      "true:  tensor([[4.5512],\n",
      "        [4.5463]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.5401],\n",
      "        [4.5306]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0062, grad_fn=<AddBackward0>)\n",
      "[Epoch 139/200] [D loss: -0.006178] [G loss: -4.544030]\n",
      "true:  tensor([[3.5751],\n",
      "        [3.5751]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.5751],\n",
      "        [3.5751]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 140/200] [D loss: 0.000000] [G loss: -3.575079]\n",
      "true:  tensor([[4.5784],\n",
      "        [4.5710]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.5692],\n",
      "        [4.5713]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0115, grad_fn=<AddBackward0>)\n",
      "[Epoch 141/200] [D loss: 0.011471] [G loss: -4.569278]\n",
      "true:  tensor([[4.6041],\n",
      "        [4.6028]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.5896],\n",
      "        [4.5796]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0058, grad_fn=<AddBackward0>)\n",
      "[Epoch 142/200] [D loss: -0.005808] [G loss: -4.602316]\n",
      "true:  tensor([[4.4161],\n",
      "        [4.3757]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.3919],\n",
      "        [4.4041]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0259, grad_fn=<AddBackward0>)\n",
      "[Epoch 143/200] [D loss: -0.025932] [G loss: -4.404655]\n",
      "true:  tensor([[4.3897],\n",
      "        [1.4173]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.4285],\n",
      "        [1.5057]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.1528, grad_fn=<AddBackward0>)\n",
      "[Epoch 144/200] [D loss: 0.152797] [G loss: -2.961174]\n",
      "true:  tensor([[4.6267],\n",
      "        [4.6498]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.6199],\n",
      "        [4.6290]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0246, grad_fn=<AddBackward0>)\n",
      "[Epoch 145/200] [D loss: 0.024570] [G loss: -4.626040]\n",
      "true:  tensor([[4.6715],\n",
      "        [4.6681]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.6492],\n",
      "        [4.6639]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0100, grad_fn=<AddBackward0>)\n",
      "[Epoch 146/200] [D loss: 0.009993] [G loss: -4.656188]\n",
      "true:  tensor([[4.7166],\n",
      "        [4.6934]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.7045],\n",
      "        [4.7197]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0161, grad_fn=<AddBackward0>)\n",
      "[Epoch 147/200] [D loss: 0.016082] [G loss: -4.700917]\n",
      "true:  tensor([[4.6750],\n",
      "        [4.6828]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.6993],\n",
      "        [4.7021]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0024, grad_fn=<AddBackward0>)\n",
      "[Epoch 148/200] [D loss: 0.002378] [G loss: -4.683025]\n",
      "true:  tensor([[4.5179],\n",
      "        [4.5005]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.4504],\n",
      "        [4.4989]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0296, grad_fn=<AddBackward0>)\n",
      "[Epoch 149/200] [D loss: 0.029600] [G loss: -4.531912]\n",
      "true:  tensor([[4.7472],\n",
      "        [4.7207]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.7135],\n",
      "        [4.7297]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0173, grad_fn=<AddBackward0>)\n",
      "[Epoch 150/200] [D loss: 0.017311] [G loss: -4.710398]\n",
      "true:  tensor([[4.5352],\n",
      "        [4.4264]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.5085],\n",
      "        [4.5410]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0162, grad_fn=<AddBackward0>)\n",
      "[Epoch 151/200] [D loss: -0.016222] [G loss: -4.526715]\n",
      "true:  tensor([[4.5615],\n",
      "        [4.5494]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.5747],\n",
      "        [4.5319]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0384, grad_fn=<AddBackward0>)\n",
      "[Epoch 152/200] [D loss: -0.038433] [G loss: -4.574218]\n",
      "true:  tensor([[4.5676],\n",
      "        [4.5657]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.5747],\n",
      "        [4.5862]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0036, grad_fn=<AddBackward0>)\n",
      "[Epoch 153/200] [D loss: -0.003625] [G loss: -4.557916]\n",
      "true:  tensor([[4.8066],\n",
      "        [4.7777]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.7790],\n",
      "        [4.7731]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0002, grad_fn=<AddBackward0>)\n",
      "[Epoch 154/200] [D loss: 0.000157] [G loss: -4.789801]\n",
      "true:  tensor([[4.7840],\n",
      "        [4.8063]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.7951],\n",
      "        [4.8246]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0243, grad_fn=<AddBackward0>)\n",
      "[Epoch 155/200] [D loss: 0.024315] [G loss: -4.796333]\n",
      "true:  tensor([[4.8264],\n",
      "        [4.8050]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.8036],\n",
      "        [4.8159]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0125, grad_fn=<AddBackward0>)\n",
      "[Epoch 156/200] [D loss: 0.012534] [G loss: -4.811965]\n",
      "true:  tensor([[4.8849],\n",
      "        [4.8852]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.8800],\n",
      "        [4.8831]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0157, grad_fn=<AddBackward0>)\n",
      "[Epoch 157/200] [D loss: 0.015731] [G loss: -4.877985]\n",
      "true:  tensor([[4.8889],\n",
      "        [3.8954]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.8986],\n",
      "        [4.0757]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.3201, grad_fn=<AddBackward0>)\n",
      "[Epoch 158/200] [D loss: 0.320090] [G loss: -4.447714]\n",
      "true:  tensor([[4.6762],\n",
      "        [4.6002]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.5923],\n",
      "        [4.6929]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0118, grad_fn=<AddBackward0>)\n",
      "[Epoch 159/200] [D loss: -0.011826] [G loss: -4.657825]\n",
      "true:  tensor([[4.8738],\n",
      "        [4.8819]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.8905],\n",
      "        [4.9034]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0099, grad_fn=<AddBackward0>)\n",
      "[Epoch 160/200] [D loss: -0.009868] [G loss: -4.887035]\n",
      "true:  tensor([[4.7192],\n",
      "        [4.6918]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.7359],\n",
      "        [4.6793]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0199, grad_fn=<AddBackward0>)\n",
      "[Epoch 161/200] [D loss: -0.019933] [G loss: -4.695757]\n",
      "true:  tensor([[4.6933],\n",
      "        [4.6927]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.7333],\n",
      "        [4.6943]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0045, grad_fn=<AddBackward0>)\n",
      "[Epoch 162/200] [D loss: -0.004512] [G loss: -4.695529]\n",
      "true:  tensor([[4.9207],\n",
      "        [4.9522]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.9350],\n",
      "        [4.9474]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0224, grad_fn=<AddBackward0>)\n",
      "[Epoch 163/200] [D loss: 0.022390] [G loss: -4.938183]\n",
      "true:  tensor([[4.7545],\n",
      "        [4.7400]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.7187],\n",
      "        [4.7711]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0236, grad_fn=<AddBackward0>)\n",
      "[Epoch 164/200] [D loss: 0.023598] [G loss: -4.713404]\n",
      "true:  tensor([[3.8880],\n",
      "        [3.8880]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.8880],\n",
      "        [3.8880]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 165/200] [D loss: 0.000000] [G loss: -3.888032]\n",
      "true:  tensor([[4.9827],\n",
      "        [2.8501]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.9627],\n",
      "        [2.4361]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.3998, grad_fn=<AddBackward0>)\n",
      "[Epoch 166/200] [D loss: -0.399817] [G loss: -3.974889]\n",
      "true:  tensor([[4.7856],\n",
      "        [4.7281]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.7410],\n",
      "        [4.8201]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0210, grad_fn=<AddBackward0>)\n",
      "[Epoch 167/200] [D loss: -0.021021] [G loss: -4.801982]\n",
      "true:  tensor([[3.9220],\n",
      "        [3.9220]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9220],\n",
      "        [3.9220]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 168/200] [D loss: 0.000000] [G loss: -3.922027]\n",
      "true:  tensor([[4.8212],\n",
      "        [4.7655]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.8160],\n",
      "        [4.8005]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0546, grad_fn=<AddBackward0>)\n",
      "[Epoch 169/200] [D loss: 0.054626] [G loss: -4.804811]\n",
      "true:  tensor([[5.0801],\n",
      "        [5.0575]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.0790],\n",
      "        [5.0630]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0053, grad_fn=<AddBackward0>)\n",
      "[Epoch 170/200] [D loss: 0.005299] [G loss: -5.075792]\n",
      "true:  tensor([[3.9590],\n",
      "        [3.9590]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9590],\n",
      "        [3.9590]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 171/200] [D loss: 0.000000] [G loss: -3.959008]\n",
      "true:  tensor([[3.9696],\n",
      "        [3.9696]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9696],\n",
      "        [3.9696]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 172/200] [D loss: 0.000000] [G loss: -3.969593]\n",
      "true:  tensor([[5.0985],\n",
      "        [5.0920]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.0882],\n",
      "        [5.0750]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0026, grad_fn=<AddBackward0>)\n",
      "[Epoch 173/200] [D loss: 0.002621] [G loss: -5.084414]\n",
      "true:  tensor([[3.9938],\n",
      "        [3.9938]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[3.9938],\n",
      "        [3.9938]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 174/200] [D loss: 0.000000] [G loss: -3.993803]\n",
      "true:  tensor([[5.1215],\n",
      "        [5.1155]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.1422],\n",
      "        [5.1258]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0242, grad_fn=<AddBackward0>)\n",
      "[Epoch 175/200] [D loss: 0.024217] [G loss: -5.129282]\n",
      "true:  tensor([[4.9133],\n",
      "        [4.9127]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.9164],\n",
      "        [4.9089]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0102, grad_fn=<AddBackward0>)\n",
      "[Epoch 176/200] [D loss: -0.010218] [G loss: -4.977263]\n",
      "true:  tensor([[5.1534],\n",
      "        [5.1361]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.1717],\n",
      "        [5.1515]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0052, grad_fn=<AddBackward0>)\n",
      "[Epoch 177/200] [D loss: 0.005220] [G loss: -5.120881]\n",
      "true:  tensor([[4.9521],\n",
      "        [4.9614]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.9491],\n",
      "        [4.9750]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0154, grad_fn=<AddBackward0>)\n",
      "[Epoch 178/200] [D loss: 0.015410] [G loss: -4.955276]\n",
      "true:  tensor([[4.9635],\n",
      "        [2.2592]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.9863],\n",
      "        [1.7046]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0600, grad_fn=<AddBackward0>)\n",
      "[Epoch 179/200] [D loss: 0.059968] [G loss: -3.494017]\n",
      "true:  tensor([[4.0674],\n",
      "        [4.0674]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.0674],\n",
      "        [4.0674]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 180/200] [D loss: 0.000000] [G loss: -4.067426]\n",
      "true:  tensor([[5.2276],\n",
      "        [5.2158]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.2026],\n",
      "        [5.2263]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0122, grad_fn=<AddBackward0>)\n",
      "[Epoch 181/200] [D loss: -0.012207] [G loss: -5.219813]\n",
      "true:  tensor([[5.2417],\n",
      "        [5.2201]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.2407],\n",
      "        [5.2234]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0084, grad_fn=<AddBackward0>)\n",
      "[Epoch 182/200] [D loss: 0.008375] [G loss: -5.225111]\n",
      "true:  tensor([[5.2881],\n",
      "        [5.2822]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.3050],\n",
      "        [5.3013]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0045, grad_fn=<AddBackward0>)\n",
      "[Epoch 183/200] [D loss: -0.004527] [G loss: -5.288975]\n",
      "true:  tensor([[5.2803],\n",
      "        [5.2679]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.2696],\n",
      "        [5.2782]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0002, grad_fn=<AddBackward0>)\n",
      "[Epoch 184/200] [D loss: -0.000236] [G loss: -5.257806]\n",
      "true:  tensor([[5.2721],\n",
      "        [5.2905]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.2774],\n",
      "        [5.2783]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0084, grad_fn=<AddBackward0>)\n",
      "[Epoch 185/200] [D loss: 0.008360] [G loss: -5.291417]\n",
      "true:  tensor([[4.1476],\n",
      "        [4.1476]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.1476],\n",
      "        [4.1476]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 186/200] [D loss: 0.000000] [G loss: -4.147641]\n",
      "true:  tensor([[4.1585],\n",
      "        [4.1585]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.1585],\n",
      "        [4.1585]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 187/200] [D loss: 0.000000] [G loss: -4.158537]\n",
      "true:  tensor([[5.3869],\n",
      "        [4.7381]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.3783],\n",
      "        [4.9239]], grad_fn=<AddmmBackward0>)\n",
      "tensor(1.0052, grad_fn=<AddBackward0>)\n",
      "[Epoch 188/200] [D loss: 1.005232] [G loss: -4.780837]\n",
      "true:  tensor([[5.0990],\n",
      "        [5.0808]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.1487],\n",
      "        [5.1775]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0066, grad_fn=<AddBackward0>)\n",
      "[Epoch 189/200] [D loss: -0.006576] [G loss: -5.178624]\n",
      "true:  tensor([[5.3467],\n",
      "        [5.3766]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.3729],\n",
      "        [5.3635]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0306, grad_fn=<AddBackward0>)\n",
      "[Epoch 190/200] [D loss: 0.030592] [G loss: -5.368952]\n",
      "true:  tensor([[5.4035],\n",
      "        [5.3882]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.3873],\n",
      "        [5.3971]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0193, grad_fn=<AddBackward0>)\n",
      "[Epoch 191/200] [D loss: 0.019349] [G loss: -5.397872]\n",
      "true:  tensor([[5.4227],\n",
      "        [5.4069]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.4097],\n",
      "        [5.4035]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0052, grad_fn=<AddBackward0>)\n",
      "[Epoch 192/200] [D loss: 0.005173] [G loss: -5.408870]\n",
      "true:  tensor([[5.4296],\n",
      "        [5.4222]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.4387],\n",
      "        [5.4225]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0225, grad_fn=<AddBackward0>)\n",
      "[Epoch 193/200] [D loss: -0.022536] [G loss: -5.427611]\n",
      "true:  tensor([[5.4570],\n",
      "        [3.2637]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.4243],\n",
      "        [3.2679]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.5173, grad_fn=<AddBackward0>)\n",
      "[Epoch 194/200] [D loss: 0.517282] [G loss: -4.507097]\n",
      "true:  tensor([[4.2637],\n",
      "        [4.2637]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[4.2637],\n",
      "        [4.2637]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0., grad_fn=<AddBackward0>)\n",
      "[Epoch 195/200] [D loss: 0.000000] [G loss: -4.263731]\n",
      "true:  tensor([[5.5220],\n",
      "        [5.5193]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.4936],\n",
      "        [5.5094]], grad_fn=<AddmmBackward0>)\n",
      "tensor(-0.0122, grad_fn=<AddBackward0>)\n",
      "[Epoch 196/200] [D loss: -0.012230] [G loss: -5.498578]\n",
      "true:  tensor([[5.5026],\n",
      "        [5.4788]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.4970],\n",
      "        [5.5028]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "[Epoch 197/200] [D loss: 0.000256] [G loss: -5.490629]\n",
      "true:  tensor([[5.3082],\n",
      "        [5.3204]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.2829],\n",
      "        [5.2353]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0164, grad_fn=<AddBackward0>)\n",
      "[Epoch 198/200] [D loss: 0.016425] [G loss: -5.272604]\n",
      "true:  tensor([[5.5695],\n",
      "        [5.5746]], grad_fn=<AddmmBackward0>)\n",
      "false:  tensor([[5.5838],\n",
      "        [5.5873]], grad_fn=<AddmmBackward0>)\n",
      "tensor(0.0017, grad_fn=<AddBackward0>)\n",
      "[Epoch 199/200] [D loss: 0.001694] [G loss: -5.570234]\n"
     ]
    }
   ],
   "source": [
    "run_epoch(models, optimizers, True, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45dc61766396859fdffae760accc4b74216ea8fbbed6d1a9d5e8fb1914e35062"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
