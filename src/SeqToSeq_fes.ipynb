{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '../'\n",
    "\n",
    "import os, sys\n",
    "from skimage import io, transform\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from functools import cmp_to_key\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import mask_rcnn\n",
    "from torch.optim import SGD\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic settings\n",
    "torch.manual_seed(470)\n",
    "torch.cuda.manual_seed(470)\n",
    "\n",
    "#!pip install easydict\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "args = edict()\n",
    "args.batch_size = 4\n",
    "args.nlayers = 2\n",
    "\n",
    "args.embedding_size = 4\n",
    "args.ninp = 4 + args.embedding_size\n",
    "args.nhid = 64 #512\n",
    "\n",
    "args.dropout = 0.2\n",
    "args.gpu = True\n",
    "\n",
    "args.tensorboard = False\n",
    "args.train_portion = 0.7\n",
    "args.slide_deck_N = 5\n",
    "args.slide_deck_embedding_size = 512\n",
    "args.padding_idx = 0\n",
    "args.max_seq_length = 8\n",
    "\n",
    "# Decoder\n",
    "args.latent_vector_dim = 28\n",
    "\n",
    "# GAN\n",
    "args.n_epochs = 200\n",
    "args.lr = 0.00005\n",
    "args.n_cpu = 4\n",
    "args.latent_dim = 100\n",
    "args.channels = 1\n",
    "args.clip_value = 0.01\n",
    "args.sample_interval = 400\n",
    "args.n_critic = 100\n",
    "args.b1 = 0.5\n",
    "args.b2 = 0.999\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() and args.gpu else 'cpu'\n",
    "# Create directory name.\n",
    "result_dir = Path(root) / 'results'\n",
    "result_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tensorboard:\n",
    "    %load_ext tensorboard\n",
    "    %tensorboard --logdir \"{str(result_dir)}\" --samples_per_plugin images=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "BB_TYPES = [\n",
    "    '<pad>',\n",
    "    'title',\n",
    "    'header',\n",
    "    'text box',\n",
    "    'footer',\n",
    "    'picture',\n",
    "    'instructor',\n",
    "    'diagram',\n",
    "    'table',\n",
    "    'figure',\n",
    "    'handwriting',\n",
    "    'chart',\n",
    "    'schematic diagram',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bbs(shape, bbs):\n",
    "    if (torch.is_tensor(bbs)):\n",
    "        bbs = np.array(bbs.tolist())\n",
    "    if (torch.is_tensor(shape)):\n",
    "        [h, w] = np.array(shape.tolist())\n",
    "        shape = (h, w)\n",
    "    \n",
    "    h, w = shape\n",
    "    fig, ax = plt.subplots(1)\n",
    "    background=patches.Rectangle((0, 0), w, h, linewidth=2, edgecolor='b', facecolor='black')\n",
    "    ax.add_patch(background)\n",
    "    for bb in bbs:\n",
    "        rect = patches.Rectangle((bb[0], bb[1]), bb[2], bb[3], linewidth=1, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "    ax.autoscale(True, 'both')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def get_BB_types(bbs):\n",
    "    return bbs[:, 4]\n",
    "\n",
    "class BBSlideDeckDataset(Dataset):\n",
    "    \"\"\" Slide Deck Dataset but with Bounding Boxes\"\"\"\n",
    "    def __init__(self, slide_deck_data, transform=None):\n",
    "        self.transform = transform\n",
    "\n",
    "        self.slide_deck_data = slide_deck_data\n",
    "        self.slide_deck_ids = list(self.slide_deck_data.keys())\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.slide_deck_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        slide_deck_id = self.slide_deck_ids[idx]\n",
    "        (h, w) = self.slide_deck_data[slide_deck_id][\"shape\"]\n",
    "        lengths_slide_deck = []\n",
    "        \n",
    "        slides = []\n",
    "        max_len_bbs = args.max_seq_length\n",
    "        for slide in self.slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            lengths_slide_deck.append(min(max_len_bbs, len(slide)))\n",
    "            np_slide = np.zeros((max_len_bbs, 5), dtype=np.double)\n",
    "            for i, bb in enumerate(slide):\n",
    "                if (i >= max_len_bbs):\n",
    "                    break\n",
    "                np_slide[i] = bb\n",
    "            slides.append(np_slide)\n",
    "        ref_slide = slides[0]\n",
    "        slide_deck = slides[1:]\n",
    "        length_ref_types = lengths_slide_deck.pop(0)\n",
    "        sample = {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": get_BB_types(ref_slide),\n",
    "            \"slide_deck\": np.asarray(slide_deck),\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleBB(object):\n",
    "    \"\"\"Rescale the bounding boxes in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def _resize_single_slide(self, slide, original_shape, new_shape):\n",
    "        h, w = original_shape\n",
    "        new_h, new_w = new_shape\n",
    "        slide = slide * np.array([new_w / w, new_h / h, new_w / w, new_h / h, 1]).T\n",
    "        return slide\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        ref_slide = self._resize_single_slide(ref_slide, (h, w), (new_h, new_w))\n",
    "        for i, slide in enumerate(slide_deck):\n",
    "            slide_deck[i] = self._resize_single_slide(slide, (h, w), (new_h, new_w))\n",
    "\n",
    "        return {\n",
    "            \"shape\": (new_h, new_w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class LeaveN(object):\n",
    "    def __init__ (self, N):\n",
    "        self.N = N\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        if slide_deck.shape[0] > self.N:\n",
    "            slide_deck = np.delete(slide_deck, range(self.N, slide_deck.shape[0]), 0)\n",
    "            lengths_slide_deck = lengths_slide_deck[:self.N]\n",
    "\n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ShuffleRefSlide(object):\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample['ref_slide']\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        lengths_slide_deck.append(length_ref_types)\n",
    "        slide_deck = np.vstack((slide_deck, ref_slide[None, :]))\n",
    "\n",
    "        idxs = np.array([*range(0, len(lengths_slide_deck))], dtype=np.int32)\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        slide_deck = slide_deck[idxs]\n",
    "\n",
    "        lengths_slide_deck = np.array(lengths_slide_deck, dtype=np.int32)\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck.tolist()\n",
    "        \n",
    "        slide_deck = slide_deck.tolist()\n",
    "        ref_slide = np.asarray(slide_deck.pop())\n",
    "        length_ref_types = lengths_slide_deck.pop()\n",
    "        ref_types = get_BB_types(ref_slide)\n",
    "\n",
    "        slide_deck = np.asarray(slide_deck)\n",
    "        \n",
    "        return {\n",
    "            \"shape\": (h, w),\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types,\n",
    "        }\n",
    "\n",
    "class ToTensorBB(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        h, w = sample[\"shape\"]\n",
    "        ref_slide = sample[\"ref_slide\"]\n",
    "        ref_types = sample[\"ref_types\"]\n",
    "        slide_deck = sample[\"slide_deck\"]\n",
    "        lengths_slide_deck = sample[\"lengths_slide_deck\"]\n",
    "        length_ref_types = sample[\"length_ref_types\"]\n",
    "\n",
    "        idxs = [*range(0, len(lengths_slide_deck))]\n",
    "\n",
    "        def by_length(p1, p2):\n",
    "            return lengths_slide_deck[p2] - lengths_slide_deck[p1]\n",
    "        idxs = sorted(idxs, key=cmp_to_key(by_length))\n",
    "\n",
    "        shape = torch.tensor([h, w], dtype=torch.float64)\n",
    "        ref_slide = torch.from_numpy(ref_slide).float()\n",
    "        ref_types = torch.from_numpy(ref_types).float()\n",
    "        \n",
    "        slide_deck = torch.from_numpy(slide_deck).float()\n",
    "        lengths_slide_deck = torch.tensor(lengths_slide_deck, dtype=torch.int32)\n",
    "        \n",
    "        slide_deck = slide_deck[idxs]\n",
    "        lengths_slide_deck = lengths_slide_deck[idxs]\n",
    "\n",
    "        length_ref_types = torch.tensor(length_ref_types, dtype=torch.int32)\n",
    "\n",
    "        return {\n",
    "            \"shape\": shape,\n",
    "            \"ref_slide\": ref_slide,\n",
    "            \"ref_types\": ref_types,\n",
    "            \"slide_deck\": slide_deck,\n",
    "            \"lengths_slide_deck\": lengths_slide_deck,\n",
    "            \"length_ref_types\": length_ref_types\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slide_deck_dataset(all_dataset):\n",
    "    slide_deck_data = {}\n",
    "    for entrance in all_dataset.iloc:\n",
    "        slide_deck_id = entrance['Slide Deck Id']\n",
    "        \n",
    "        slide_id = entrance[\"Slide Id\"]\n",
    "        if (slide_deck_id not in slide_deck_data):\n",
    "            slide_deck_data[slide_deck_id] = {\n",
    "                'slides': {},\n",
    "                'shape': (entrance['Image Height'], entrance['Image Width'])\n",
    "            }\n",
    "        \n",
    "        if slide_id not in slide_deck_data[slide_deck_id][\"slides\"]:\n",
    "            slide_deck_data[slide_deck_id][\"slides\"][slide_id] = []\n",
    "        bb_type = BB_TYPES.index(entrance['Type'])\n",
    "        if (bb_type < 0 or bb_type >= len(BB_TYPES)):\n",
    "            bb_type = len(BB_TYPES)\n",
    "\n",
    "        bb = np.array([\n",
    "            entrance['X'],\n",
    "            entrance['Y'],\n",
    "            entrance['BB Width'],\n",
    "            entrance['BB Height'],\n",
    "            bb_type\n",
    "        ]).T\n",
    "        slide_deck_data[slide_deck_id]['slides'][slide_id].append(bb)\n",
    "    for key in slide_deck_data.keys():\n",
    "        \n",
    "        # if key == 100:\n",
    "        #     for (id, value) in slide_deck_data[key][\"slides\"].items():\n",
    "        #         print(56, id)\n",
    "        #         draw_bbs(slide_deck_data[key][\"shape\"], value)\n",
    "\n",
    "        values = list(slide_deck_data[key][\"slides\"].values())\n",
    "        slide_deck_data[key][\"slides\"] = [np.asarray(value) for value in values]\n",
    "    return slide_deck_data\n",
    "\n",
    "def slice_dict(dictionary, l, r):\n",
    "    keys = list(dictionary.keys())\n",
    "    keys = keys[l:r]\n",
    "    ret_dictionary = {}\n",
    "    for key in keys:\n",
    "        ret_dictionary[key] = dictionary[key]\n",
    "    return ret_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = os.path.join(os.path.dirname(os.getcwd()), \"data\", \"slide_deck_dataset.csv\")\n",
    "\n",
    "dataset = pd.read_csv(csv_file)\n",
    "slide_deck_data = process_slide_deck_dataset(dataset)\n",
    "\n",
    "division = int(args.train_portion * len(slide_deck_data))\n",
    "\n",
    "train_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, 0, division),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(args.slide_deck_N),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n",
    "\n",
    "test_slide_deck_dataset = BBSlideDeckDataset(\n",
    "    slide_deck_data=slice_dict(slide_deck_data, division, len(slide_deck_data)),\n",
    "    transform=transforms.Compose([\n",
    "        RescaleBB((1, 1)),\n",
    "        ShuffleRefSlide(),\n",
    "        LeaveN(args.slide_deck_N),\n",
    "        ToTensorBB()\n",
    "    ])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5., 3., 1., 0., 0., 0.])\n",
      "tensor([[0.5811, 0.3125, 0.3308, 0.4249, 3.0000],\n",
      "        [0.2325, 0.1195, 0.5372, 0.1099, 1.0000],\n",
      "        [0.0650, 0.3130, 0.3255, 0.3700, 3.0000],\n",
      "        [0.5932, 0.7883, 0.3319, 0.1588, 9.0000],\n",
      "        [0.0631, 0.7888, 0.3346, 0.1578, 3.0000],\n",
      "        [0.0631, 0.7888, 0.3346, 0.1578, 9.0000],\n",
      "        [0.5860, 0.3448, 0.3202, 0.4234, 3.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0824, 0.6295, 0.7762, 0.1028, 3.0000],\n",
      "        [0.0711, 0.2939, 0.8537, 0.2132, 3.0000],\n",
      "        [0.3043, 0.1210, 0.3898, 0.0867, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[ 0.4752,  0.3770,  0.5138,  0.5091, 11.0000],\n",
      "        [ 0.0567,  0.3241,  0.3490,  0.3034,  3.0000],\n",
      "        [ 0.2306,  0.1235,  0.5361,  0.0993,  1.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[0.1856, 0.0585, 0.6272, 0.2268, 1.0000],\n",
      "        [0.0798, 0.3800, 0.3898, 0.3926, 5.0000],\n",
      "        [0.5248, 0.3836, 0.3921, 0.3891, 5.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([[0.0578, 0.3256, 0.8306, 0.1749, 3.0000],\n",
      "        [0.2938, 0.1230, 0.4095, 0.0892, 1.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n",
      "tensor([7, 3, 3, 3, 2], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "single = train_slide_deck_dataset[0]\n",
    "draw_bbs(single[\"shape\"], single[\"ref_slide\"])\n",
    "\n",
    "print(single[\"ref_types\"])\n",
    "for i in range(5):\n",
    "    print(single[\"slide_deck\"][i])\n",
    "print(single[\"lengths_slide_deck\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SortByRefSlide(batch):\n",
    "    idx = [*range(batch[\"ref_slide\"].shape[0])]\n",
    "\n",
    "    def by_length(p1, p2):\n",
    "        return batch[\"length_ref_types\"][p2] - batch[\"length_ref_types\"][p1]\n",
    "    idx = sorted(idx, key=cmp_to_key(by_length))\n",
    "\n",
    "    idx = torch.tensor(idx).to(device).long()\n",
    "    for prop in batch.keys():\n",
    "        batch[prop] = batch[prop][idx]\n",
    "    \n",
    "    return batch\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_slide_deck_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_slide_deck_dataset, batch_size=args.batch_size, shuffle=True, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlideEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlideEncoder, self).__init__()\n",
    "        ninp = args.ninp\n",
    "        nhid = args.nhid\n",
    "        nlayers = args.nlayers\n",
    "        dropout = args.dropout\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, args.padding_idx)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(ninp, nhid, nlayers, bias=True).float()\n",
    "\n",
    "    def forward(self, x, states, lengths=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: tensor(B, L, 5)\n",
    "            states: List[Tuple(h_0, c_0), ..., Tuple(h_{B-1}, c_{B-1})]\n",
    "            lengths: tensor(B)\n",
    "        \"\"\"\n",
    "        idxs = [*range(0, len(lengths))]\n",
    "        def by_lengths(p1, p2):\n",
    "            return lengths[p2] - lengths[p1]\n",
    "\n",
    "        idxs = sorted(idxs, key=cmp_to_key(by_lengths))\n",
    "\n",
    "        x = x[:, idxs]\n",
    "        lengths = lengths[idxs]\n",
    "        \n",
    "        input = x[:, :, :-1]\n",
    "        types = x[:, :, -1:].long()\n",
    "        types = torch.squeeze(self.embed(types))\n",
    "        input = torch.cat((input, types), dim=-1)\n",
    "\n",
    "        output = self.dropout(input)\n",
    "        \n",
    "\n",
    "        output = torch.nn.utils.rnn.pack_padded_sequence(output, lengths)\n",
    "\n",
    "        h_0 = torch.stack([h for (h, _) in states], dim=0)\n",
    "        c_0 = torch.stack([c for (_, c) in states], dim=0)\n",
    "\n",
    "        (output, context_vector) = self.lstm(output, (h_0, c_0))\n",
    "        output, lengths = torch.nn.utils.rnn.pad_packed_sequence(output, total_length = args.max_seq_length)\n",
    "        return (output, context_vector)\n",
    "\n",
    "class SlideDeckEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SlideDeckEncoder, self).__init__()\n",
    "        self.slide_encoder = SlideEncoder()\n",
    "\n",
    "        input_size = args.nhid * args.slide_deck_N\n",
    "        output_size = args.slide_deck_embedding_size\n",
    "\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        return\n",
    "\n",
    "    def _get_init_states(self, x):\n",
    "        init_states = [\n",
    "            (torch.zeros((x.size(1), args.nhid)).to(x.device),\n",
    "            torch.zeros((x.size(1), args.nhid)).to(x.device))\n",
    "            for _ in range(args.nlayers)\n",
    "        ]\n",
    "        return init_states\n",
    "    \n",
    "    def forward(self, xs, lengths):\n",
    "        states = None\n",
    "        embedding = []\n",
    "        for i, x in enumerate(xs):\n",
    "            if states is None:\n",
    "                states = self._get_init_states(x)\n",
    "            length = lengths[i]\n",
    "            output, states = self.slide_encoder(x, states, length)\n",
    "            output = output[length.long() - 1,:,:]\n",
    "            idxs = torch.arange(args.batch_size)\n",
    "            output = output[idxs, idxs, :]\n",
    "            embedding.append(output.squeeze())\n",
    "        \n",
    "        output = torch.cat(embedding, dim=-1)\n",
    "        output = self.relu(self.linear(output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/w3/zf9g_hvj5k77gd35xml1b9m40000gn/T/ipykernel_28292/838535811.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lengths_slide_deck\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mslide_deck_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = SlideDeckEncoder().to(device)\n",
    "\n",
    "for epoch in range(args.n_epochs):\n",
    "    for batch in train_loader:\n",
    "        xs = torch.transpose(batch[\"slide_deck\"], 0, 1)\n",
    "        xs = torch.transpose(xs, 1, 2)\n",
    "        lengths = torch.transpose(batch[\"lengths_slide_deck\"], 0, 1)\n",
    "        slide_deck_embedding = model(xs, lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y, n_dims=None):\n",
    "    \"\"\" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. \"\"\"\n",
    "    y_tensor = y.data if isinstance(y, torch.autograd.Variable) else y\n",
    "    y_tensor = y_tensor.type(torch.LongTensor).view(-1, 1)\n",
    "    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n",
    "    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims).scatter_(1, y_tensor, 1)\n",
    "    y_one_hot = y_one_hot.view(*y.shape, -1)\n",
    "    return torch.autograd.Variable(y_one_hot) if isinstance(y, torch.autograd.Variable) else y_one_hot\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, embed_weights=None, ganlike=True):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ganlike = ganlike\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.lstm = nn.LSTM(input_size=args.latent_vector_dim + args.embedding_size, hidden_size=args.nhid, num_layers=2, \n",
    "            batch_first=True, dropout=args.dropout, bias=True)\n",
    "        self.linear1 = nn.Linear(args.slide_deck_embedding_size, args.nhid)\n",
    "        self.linear2 = nn.Linear(args.nhid, 4)\n",
    "        if embed_weights is not None:\n",
    "            self.embed.weight.data = embed_weights\n",
    "        \n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            # if normalize:\n",
    "            #     layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.gen_model = nn.Sequential(\n",
    "            *block(args.nhid, 32, normalize=False),\n",
    "            *block(32, 32),\n",
    "            nn.Linear(32, 4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x, z, slide_deck_embedding, lengths=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "            z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            lengths (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            bb sequence: (tensor), (Batch_size, Sequence_size, 5)\n",
    "        \"\"\"\n",
    "        # print(x.shape, z.shape, slide_deck_embedding.shape, lengths)\n",
    "        x = x.int()\n",
    "        (Batch_size, Sequence_size) = x.shape\n",
    "        temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        # print(temp_input_1)\n",
    "        # print(\"1\",temp_input_1.shape)\n",
    "        temp_input_2 = z.unsqueeze(1).repeat((1, Sequence_size, 1))\n",
    "        # print(\"2\",temp_input_2.shape)\n",
    "        input_1 = torch.cat((temp_input_2, temp_input_1), dim=-1)\n",
    "        # print(input_1.shape)\n",
    "        input_1 = torch.nn.utils.rnn.pack_padded_sequence(input_1, lengths, batch_first=True)\n",
    "        # print(input_1.data.shape)\n",
    "        # print(\"3\",input_1.shape)\n",
    "        hidden_0 = self.dropout(self.linear1(slide_deck_embedding)).unsqueeze(0).repeat((2, 1, 1))\n",
    "        # print(\"4\",hidden_0.shape)\n",
    "        c_0 = torch.zeros(size=(2, Batch_size, args.nhid))\n",
    "        # print(\"5\",c_0.shape)\n",
    "        output, (h_n, c_n) = self.lstm(input_1, (hidden_0, c_0))\n",
    "        # print(output.data.shape)\n",
    "        output, length = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=args.max_seq_length)\n",
    "\n",
    "        # output = output.transpose(0, 1)\n",
    "        if self.ganlike:\n",
    "            # output = output.transpose(1, 2)\n",
    "            # print(output.shape)\n",
    "            output = self.gen_model(output)\n",
    "        else:\n",
    "            output = self.linear2(output)\n",
    "\n",
    "        return output, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 4])"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(args.batch_size)\n",
    "mydec = Generator()\n",
    "z = torch.randn((args.batch_size, args.latent_vector_dim))\n",
    "s = SortByRefSlide(list(train_loader)[0])\n",
    "# print(len(s))\n",
    "# for sl in s:\n",
    "#     print(sl)\n",
    "#     print(s[sl].shape)\n",
    "\n",
    "idx = [i for i in range(s['ref_types'].size(0)-1, -1, -1)]\n",
    "idx = torch.LongTensor(idx)\n",
    "#print(idx)\n",
    "t = s['ref_types']\n",
    "l = s['length_ref_types']\n",
    "#print(t,l)\n",
    "v = mydec(x=t, z=z, slide_deck_embedding=torch.randn((args.batch_size, args.slide_deck_embedding_size)), lengths=s['length_ref_types'])\n",
    "# draw_bbs(,)\n",
    "v[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embed_weights=None):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.embed = nn.Embedding(len(BB_TYPES), args.embedding_size, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.lstm = nn.LSTM(input_size = args.ninp, hidden_size=args.nhid, num_layers=args.nlayers, \n",
    "            batch_first=True, bias=True)\n",
    "        self.linear1 = nn.Linear(args.slide_deck_embedding_size, args.nhid)\n",
    "        self.d_model = nn.Sequential(\n",
    "            nn.Linear(args.nhid, args.nhid//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(args.nhid//2, args.nhid//2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(args.nhid//2, 1)\n",
    "        )\n",
    "        if embed_weights is not None:\n",
    "            self.embed.weight.data = embed_weights\n",
    "\n",
    "\n",
    "    def forward(self, x, bb, slide_deck_embedding, lengths=None):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "            x (tensor): type labels, (Batch_size, Sequence_size)\n",
    "            bb (tensor): (Batch_size, Sequence_size, 4)\n",
    "            slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            length (tensor): (Batch_size,)\n",
    "\n",
    "        Returns:\n",
    "            \n",
    "        \"\"\"\n",
    "        x = x.int()\n",
    "        \n",
    "        (Batch_size, _) = x.shape\n",
    "        temp_input_1 = self.dropout(self.embed(x))   # Batch_size, Sequence_size, input_size\n",
    "        input_1 = torch.cat((bb, temp_input_1), dim=-1)\n",
    "        input_1 = torch.nn.utils.rnn.pack_padded_sequence(input_1, lengths, batch_first=True)\n",
    "\n",
    "        h_0 = self.dropout(self.linear1(slide_deck_embedding)).unsqueeze(0).repeat((2, 1, 1))\n",
    "        c_0 = torch.zeros(size=(2,Batch_size, args.nhid))\n",
    "        output, (h_n, c_n) = self.lstm(input_1, (h_0, c_0))\n",
    "        output, length = torch.nn.utils.rnn.pad_packed_sequence(output, batch_first=True, total_length=args.max_seq_length)\n",
    "        \n",
    "        output = output[:,length-1,:].squeeze()\n",
    "        idxs = torch.arange(args.batch_size)\n",
    "        output = output[idxs, idxs, :]\n",
    "        #print(output[:, :10])\n",
    "        output = self.d_model(output.squeeze())\n",
    "        #print(output[:, :10])\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0876],\n",
       "        [-0.0872],\n",
       "        [-0.0913],\n",
       "        [-0.0840]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = Discriminator()\n",
    "\n",
    "z = torch.randn((args.batch_size, args.latent_vector_dim))\n",
    "s = SortByRefSlide(list(train_loader)[0])\n",
    "# print(len(s))\n",
    "# for sl in s:\n",
    "#     print(sl)\n",
    "#     print(s[sl].shape)\n",
    "# print(v[0])\n",
    "sss = c(x=s['ref_types'], bb = v[0], slide_deck_embedding=torch.randn((args.batch_size, args.slide_deck_embedding_size)), lengths=s['length_ref_types'])\n",
    "sss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch=\"last\"):\n",
    "    torch.save(model.state_dict(),  result_dir / f'{type(model).__name__}_{mode}.ckpt')\n",
    "\n",
    "def load_model(model, epoch=\"last\"):\n",
    "    if os.path.exists(result_dir / f'{type(model).__name__}_{mode}.ckpt'):\n",
    "        model.load_state_dict(torch.load(result_dir / f'{type(model).__name__}_{mode}.ckpt'))\n",
    "\n",
    "def load_model(model, epoch=\"last\"):\n",
    "    if os.path.exists(result_dir / f'{type(model).__name__}_{mode}.ckpt'):\n",
    "        model.load_state_dict(torch.load(result_dir / f'{type(model).__name__}_{mode}.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logs and ckpts will be saved in : ../results/trial_9\n"
     ]
    }
   ],
   "source": [
    "num_trial=0\n",
    "result_dir= Path(root) / 'results'\n",
    "parent_dir = result_dir / f'trial_{num_trial}'\n",
    "while parent_dir.is_dir():\n",
    "    num_trial = int(parent_dir.name.replace('trial_',''))\n",
    "    parent_dir = result_dir / f'trial_{num_trial+1}'\n",
    "\n",
    "# Modify parent_dir here if you want to resume from a checkpoint, or to rename directory.\n",
    "# parent_dir = result_dir / 'trial_99'\n",
    "print(f'Logs and ckpts will be saved in : {parent_dir}')\n",
    "\n",
    "log_dir = parent_dir\n",
    "ckpt_dir = parent_dir\n",
    "encoder_ckpt_path = parent_dir / 'encoder.pt'\n",
    "generator_ckpt_path = parent_dir / 'generator.pt'\n",
    "discriminator_ckpt_path = parent_dir / 'discriminator.pt'\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([4, 2])\n",
      "ref_slide torch.Size([4, 8, 5])\n",
      "ref_types torch.Size([4, 8])\n",
      "slide_deck torch.Size([4, 5, 8, 5])\n",
      "lengths_slide_deck torch.Size([4, 5])\n",
      "length_ref_types torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "for k in list(train_loader)[0].keys():\n",
    "    print(k,list(train_loader)[0][k].shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    alpha = Tensor(np.random.random((real_samples.size(0), 1, 1, 1)))\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(Tensor(real_samples.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = SlideDeckEncoder().to(device)\n",
    "discriminator = Discriminator(encoder.slide_encoder.embed.weight.data).to(device)\n",
    "generator = Generator(encoder.slide_encoder.embed.weight.data).to(device)\n",
    "\n",
    "models = {\n",
    "    \"discriminator\": discriminator,\n",
    "    \"encoder\" : encoder,\n",
    "    \"generator\" : generator,\n",
    "}\n",
    "\n",
    "optimizers = {\n",
    "    \"discriminator\": torch.optim.RMSprop(models[\"discriminator\"].parameters(), lr=args.lr),\n",
    "    \"generator\": torch.optim.RMSprop(models[\"generator\"].parameters(), lr=args.lr),\n",
    "    \"encoder\" : torch.optim.RMSprop(models[\"encoder\"].parameters(), lr=args.lr)\n",
    "}\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if device == 'cuda:0' else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(models, optimizers, is_train=True, dataloader=None):\n",
    "    batches_done = 0\n",
    "    for epoch in range(args.n_epochs):\n",
    "        total_loss_G = 0\n",
    "        total_loss_D = 0\n",
    "        G_num = 0\n",
    "        D_num = 0\n",
    "        loss_G = 0\n",
    "        loss_D = 0\n",
    "        if is_train:\n",
    "            for model in models:\n",
    "                models[model].train()\n",
    "        else:\n",
    "            for model in models:\n",
    "                models[model].eval()\n",
    "        \n",
    "        for i, batch in enumerate(dataloader):\n",
    "            batch = SortByRefSlide(batch)\n",
    "\n",
    "            # ['shape', 'ref_slide', 'ref_types', 'slide_deck', 'lengths_slide_deck', 'length_ref_types']\n",
    "\n",
    "            # conditioning\n",
    "            x_slide_deck = batch[\"slide_deck\"].to(device)\n",
    "            length_ref = batch[\"length_ref_types\"].to(device)\n",
    "            ref_types = batch[\"ref_types\"].to(device)\n",
    "            ref_slide = batch[\"ref_slide\"].to(device)\n",
    "\n",
    "            x_slide_deck = torch.transpose(x_slide_deck, 0, 1)\n",
    "            x_slide_deck = torch.transpose(x_slide_deck, 1, 2)\n",
    "            lengths_slide_deck = torch.transpose(batch[\"lengths_slide_deck\"], 0, 1).to(device)\n",
    "            \n",
    "            optimizers[\"encoder\"].zero_grad()\n",
    "            optimizers[\"discriminator\"].zero_grad()\n",
    "\n",
    "            slide_deck_embedding = models['encoder'](x_slide_deck, lengths_slide_deck)\n",
    "            \n",
    "            batch_size, _ = ref_types.shape\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            z = torch.autograd.Variable(Tensor(np.random.normal(0, 1, (batch_size, args.latent_vector_dim))))\n",
    "            \n",
    "            #   x (tensor): bb labels, (Batch_size, Sequence_size)\n",
    "            #     z (tensor): latent vector, (Batch_size, latent_vector_dim)\n",
    "            #     slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            #     length (tensor): (Batch_size,)\n",
    "            # (batch_size, seq, 4)\n",
    "            \n",
    "            # Configure input\n",
    "            # both have ref_types\n",
    "            \n",
    "            real_layouts_bbs = ref_slide[:,:,:-1]\n",
    "\n",
    "            fake_layouts_bbs = models['generator'](ref_types, z, slide_deck_embedding, length_ref)[0].detach()\n",
    "\n",
    "    #        print(\"true\", real_layouts_bbs[:,:,:4])\n",
    "   #         print(\"fake\", fake_layouts_bbs[:,:,:4])\n",
    "\n",
    "\n",
    "  #          print(\"true: \", models[\"discriminator\"](ref_types, real_layouts_bbs, slide_deck_embedding, length_ref))\n",
    " #           print(\"false: \", models[\"discriminator\"](ref_types, fake_layouts_bbs, slide_deck_embedding, length_ref))\n",
    "\n",
    "#            break\n",
    "\n",
    "            loss_D = (-torch.mean(models[\"discriminator\"](ref_types, real_layouts_bbs, slide_deck_embedding, length_ref))\n",
    "                + torch.mean(models[\"discriminator\"](ref_types, fake_layouts_bbs, slide_deck_embedding, length_ref)))\n",
    "            #     x (tensor): type labels, (Batch_size, Sequence_size)\n",
    "            #     bb (tensor): (Batch_size, Sequence_size, 4)\n",
    "            #     slide_deck_embedding (tensor): slide_deck_embedding vector, (Batch_size, slide_deck_embedding_dim)\n",
    "            #     length (tensor): (Batch_size,)\n",
    "            total_loss_D += loss_D.item()\n",
    "            D_num += 1\n",
    "            loss_D.backward()\n",
    "            optimizers[\"discriminator\"].step()\n",
    "            optimizers[\"encoder\"].step()\n",
    "\n",
    "            # # Clip weights of discriminator\n",
    "            # for p in models[\"discriminator\"].parameters():\n",
    "            #     p.data.clamp_(-args.clip_value, args.clip_value)\n",
    "\n",
    "            # Train the generator every n_critic iterations\n",
    "            if i % args.n_critic == 0:\n",
    "                optimizers[\"encoder\"].zero_grad()\n",
    "                optimizers[\"generator\"].zero_grad()\n",
    "\n",
    "                slide_deck_embedding = models['encoder'](x_slide_deck, lengths_slide_deck)\n",
    "                layouts_bbs = models['generator'](ref_types, z, slide_deck_embedding, length_ref)[0]\n",
    "                \n",
    "                # Adversarial loss\n",
    "                loss_G = -torch.mean(models[\"discriminator\"](ref_types, layouts_bbs, slide_deck_embedding, length_ref))\n",
    "                total_loss_G += loss_G.item()\n",
    "                G_num += 1\n",
    "                loss_G.backward()\n",
    "                optimizers[\"generator\"].step()\n",
    "                optimizers[\"encoder\"].step()\n",
    "        \n",
    "            #if batches_done % args.sample_interval == 0:\n",
    "            batches_done += 1\n",
    "           \n",
    "        print(\n",
    "            \"[Epoch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "            % (epoch, args.n_epochs, total_loss_D/D_num, total_loss_G/G_num)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/200] [D loss: -0.000654] [G loss: 0.072595]\n",
      "[Epoch 1/200] [D loss: -0.001813] [G loss: 0.074508]\n",
      "[Epoch 2/200] [D loss: -0.002973] [G loss: 0.076629]\n",
      "[Epoch 3/200] [D loss: -0.003345] [G loss: 0.076107]\n",
      "[Epoch 4/200] [D loss: -0.002414] [G loss: 0.076122]\n",
      "[Epoch 5/200] [D loss: -0.004209] [G loss: 0.078037]\n",
      "[Epoch 6/200] [D loss: -0.003662] [G loss: 0.077795]\n",
      "[Epoch 7/200] [D loss: -0.004704] [G loss: 0.075955]\n",
      "[Epoch 8/200] [D loss: -0.004529] [G loss: 0.077915]\n",
      "[Epoch 9/200] [D loss: -0.006485] [G loss: 0.077008]\n",
      "[Epoch 10/200] [D loss: -0.006896] [G loss: 0.077401]\n",
      "[Epoch 11/200] [D loss: -0.006062] [G loss: 0.079929]\n",
      "[Epoch 12/200] [D loss: -0.007804] [G loss: 0.079966]\n",
      "[Epoch 13/200] [D loss: -0.007960] [G loss: 0.080721]\n",
      "[Epoch 14/200] [D loss: -0.008220] [G loss: 0.081972]\n",
      "[Epoch 15/200] [D loss: -0.008683] [G loss: 0.079658]\n",
      "[Epoch 16/200] [D loss: -0.009027] [G loss: 0.076237]\n",
      "[Epoch 17/200] [D loss: -0.010896] [G loss: 0.081505]\n",
      "[Epoch 18/200] [D loss: -0.012879] [G loss: 0.079361]\n",
      "[Epoch 19/200] [D loss: -0.015593] [G loss: 0.084490]\n",
      "[Epoch 20/200] [D loss: -0.014343] [G loss: 0.081016]\n",
      "[Epoch 21/200] [D loss: -0.014193] [G loss: 0.083492]\n",
      "[Epoch 22/200] [D loss: -0.012156] [G loss: 0.076075]\n",
      "[Epoch 23/200] [D loss: -0.016540] [G loss: 0.080484]\n",
      "[Epoch 24/200] [D loss: -0.017799] [G loss: 0.087660]\n",
      "[Epoch 25/200] [D loss: -0.020276] [G loss: 0.080552]\n",
      "[Epoch 26/200] [D loss: -0.019743] [G loss: 0.082748]\n",
      "[Epoch 27/200] [D loss: -0.022142] [G loss: 0.086361]\n",
      "[Epoch 28/200] [D loss: -0.013593] [G loss: 0.076756]\n",
      "[Epoch 29/200] [D loss: -0.026646] [G loss: 0.084883]\n",
      "[Epoch 30/200] [D loss: -0.022660] [G loss: 0.084063]\n",
      "[Epoch 31/200] [D loss: -0.024088] [G loss: 0.086120]\n",
      "[Epoch 32/200] [D loss: -0.026256] [G loss: 0.079508]\n",
      "[Epoch 33/200] [D loss: -0.031945] [G loss: 0.077532]\n",
      "[Epoch 34/200] [D loss: -0.027572] [G loss: 0.085985]\n",
      "[Epoch 35/200] [D loss: -0.035311] [G loss: 0.089781]\n",
      "[Epoch 36/200] [D loss: -0.031019] [G loss: 0.069896]\n",
      "[Epoch 37/200] [D loss: -0.030260] [G loss: 0.076941]\n",
      "[Epoch 38/200] [D loss: -0.048592] [G loss: 0.080649]\n",
      "[Epoch 39/200] [D loss: -0.035758] [G loss: 0.079080]\n",
      "[Epoch 40/200] [D loss: -0.052441] [G loss: 0.085232]\n",
      "[Epoch 41/200] [D loss: -0.051994] [G loss: 0.089159]\n",
      "[Epoch 42/200] [D loss: -0.048418] [G loss: 0.082148]\n",
      "[Epoch 43/200] [D loss: -0.041037] [G loss: 0.088551]\n",
      "[Epoch 44/200] [D loss: -0.058646] [G loss: 0.096351]\n",
      "[Epoch 45/200] [D loss: -0.058304] [G loss: 0.074616]\n",
      "[Epoch 46/200] [D loss: -0.065077] [G loss: 0.070825]\n",
      "[Epoch 47/200] [D loss: -0.061075] [G loss: 0.089204]\n",
      "[Epoch 48/200] [D loss: -0.045391] [G loss: 0.069368]\n",
      "[Epoch 49/200] [D loss: -0.065594] [G loss: 0.065833]\n",
      "[Epoch 50/200] [D loss: -0.079158] [G loss: 0.075056]\n",
      "[Epoch 51/200] [D loss: -0.088004] [G loss: 0.085884]\n",
      "[Epoch 52/200] [D loss: -0.069742] [G loss: 0.073495]\n",
      "[Epoch 53/200] [D loss: -0.083543] [G loss: 0.058783]\n",
      "[Epoch 54/200] [D loss: -0.066869] [G loss: 0.069047]\n",
      "[Epoch 55/200] [D loss: -0.058961] [G loss: 0.030117]\n",
      "[Epoch 56/200] [D loss: -0.091463] [G loss: 0.083290]\n",
      "[Epoch 57/200] [D loss: -0.079609] [G loss: 0.068353]\n",
      "[Epoch 58/200] [D loss: -0.102079] [G loss: 0.058915]\n",
      "[Epoch 59/200] [D loss: -0.097452] [G loss: 0.052312]\n",
      "[Epoch 60/200] [D loss: -0.111961] [G loss: 0.077561]\n",
      "[Epoch 61/200] [D loss: -0.079648] [G loss: 0.042862]\n",
      "[Epoch 62/200] [D loss: -0.068636] [G loss: 0.060119]\n",
      "[Epoch 63/200] [D loss: -0.073507] [G loss: 0.062960]\n",
      "[Epoch 64/200] [D loss: -0.080618] [G loss: 0.071938]\n",
      "[Epoch 65/200] [D loss: -0.097604] [G loss: 0.112849]\n",
      "[Epoch 66/200] [D loss: -0.102462] [G loss: 0.064057]\n",
      "[Epoch 67/200] [D loss: -0.052800] [G loss: 0.059097]\n",
      "[Epoch 68/200] [D loss: -0.121945] [G loss: 0.091341]\n",
      "[Epoch 69/200] [D loss: -0.089714] [G loss: 0.016024]\n",
      "[Epoch 70/200] [D loss: -0.120547] [G loss: 0.039516]\n",
      "[Epoch 71/200] [D loss: -0.094089] [G loss: 0.063769]\n",
      "[Epoch 72/200] [D loss: -0.067407] [G loss: 0.041036]\n",
      "[Epoch 73/200] [D loss: -0.109274] [G loss: 0.054686]\n",
      "[Epoch 74/200] [D loss: -0.160116] [G loss: 0.063684]\n",
      "[Epoch 75/200] [D loss: -0.068797] [G loss: -0.016703]\n",
      "[Epoch 76/200] [D loss: -0.052493] [G loss: 0.103995]\n",
      "[Epoch 77/200] [D loss: -0.079154] [G loss: 0.064469]\n",
      "[Epoch 78/200] [D loss: -0.041823] [G loss: 0.023272]\n",
      "[Epoch 79/200] [D loss: -0.104627] [G loss: 0.077926]\n",
      "[Epoch 80/200] [D loss: -0.093080] [G loss: 0.012559]\n",
      "[Epoch 81/200] [D loss: -0.039926] [G loss: 0.048645]\n",
      "[Epoch 82/200] [D loss: -0.068590] [G loss: 0.019955]\n",
      "[Epoch 83/200] [D loss: -0.091635] [G loss: 0.105908]\n",
      "[Epoch 84/200] [D loss: -0.074750] [G loss: -0.062664]\n",
      "[Epoch 85/200] [D loss: -0.108966] [G loss: 0.067077]\n",
      "[Epoch 86/200] [D loss: -0.069567] [G loss: 0.056483]\n",
      "[Epoch 87/200] [D loss: -0.044996] [G loss: 0.079667]\n",
      "[Epoch 88/200] [D loss: -0.061940] [G loss: -0.019059]\n",
      "[Epoch 89/200] [D loss: -0.101191] [G loss: 0.029295]\n",
      "[Epoch 90/200] [D loss: 0.015748] [G loss: 0.046919]\n",
      "[Epoch 91/200] [D loss: 0.011272] [G loss: 0.002371]\n",
      "[Epoch 92/200] [D loss: -0.026703] [G loss: -0.014131]\n",
      "[Epoch 93/200] [D loss: -0.066153] [G loss: 0.018907]\n",
      "[Epoch 94/200] [D loss: -0.036349] [G loss: 0.025070]\n",
      "[Epoch 95/200] [D loss: -0.055432] [G loss: 0.080873]\n",
      "[Epoch 96/200] [D loss: -0.022065] [G loss: 0.046745]\n",
      "[Epoch 97/200] [D loss: -0.023399] [G loss: 0.047238]\n",
      "[Epoch 98/200] [D loss: -0.073453] [G loss: -0.022031]\n",
      "[Epoch 99/200] [D loss: 0.020983] [G loss: 0.094853]\n",
      "[Epoch 100/200] [D loss: -0.056238] [G loss: -0.049795]\n",
      "[Epoch 101/200] [D loss: -0.073279] [G loss: 0.039224]\n",
      "[Epoch 102/200] [D loss: -0.005887] [G loss: -0.051017]\n",
      "[Epoch 103/200] [D loss: 0.010242] [G loss: -0.029828]\n",
      "[Epoch 104/200] [D loss: 0.034746] [G loss: 0.051289]\n",
      "[Epoch 105/200] [D loss: 0.022012] [G loss: 0.086716]\n",
      "[Epoch 106/200] [D loss: -0.006384] [G loss: 0.050740]\n",
      "[Epoch 107/200] [D loss: -0.027271] [G loss: 0.103646]\n",
      "[Epoch 108/200] [D loss: 0.021296] [G loss: 0.081723]\n",
      "[Epoch 109/200] [D loss: -0.051076] [G loss: 0.039062]\n",
      "[Epoch 110/200] [D loss: 0.028419] [G loss: 0.053376]\n",
      "[Epoch 111/200] [D loss: -0.010640] [G loss: 0.086428]\n",
      "[Epoch 112/200] [D loss: -0.048757] [G loss: 0.137167]\n",
      "[Epoch 113/200] [D loss: 0.016584] [G loss: 0.069165]\n",
      "[Epoch 114/200] [D loss: 0.106582] [G loss: 0.074725]\n",
      "[Epoch 115/200] [D loss: 0.013269] [G loss: 0.049352]\n",
      "[Epoch 116/200] [D loss: 0.019040] [G loss: 0.086340]\n",
      "[Epoch 117/200] [D loss: 0.050041] [G loss: 0.076487]\n",
      "[Epoch 118/200] [D loss: 0.062256] [G loss: 0.064062]\n",
      "[Epoch 119/200] [D loss: 0.028343] [G loss: 0.124256]\n",
      "[Epoch 120/200] [D loss: -0.002383] [G loss: 0.134388]\n",
      "[Epoch 121/200] [D loss: 0.012711] [G loss: 0.120632]\n",
      "[Epoch 122/200] [D loss: 0.045180] [G loss: 0.126295]\n",
      "[Epoch 123/200] [D loss: 0.055726] [G loss: 0.159178]\n",
      "[Epoch 124/200] [D loss: 0.013743] [G loss: 0.190372]\n",
      "[Epoch 125/200] [D loss: 0.015823] [G loss: 0.153417]\n",
      "[Epoch 126/200] [D loss: 0.020106] [G loss: 0.145469]\n",
      "[Epoch 127/200] [D loss: 0.016232] [G loss: 0.178902]\n",
      "[Epoch 128/200] [D loss: -0.028694] [G loss: 0.189140]\n",
      "[Epoch 129/200] [D loss: 0.012738] [G loss: 0.111686]\n",
      "[Epoch 130/200] [D loss: 0.019891] [G loss: 0.144702]\n",
      "[Epoch 131/200] [D loss: 0.032033] [G loss: 0.208896]\n",
      "[Epoch 132/200] [D loss: 0.016614] [G loss: 0.204487]\n",
      "[Epoch 133/200] [D loss: 0.052665] [G loss: 0.181277]\n",
      "[Epoch 134/200] [D loss: -0.007996] [G loss: 0.206250]\n",
      "[Epoch 135/200] [D loss: 0.044467] [G loss: 0.198062]\n",
      "[Epoch 136/200] [D loss: 0.002308] [G loss: 0.209432]\n",
      "[Epoch 137/200] [D loss: 0.019021] [G loss: 0.180866]\n",
      "[Epoch 138/200] [D loss: -0.010489] [G loss: 0.187402]\n",
      "[Epoch 139/200] [D loss: 0.007048] [G loss: 0.135184]\n",
      "[Epoch 140/200] [D loss: -0.002441] [G loss: 0.180836]\n",
      "[Epoch 141/200] [D loss: 0.032336] [G loss: 0.188043]\n",
      "[Epoch 142/200] [D loss: 0.017636] [G loss: 0.210977]\n",
      "[Epoch 143/200] [D loss: 0.044613] [G loss: 0.216897]\n",
      "[Epoch 144/200] [D loss: 0.034013] [G loss: 0.187032]\n",
      "[Epoch 145/200] [D loss: 0.011819] [G loss: 0.212020]\n",
      "[Epoch 146/200] [D loss: 0.001448] [G loss: 0.178954]\n",
      "[Epoch 147/200] [D loss: 0.002837] [G loss: 0.195739]\n",
      "[Epoch 148/200] [D loss: 0.016219] [G loss: 0.203122]\n",
      "[Epoch 149/200] [D loss: 0.015426] [G loss: 0.165759]\n",
      "[Epoch 150/200] [D loss: 0.028681] [G loss: 0.203800]\n",
      "[Epoch 151/200] [D loss: 0.016789] [G loss: 0.181558]\n",
      "[Epoch 152/200] [D loss: 0.021260] [G loss: 0.222421]\n",
      "[Epoch 153/200] [D loss: 0.013416] [G loss: 0.217183]\n",
      "[Epoch 154/200] [D loss: 0.032118] [G loss: 0.248014]\n",
      "[Epoch 155/200] [D loss: 0.018578] [G loss: 0.203951]\n",
      "[Epoch 156/200] [D loss: 0.012654] [G loss: 0.201679]\n",
      "[Epoch 157/200] [D loss: 0.005189] [G loss: 0.212147]\n",
      "[Epoch 158/200] [D loss: 0.019635] [G loss: 0.193009]\n",
      "[Epoch 159/200] [D loss: 0.016923] [G loss: 0.181764]\n",
      "[Epoch 160/200] [D loss: 0.005913] [G loss: 0.186392]\n",
      "[Epoch 161/200] [D loss: 0.018062] [G loss: 0.210843]\n",
      "[Epoch 162/200] [D loss: 0.024035] [G loss: 0.217489]\n",
      "[Epoch 163/200] [D loss: 0.010490] [G loss: 0.201936]\n",
      "[Epoch 164/200] [D loss: 0.003069] [G loss: 0.179707]\n",
      "[Epoch 165/200] [D loss: 0.012382] [G loss: 0.234732]\n",
      "[Epoch 166/200] [D loss: 0.023553] [G loss: 0.197856]\n",
      "[Epoch 167/200] [D loss: 0.008168] [G loss: 0.195693]\n",
      "[Epoch 168/200] [D loss: 0.006209] [G loss: 0.176989]\n",
      "[Epoch 169/200] [D loss: -0.004262] [G loss: 0.184830]\n",
      "[Epoch 170/200] [D loss: 0.023706] [G loss: 0.196845]\n",
      "[Epoch 171/200] [D loss: 0.020811] [G loss: 0.223529]\n",
      "[Epoch 172/200] [D loss: 0.011915] [G loss: 0.173652]\n",
      "[Epoch 173/200] [D loss: 0.029209] [G loss: 0.179960]\n",
      "[Epoch 174/200] [D loss: 0.028903] [G loss: 0.190281]\n",
      "[Epoch 175/200] [D loss: 0.008138] [G loss: 0.224794]\n",
      "[Epoch 176/200] [D loss: -0.000986] [G loss: 0.201720]\n",
      "[Epoch 177/200] [D loss: 0.014437] [G loss: 0.202115]\n",
      "[Epoch 178/200] [D loss: 0.016794] [G loss: 0.173863]\n",
      "[Epoch 179/200] [D loss: 0.004410] [G loss: 0.217705]\n",
      "[Epoch 180/200] [D loss: 0.030302] [G loss: 0.196416]\n",
      "[Epoch 181/200] [D loss: 0.015251] [G loss: 0.208437]\n",
      "[Epoch 182/200] [D loss: 0.014760] [G loss: 0.204238]\n",
      "[Epoch 183/200] [D loss: 0.015466] [G loss: 0.204636]\n",
      "[Epoch 184/200] [D loss: 0.023137] [G loss: 0.193613]\n",
      "[Epoch 185/200] [D loss: -0.001791] [G loss: 0.220979]\n",
      "[Epoch 186/200] [D loss: 0.015511] [G loss: 0.202551]\n",
      "[Epoch 187/200] [D loss: 0.004859] [G loss: 0.203093]\n",
      "[Epoch 188/200] [D loss: 0.015779] [G loss: 0.187694]\n",
      "[Epoch 189/200] [D loss: 0.006597] [G loss: 0.210165]\n",
      "[Epoch 190/200] [D loss: 0.005328] [G loss: 0.197649]\n",
      "[Epoch 191/200] [D loss: 0.009662] [G loss: 0.202240]\n",
      "[Epoch 192/200] [D loss: 0.012116] [G loss: 0.183775]\n",
      "[Epoch 193/200] [D loss: 0.013664] [G loss: 0.196555]\n",
      "[Epoch 194/200] [D loss: 0.008677] [G loss: 0.192046]\n",
      "[Epoch 195/200] [D loss: 0.023206] [G loss: 0.192267]\n",
      "[Epoch 196/200] [D loss: 0.015935] [G loss: 0.215316]\n",
      "[Epoch 197/200] [D loss: 0.004510] [G loss: 0.194277]\n",
      "[Epoch 198/200] [D loss: 0.007469] [G loss: 0.198829]\n",
      "[Epoch 199/200] [D loss: 0.016476] [G loss: 0.204455]\n"
     ]
    }
   ],
   "source": [
    "run_epoch(models, optimizers, True, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "45dc61766396859fdffae760accc4b74216ea8fbbed6d1a9d5e8fb1914e35062"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
